{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "0c967fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using Random\n",
    "using Plots\n",
    "using PyPlot\n",
    "using StatsBase\n",
    "using StatsPlots\n",
    "using Serialization\n",
    "using LinearAlgebra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b1ef523",
   "metadata": {},
   "source": [
    "# Problem-Suite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebbc7791",
   "metadata": {},
   "source": [
    "Problem-Suite is a large structured notebook containing all of the functions created so far for this project.\n",
    "\n",
    "Sections:\n",
    "\n",
    "-[Miscellaneous Functions](#Miscellaneous-Functions)\n",
    "\n",
    "-[Pre-requisite functions for uniformised AVI](#Pre-requisite-functions-for-uniformised-AVI)\n",
    "\n",
    "-[Uniformised AVI functions](#Uniformised-AVI-functions)\n",
    "\n",
    "-[Pre-requisite functions for SMARVI](#Pre-requisite-functions-for-SMARVI)\n",
    "\n",
    "-[SMARVI Functions](#SMARVI-Functions)\n",
    "\n",
    "-[Pre-requisite Functions for Exact DP on Homogeneous Problems](#Pre-requisite-Functions-for-Exact-DP-on-Homogeneous-Problems)\n",
    "\n",
    "-[Exact DP on Homogeneous Problems (RVIA and PE/PI)](#Exact-DP-for-Homogeneous-problem)\n",
    "\n",
    "-[Pre-requisite Functions for Exact DP on Inhomogeneous Problems](#Pre-requisite-Functions-for-Exact-DP-on-Inhomogeneous-Problems)\n",
    "\n",
    "-[Exact DP on Inhomogeneous Problems (RVIA and PE/PI)](#Exact-DP-for-Inhomogeneous-Problem-(using-exact-h-or-VFA))\n",
    "\n",
    "-[Evaluation via simulation](#Evaluation-via-simulation)\n",
    "\n",
    "-[APE on Fully Active Policy](#APE-on-Fully-Active-Policy)\n",
    "\n",
    "-[SMARPE](#SMARPE)\n",
    "\n",
    "-[Tabular SMARVI and gEval](#tabular-smarvi-and-geval)\n",
    "\n",
    "-[SMART Functions](#SMART-Functions)\n",
    "\n",
    "-[SMARVI - New VFA](#SMARVI---New-VFA)\n",
    "\n",
    "-[SMARPI New VFA Functions](#SMARPI-New-VFA-Functions)\n",
    "\n",
    "-[Tests](#Tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12fe12",
   "metadata": {},
   "source": [
    "# Miscellaneous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2556d9f",
   "metadata": {},
   "source": [
    "-Functions for enumerating state and action spaces\n",
    "\n",
    "-Functions for calculating flows given a state or state-action pair\n",
    "\n",
    "-Function for evaluating a VFA at a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "26a0da98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arrayToString (generic function with 1 method)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#produce an array of array representations of all possible states\n",
    "function enumerateStates(N::Int64)\n",
    "    if N==1\n",
    "        return [[1],[2],[3]]\n",
    "    end\n",
    "    \n",
    "    output = []\n",
    "    lower = enumerateStates(N-1)\n",
    "    for s in lower\n",
    "        new1 = append!([1],s)\n",
    "        new2 = append!([2],s)\n",
    "        new3 = append!([3],s)\n",
    "        append!(output,[new1])\n",
    "        append!(output,[new2])\n",
    "        append!(output,[new3])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "#produce an array of array representations of all possible actions\n",
    "function enumerateActions(N::Int64)\n",
    "    if N==1\n",
    "        return [[0],[1]]\n",
    "    end\n",
    "    \n",
    "    output = []\n",
    "    lower = enumerateActions(N-1)\n",
    "    for a in lower\n",
    "        new1 = append!([0],a)\n",
    "        new2 = append!([1],a)\n",
    "        append!(output,[new1])\n",
    "        append!(output,[new2])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end    \n",
    "\n",
    "#produce array of array representations of all restricted, or single-repair, actions\n",
    "function enumerateRestrictedActions(N::Int64)\n",
    "    if N==1\n",
    "        return [[0],[1]]\n",
    "    end\n",
    "    \n",
    "    output = [zeros(Int64,N)]\n",
    "    for i in 1:N\n",
    "        temp = zeros(N)\n",
    "        temp[i] = 1\n",
    "        append!(output,[temp])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "#convert all array elements to string, then concatanate all elements (DEPRECATED AS DICTS CAN TAKE ARRAYS AS KEYS)\n",
    "function arrayToString(x)\n",
    "    return join(string.(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "57ba543e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateFlows (generic function with 2 methods)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function for calculating the flows given a state\n",
    "function calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    #update flows\n",
    "    flows = zeros(N)\n",
    "    healthy = sum(i == 1 for i in s)\n",
    "    \n",
    "    #if no links are healthy, return \n",
    "    if healthy == 0\n",
    "        return flows, c1\n",
    "    end\n",
    "    \n",
    "    #otherwise, find best route, and return\n",
    "    bestCost = maximum(c0) + 1\n",
    "    usedLink = 0\n",
    "    for k in 1:N\n",
    "        if s[k] == 1 && c0[k] < bestCost\n",
    "            bestCost = c0[k]\n",
    "            usedLink = k\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[usedLink] = beta\n",
    "    \n",
    "    return flows, bestCost\n",
    "end\n",
    "\n",
    "#function for calculating the flows given a state-action pair\n",
    "function calculateFlows(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    sPrime = s - a\n",
    "    return calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "127d3f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 4 methods)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate a VFA at a given state\n",
    "function v(s::Vector{Int64}, params::Vector{Float64}, features::Vector{Function})\n",
    "    numFeatures = length(features)\n",
    "    return params[1] + sum(params[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "960bf4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 4 methods)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version of v that takes flows for the features\n",
    "function v(s::Vector{Int64}, flows::Vector{Float64}, params::Vector{Float64}, features::Vector{Function})\n",
    "    N = length(params)\n",
    "    return params[1] + sum(params[i]*features[i-1](s, flows) for i in 2:N)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "a47c84ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculatePi_SSP (generic function with 1 method)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note here that tau is a vector of independend repair rates\n",
    "function calculatePi_SSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, lambda)\n",
    "    mu = fill(0.0, N)\n",
    "    nu = tau + alpha_r + lambda\n",
    "    pi = fill(0.0, N, 3) #index by pi[edge, state]\n",
    "    alpha_u = beta*alpha_d + alpha_r\n",
    "    mu[1] = alpha_u[1]\n",
    "    pi[1,1] = lambda[1]*tau[1]/(mu[1]*nu[1]+lambda[1]*tau[1])\n",
    "    pi[1,2] = (lambda[1]/nu[1])*(1 - pi[1,1])\n",
    "    pi[1,3] = 1 - pi[1,1] - pi[1,2]\n",
    "\n",
    "    useprob = 1\n",
    "    for i in 2:N\n",
    "        useprob = useprob*(1 - pi[i-1,1])\n",
    "        mu[i] = useprob*alpha_u[i] + (1 - useprob)alpha_r[i]\n",
    "        normConst = mu[i]*nu[i] + lambda[i]*tau[i]\n",
    "        pi[i,1] = tau[i]*lambda[i]/normConst\n",
    "        pi[i,2] = (lambda[i]/nu[i])*(1 - pi[i,1])\n",
    "        pi[i,3] = 1 - pi[i,1] - pi[i,2]\n",
    "    end\n",
    "\n",
    "    return pi\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "06ad4759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateG_SSP (generic function with 1 method)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calculateG_SSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, lambda)\n",
    "    pi = calculatePi_SSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, lambda)\n",
    "    prodNonHealthy = fill(0.0, N)\n",
    "    prodNonHealthy[1] = 1 - pi[1,1]\n",
    "    for i in 2:N\n",
    "        prodNonHealthy[i] = prodNonHealthy[i-1]*(1 - pi[i,1])\n",
    "    end\n",
    "\n",
    "    return sum(r[i]*pi[i,2] for i in 1:N) + beta*c0[1]pi[1,1] + sum(beta*c0[i]*pi[i,1]*prodNonHealthy[i-1] for i in 2:N)+ beta*c1*prodNonHealthy[N], pi\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "804d4962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateG_DSP (generic function with 1 method)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, dsPolicy)\n",
    "    if dsPolicy == fill(0,N)\n",
    "        return beta*c1\n",
    "    end\n",
    "    S = []\n",
    "    K = 0\n",
    "\n",
    "    #calculate S the set of maintained links and K the number of maintained links\n",
    "    prodNonHealthy = [1.0]\n",
    "    prodNonHealthyCurrent = 1\n",
    "    for i in 1:N\n",
    "        if dsPolicy[i] == 1\n",
    "            push!(S, i)\n",
    "            K+=1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #calculate p_AD(j in S, j<i) for all i in S, and p_AD(i in S)\n",
    "    for i in S\n",
    "        prodNonHealthyCurrent = prodNonHealthyCurrent*alpha_r[i]/(alpha_r[i] + tau[i])\n",
    "        push!(prodNonHealthy, prodNonHealthyCurrent)\n",
    "    end\n",
    "\n",
    "    #calculate and return g\n",
    "    return sum(r[S[i]]*alpha_r[S[i]]/(alpha_r[S[i]] + tau[S[i]]) + beta*c0[S[i]]*tau[S[i]]*prodNonHealthy[i]/(alpha_r[S[i]] + tau[S[i]]) for i in 1:K) + beta*c1*prodNonHealthy[K+1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "a59ff8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateG_SSP_Homog (generic function with 1 method)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calculateG_SSP_Homog(N,alpha_d, alpha_r, beta, tau, c0, c1, r, lambdaActive, K)\n",
    "    alpha_d = [alpha_d for i in 1:N]\n",
    "    alpha_r = [alpha_r for i in 1:N]\n",
    "    c0 = fill(c0, N)\n",
    "    r = fill(r, N)\n",
    "    lambda = fill(0.0, N)\n",
    "    for i in 1:K\n",
    "        lambda[i] = lambdaActive\n",
    "    end\n",
    "\n",
    "    return calculateG_SSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, lambda)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "7479409f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "faAction (generic function with 1 method)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computes the Fully Active action for a given state s\n",
    "function faAction(s)\n",
    "    N = length(s)\n",
    "    a = zeros(Int64,N)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a[i] = 1\n",
    "        end\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05d0fc",
   "metadata": {},
   "source": [
    "# Pre-requisite functions for uniformised AVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c56072f",
   "metadata": {},
   "source": [
    "This section contains functions used within the AVI algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "476755cd",
   "metadata": {},
   "source": [
    "Given a state-action pair, return the next random pre-decision state, the instant cost, and the updated flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "1d157b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateStateAndFlowsUnif (generic function with 1 method)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateStateAndFlowsUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #observe exogenous information\n",
    "    w = rand(Uniform(0, 1))\n",
    "    \n",
    "    #interpret exog info: is it a demand deg, rare deg, or completed repair \n",
    "    found = false\n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        if runningTotal <= w <= runningTotal + flows[k]*alpha_d[k]*del\n",
    "            found = true\n",
    "            sPrime[k] = 3\n",
    "            #println(\"Demand Deg at \"*string.(k))\n",
    "            break\n",
    "        end\n",
    "        runningTotal = runningTotal + flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    if found == false\n",
    "        for k in 1:N\n",
    "            if runningTotal <= w <= runningTotal + (s[k]!=3)*alpha_r[k]*del\n",
    "                found = true\n",
    "                sPrime[k] = 3\n",
    "                #println(\"Rare Deg at \"*string.(k))\n",
    "                break\n",
    "            end\n",
    "            runningTotal = runningTotal + alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if found == false\n",
    "        for k in 1:N\n",
    "            if runningTotal <= w <= runningTotal + (s[k]==2)*tau[k]*del\n",
    "                found = true\n",
    "                sPrime[k] = 1\n",
    "                break\n",
    "            end\n",
    "            runningTotal = runningTotal + (s[k]==2)*tau[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if found == false\n",
    "        #println(\"NO EVENT\")\n",
    "    end\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    \n",
    "    return sPrime, (beta*bestCost + sum(r[k]*(sPrime[k]==2) for k in 1:N))*del, newFlows\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "836f070e",
   "metadata": {},
   "source": [
    "Given a state action pair, return the instant cost over the delta timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "685eb320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostUnif (generic function with 1 method)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instant cost over the timestep\n",
    "function instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(sPrime[i] == 1 for i in 1:N)\n",
    "    repair = sum(sPrime[i] == 2 for i in 1:N)\n",
    "    damaged = sum(sPrime[i] == 3 for i in 1:N)\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    \n",
    "    return (beta*bestCost + sum(r[k]*(sPrime[k]==2) for k in 1:N))*del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f587fd5b",
   "metadata": {},
   "source": [
    "# Pre-requisite functions for SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2008a2c4",
   "metadata": {},
   "source": [
    "Helper functions for the SMARVI algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1f36cda",
   "metadata": {},
   "source": [
    "Given a state-action pair and pre-calculated flows, return the expected sojourn time for the state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "1506350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sojournTime (generic function with 1 method)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the expected sojourn time of a state-action pair\n",
    "function sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    s = s - a\n",
    "    if s == fill(3,N)\n",
    "        return 1/(beta*sum(alpha_d) + sum(alpha_r) + sum(tau))\n",
    "    end\n",
    "    \n",
    "    \n",
    "    totalRate = sum(flows[i]*alpha_d[i] + alpha_r[i]*(s[i]!=3) + tau[i]*(s[i]==2) for i in 1:N)\n",
    "    \n",
    "    return 1/totalRate\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb2c6d2b",
   "metadata": {},
   "source": [
    "Given a state-action pair and flows, calculate the expected cost accumulated until a transition occurs, or calculate the simulated cost accumulated over a simulated time del."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "66c5307b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostCont (generic function with 1 method)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the expected cost accumulated until a transition \n",
    "function instantCostCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows; del = 0)\n",
    "    if del == 0\n",
    "        del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    end\n",
    "    \n",
    "    return instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ecc13f2",
   "metadata": {},
   "source": [
    "Given a state-action pair, return the next random pre-decision state, the cost accumulated over the sojourn time, and the updated flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "b4dfb647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateStateAndFlowsCont (generic function with 1 method)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateStateAndFlowsCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    sOld = copy(sPrime)\n",
    "    flowsOld = copy(flows)\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #deal with all-damaged edge case\n",
    "    if sPrime == fill(3, N)\n",
    "        del = 1/(sum(beta*alpha_d) + sum(alpha_r) + tau(N))\n",
    "        return sPrime, beta*c1*del, flows, del\n",
    "    end\n",
    "\n",
    "    #create list of events (degradations for all non-damaged links and repair events for all repairing links) and list of rates\n",
    "    events = [i for i in 1:N if sPrime[i]!=3]\n",
    "    eventsRep = [N + i for i in 1:N if sPrime[i] == 2]\n",
    "    append!(events, eventsRep)\n",
    "\n",
    "    rates = [flows[i]*alpha_d[i] + alpha_r[i] for i in 1:N if sPrime[i]!=3]\n",
    "    ratesRep = [tau[i] for i in 1:N if sPrime[i] == 2]\n",
    "    append!(rates, ratesRep)\n",
    "\n",
    "    #sample event\n",
    "    del = 1/sum(rates)\n",
    "    event = wsample(events, rates)\n",
    "\n",
    "    #if event >= N+1, it's a repair, otherwise it's a degradation\n",
    "    if event >= N + 1 \n",
    "        sPrime[event - N] = 1\n",
    "    else\n",
    "        sPrime[event] = 3\n",
    "    end\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    \n",
    "    return sPrime, (beta*c1*(healthy == 0) + sum(flowsOld[k]*c0[k] + r[k]*(sOld[k]==2) for k in 1:N))*del, newFlows, del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "620585dd",
   "metadata": {},
   "source": [
    "Generate random action for e-greedy algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "d2fd2e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randomActionAllDamaged (generic function with 1 method)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate random action for e-greedy algorithms\n",
    "function randomAction(s,N)\n",
    "    #deal with all-damaged case\n",
    "    if s == fill(3,N)\n",
    "        return randomActionAllDamaged(N)\n",
    "    end\n",
    "\n",
    "    damaged = [0]\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            append!(damaged, [i])\n",
    "        end\n",
    "    end\n",
    "            \n",
    "    choice = sample(damaged)\n",
    "    optA = zeros(Int64, N)\n",
    "    if choice == 0\n",
    "        return optA\n",
    "    else\n",
    "        optA[choice] = 1\n",
    "        return optA\n",
    "    end\n",
    "end\n",
    "\n",
    "#calculate random action for [3,3...,3] state\n",
    "function randomActionAllDamaged(N)\n",
    "    choice = sample(1:N)\n",
    "    a = zeros(Int64, N)\n",
    "    a[choice] = 1\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eaa235",
   "metadata": {},
   "source": [
    "# Pre-requisite Functions for Exact DP on Homogeneous Problems "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7b7c104",
   "metadata": {},
   "source": [
    "Helper functions for Homogeneous Exact DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1801225",
   "metadata": {},
   "source": [
    "Calculates instant cost for homogeneous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "22678b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instant cost function strictly for homogeneous problem\n",
    "function instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    i1Prime = i1 - a\n",
    "    i2Prime = i2 + a\n",
    "    \n",
    "    #if no links are healthy, return \n",
    "    if N - i1 - i2 == 0\n",
    "        return (beta*c1 + r*i2Prime)*del\n",
    "    end\n",
    "    \n",
    "    \n",
    "    return (beta*c0 + r*i2Prime)*del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94e504b7",
   "metadata": {},
   "source": [
    "Given state (i_1, i_2) and action a, calculates the expected next value function after one timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "a17eacf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates E(h(s')) from s,a strictly for a homogeneous problem\n",
    "function expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    i1Prime = i1 - a\n",
    "    i2Prime = i2 + a\n",
    "    thisH = h[i1+1,i2+1]\n",
    "    \n",
    "    #if all are damaged\n",
    "    if i1Prime == N\n",
    "        return thisH\n",
    "    end\n",
    "    \n",
    "    #if none are healthy\n",
    "    if N - i1 - i2 == 0\n",
    "        return thisH + tau*i2Prime*del*(h[i1Prime+1,i2Prime-1+1] - thisH) + i2Prime*del*alpha_r*(h[i1Prime+1+1, i2Prime-1+1] - thisH)\n",
    "    end\n",
    "    \n",
    "    #if none are repairing\n",
    "    if i2Prime == 0\n",
    "        return thisH + (beta*alpha_d + (N - i1 - i2)*alpha_r)*del*(h[i1Prime+1+1,i2Prime+1] - thisH)\n",
    "    end\n",
    "    \n",
    "    return thisH + (beta*alpha_d + (N - i1 - i2)*alpha_r)*del*(h[i1Prime+1+1,i2Prime+1] - thisH) + i2Prime*alpha_r*del*(h[i1Prime+1+1,i2Prime-1+1] - thisH) + tau*i2Prime*del*(h[i1Prime+1,i2Prime-1+1] - thisH)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68c7af12",
   "metadata": {},
   "source": [
    "Given state (i_1,i_2) and value function h, calculates and returns the best action for the state using full expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "beeb2b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the exact PI action strictly for a homogeneous problem\n",
    "function piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "    if i1 == 0\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    if i1 == N && forceActive\n",
    "        optA = 1\n",
    "        optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        for a in 2:i1\n",
    "            testH = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            if testH <= optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "        return optA\n",
    "    end\n",
    "    \n",
    "    optA = 0\n",
    "    optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    for a in 1:i1\n",
    "        testH = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        if testH <= optH\n",
    "            optA = a\n",
    "            optH = testH\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71928099",
   "metadata": {},
   "source": [
    "Similar to above, but uses the approximation Q(s,a) = h(s+a) for a!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "b58a1177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the approx PI action based on instantaneous approximation, strictly for a homogeneous problem\n",
    "function piActionHomogApprox(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = false)\n",
    "    #deal with \"nothing damaged\" edge case\n",
    "    if i1 == 0\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    #deal with \"everything damaged\" edge case\n",
    "    if i1 == N && forceActive\n",
    "        optA = 1\n",
    "        optH = h[i1-optA+1,i2+optA+1]\n",
    "        for a in 2:i1\n",
    "            testH = h[i1-a+1,i2+a+1]\n",
    "            if testH <= optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "        return optA\n",
    "    end\n",
    "    \n",
    "    optA = 0\n",
    "    optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) - g*del\n",
    "    for a in 1:i1\n",
    "        testH = h[i1-a+1,i2+a+1]\n",
    "        if testH <= optH\n",
    "            optA = a\n",
    "            optH = testH\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8ab05cf",
   "metadata": {},
   "source": [
    "Given h, constructs optimal policy using exact PI method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "6335a065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the exact PI policy strictly for a homogeneous problem\n",
    "function piPolicyHomog(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "    policy = zeros(Int64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            policy[i1+1,i2+1] = piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34525984",
   "metadata": {},
   "source": [
    "Given h, constructs optimal policy using Q(s,a) = h(s,a) for a!=0 approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "404bfec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the approx PI policy based on instantaneous approximation, strictly for a homogeneous problem\n",
    "function piPolicyHomogApprox(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = false)\n",
    "    policy = zeros(Int64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            policy[i1+1,i2+1] = piActionHomogApprox(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = forceActive)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905b77bf",
   "metadata": {},
   "source": [
    "Constructs a h table from a VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "42156f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hFromVFAHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hFromVFAHomog(N, params, features)\n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return hIn\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c11abe",
   "metadata": {},
   "source": [
    "# Exact DP for Homogeneous problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fc32308",
   "metadata": {},
   "source": [
    "Actual DP algorithms "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99f7e6cf",
   "metadata": {},
   "source": [
    "Given a h table, performs PE on PI policy derived from h, and returns g, h, n (number of iterations), and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "fc148338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given exact h function, strictly for a homogeneous problem \n",
    "function rpiHomog(N::Int64, hIn, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + N*tau))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    policy = piPolicyHomog(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = policy[i1+1,i2+1]\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b8046e6",
   "metadata": {},
   "source": [
    "Performs PE on the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "e39012f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpeFAHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates the fully active policy, strictly for a homogeneous problem \n",
    "function rpeFAHomog(N::Int64, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + N*tau))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = i1\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c051660",
   "metadata": {},
   "source": [
    "Similar to rpiHomog, but uses Q(s,a) = h(s,a) approximation for PI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "964c01ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates an approximate PI policy based on a given exact h function and instananeous actions, strictly for a homogeneous problem \n",
    "function rpiHomogApprox(N::Int64, hIn, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64, g::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + N*tau))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    policy = piPolicyHomogApprox(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = forceActive)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = policy[i1+1,i2+1]\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "690683ed",
   "metadata": {},
   "source": [
    "Performs RVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "939c3f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rviHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs RVIA \n",
    "function rviHomog(N::Int64, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + N*tau))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, piPolicyHomog(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b3849",
   "metadata": {},
   "source": [
    "# Pre-requisite Functions for Exact DP on Inhomogeneous Problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1e53ece",
   "metadata": {},
   "source": [
    "Helper functions for inhomogeneous exact DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14ea586f",
   "metadata": {},
   "source": [
    "Given a state-action pair and h, calculates the expected next value of the value function after one timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "d45adbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueExact (generic function with 1 method)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates E(h(s')) from s,a using exact h table\n",
    "function expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    \n",
    "    flows = zeros(Float64, N)\n",
    "    if healthy > 0\n",
    "        #otherwise, find best route, and return\n",
    "        bestCost = maximum(c0) + 1\n",
    "        usedLink = 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 1 && c0[k] < bestCost\n",
    "                bestCost = c0[k]\n",
    "                usedLink = k\n",
    "            end \n",
    "        end\n",
    "        \n",
    "        flows[usedLink] = beta\n",
    "    end\n",
    "    \n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*h[sNext]\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*h[sNext]\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] == 2\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 1\n",
    "            runningTotal += tau[k]*del*h[sNext]\n",
    "            runningTotalProb += tau[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*h[sPrime]\n",
    "end "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f2dad95",
   "metadata": {},
   "source": [
    "Given a state and a h table, calculates the PI action for s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "4b71c013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExact (generic function with 1 method)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table\n",
    "function piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2cb0d43",
   "metadata": {},
   "source": [
    "Similar to above, but uses the approximation Q(s,a) = h(s+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "17b1bbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table, based off instananeous actions\n",
    "function piActionExactInstant(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h) - g*del\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = h[s-a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884618d7",
   "metadata": {},
   "source": [
    "Constructs PI policy using h table and no approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "e1dfe8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExact (generic function with 1 method)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy using exact h table\n",
    "function piPolicyExact(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8842b6de",
   "metadata": {},
   "source": [
    "Constructs PI policy using h table and Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "bfb9dcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy using exact h table, interpretting h with instant actions\n",
    "function piPolicyExactInstant(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactInstant(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7172ee4",
   "metadata": {},
   "source": [
    "# Exact DP for Inhomogeneous Problem (using exact h or VFA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43e2ee1b",
   "metadata": {},
   "source": [
    "DP algorithms for inhomogeneous problem\n",
    "\n",
    "Note that throughout when we talk of a Q(s,a) = h(s+a) approximation, this only refers to action selection and not update rules, and excludes a=0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2225adc2",
   "metadata": {},
   "source": [
    "Given an explicit policy table, performs PE, returns g, h and n (# of iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "8b118e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpe (generic function with 1 method)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs PE using exact policy table\n",
    "function rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + sum(tau)))\n",
    "    h = Dict()\n",
    "    w = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        w[s] = 0.0\n",
    "    end\n",
    "    s0  = fill(1, N)\n",
    "    n = 0\n",
    "    \n",
    "    #do until max iterations met or epsilon convergence\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #find updates for every state\n",
    "        for s in stateSpace\n",
    "            a = policy[s]\n",
    "            w[s] = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "        end\n",
    "        \n",
    "        #calculate relative values and delta\n",
    "        delta = 0\n",
    "        for s in stateSpace\n",
    "            update = w[s] - w[s0]\n",
    "            if delta < update - h[s] || delta == 0\n",
    "                delta = update - h[s]\n",
    "            end\n",
    "            \n",
    "            h[s] = update\n",
    "        end\n",
    "        \n",
    "        #stopping condition\n",
    "        if delta < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if printProgress && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    a = zeros(Int64, N)\n",
    "    g = instantCostUnif(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h) - h[s0]\n",
    "    \n",
    "    return g/del, h, n\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1ad5a2b",
   "metadata": {},
   "source": [
    "Given a h table, constructs PI policy and performs PE, returning g, h, n and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "7c57636f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiExact (generic function with 1 method)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using exact h table\n",
    "function rpiExact(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + sum(tau)))\n",
    "    policy = piPolicyExact(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    output = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "    return output[1], output[2], output[3], policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec78eb5a",
   "metadata": {},
   "source": [
    "Similar to above, but uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "266027ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using exact h table, using instant actions to interpet h\n",
    "function rpiExactInstant(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + sum(tau)))\n",
    "    policy = piPolicyExactInstant(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    output = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "    return output[1], output[2], output[3], policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa6660e",
   "metadata": {},
   "source": [
    "Performs PE on the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "f773f8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpeFA (generic function with 1 method)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs exact PE on the fully-active policy\n",
    "function rpeFA(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = faAction(s)\n",
    "    end\n",
    "    \n",
    "    return rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c325a927",
   "metadata": {},
   "source": [
    "Performs PE on fully passive policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "d304fb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpePassive (generic function with 1 method)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs exact PE on the passive policy\n",
    "function rpePassive(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    return rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b0b6c10",
   "metadata": {},
   "source": [
    "Performs RVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "7d38fbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rvi (generic function with 1 method)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs RVIA\n",
    "function rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = N, printProgress = true, modCounter = 1000)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + sum(tau)))\n",
    "    h = Dict()\n",
    "    w = Dict()\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        w[s] = 0.0\n",
    "        policy[s] = zeros(Int64,N)\n",
    "    end\n",
    "    s0  = fill(1, N)\n",
    "    n = 0\n",
    "    \n",
    "    #do until max iterations met or epsilon convergence\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #find updates for every state\n",
    "        for s in stateSpace\n",
    "            a = piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            w[s] = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "            policy[s] = a\n",
    "        end\n",
    "        \n",
    "        #calculate relative values and delta\n",
    "        delta = 0\n",
    "        for s in stateSpace\n",
    "            update = w[s] - w[s0]\n",
    "            if delta < update - h[s] || delta == 0\n",
    "                delta = update - h[s]\n",
    "            end\n",
    "            \n",
    "            h[s] = update\n",
    "        end\n",
    "        \n",
    "        #stopping condition\n",
    "        if delta < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if printProgress && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    a = zeros(Int64, N)\n",
    "    g = instantCostUnif(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h) - h[s0]\n",
    "    \n",
    "    return g/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb424a1e",
   "metadata": {},
   "source": [
    "# Evaluation via simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bcbbb37",
   "metadata": {},
   "source": [
    "Various evaluation functions for approximating g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6636431",
   "metadata": {},
   "source": [
    "Finds the g of the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "33be91d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFA (generic function with 1 method)"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates the FA policy\n",
    "function gEvaluationFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax; s = fill(1,N), printProgress = false, modCounter = 100000, stateTrace = false)\n",
    "    #initialise\n",
    "    s0 = fill(1,N)\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        \n",
    "        #formulate FA action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)        \n",
    "        end\n",
    "\n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            g = (g*timePassed + c)/(timePassed + time)\n",
    "            timePassed += time\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, timePassed\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "bdcb65b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationDSP (generic function with 1 method)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a Deterministic Static Policy (e-soft)\n",
    "function gEvaluationDSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, dsPolicy; s = fill(1, N),  epsilon = -1.0, printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate DSP action\n",
    "        bestA = fill(0, N)\n",
    "        if rand(Uniform(0,1)) > epsilon\n",
    "            for i in 1:N\n",
    "                if s[i] == 3 && dsPolicy[i] == 1\n",
    "                    bestA[i] = 1\n",
    "                end\n",
    "            end\n",
    "        else\n",
    "            bestA = randomAction(s, N)\n",
    "        end\n",
    "        \n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, runningTotals, times, s\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d24567",
   "metadata": {},
   "source": [
    "# Tabular SMARVI and gEval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b101ab9",
   "metadata": {},
   "source": [
    "Tabular SMARVI algorithms (non e-greedy, e-greedy, and e-greedt with state trace), associated gEval function, and helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a829f15",
   "metadata": {},
   "source": [
    "Given a state s, its flows, and a h-g pair, return the optimal action and V value for s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "d500f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromTable (generic function with 1 method)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "    #find optimal action\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        \n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "               \n",
    "            if h[s-a] <= optV\n",
    "                optV = h[s-a]\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    #Fix choose optimal non-passive action if state is [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optV = h[s-optA]\n",
    "            \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                testV = h[s-a]\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94fd262e",
   "metadata": {},
   "source": [
    "Given a state-action pair and a h table, compute the next expected h value given that a transition has occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "bd0e5b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContTab (generic function with 1 method)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and tabular h\n",
    "function expectedNextValueContTab(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return h[sPrime]\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*h[sNext]\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*h[sNext]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] == 2\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 1\n",
    "            runningTotal += tau[k]*del*h[sNext]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2833561",
   "metadata": {},
   "source": [
    "Tabular SMARVI, without ST (set epsilon = 0 for full-exploitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "cf860122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab (generic function with 1 method)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Uses tabular representation instead of VFA and e-greedy action selection\n",
    "function smarviTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = h[s]\n",
    "        h[s] += (b/(b + numVisits[s]))*(bestV - currentEst)\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f7777e",
   "metadata": {},
   "source": [
    "Similar to above, but incorporates the state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "de3e948e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab_ST (generic function with 1 method)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses tabular representation instead of VFA, e-greedy action selection, and state trace \n",
    "function smarviTab_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        #choose actual action\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5619d4e1",
   "metadata": {},
   "source": [
    "Similar to above (so e-greedy and state trace), but uses a moving average window to approximate g, allowing old estimates to be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "5e0f86bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTabMA (generic function with 1 method)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tabular SMARVI with moving average online approximation for g, e-greedy action selection, and state trace\n",
    "function smarviTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; window = 2500000, printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    totalCosts = [0.0]\n",
    "    timePassed = 0.0\n",
    "    totalTimes = [0.0]\n",
    "    lenTotals = 1\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        #choose actual action\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            push!(totalCosts, runningTotal)\n",
    "            push!(totalTimes, timePassed)\n",
    "            lenTotals += 1\n",
    "            if lenTotals <= window\n",
    "                g = runningTotal/timePassed\n",
    "            else\n",
    "                g = (runningTotal - totalCosts[lenTotals - window])/(timePassed - totalTimes[lenTotals - window])\n",
    "            end\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0ca583e",
   "metadata": {},
   "source": [
    "Given a state, a h table and g, return the PI action for s. Uses the Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "a89e1d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactCont (generic function with 1 method)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table, based off continuous model\n",
    "function piActionExactCont(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optH = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows,h) - g*t\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = h[s-a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optH = h[s - optA]\n",
    "\n",
    "        for i in 2:N\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            testH = h[s - a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5696a2b2",
   "metadata": {},
   "source": [
    "Constructs a PI policy using the above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "47ec161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactCont (generic function with 1 method)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piPolicyExactCont(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactCont(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c7cc371",
   "metadata": {},
   "source": [
    "Given a h table and fixed g0, approximates the g of the PI policy derived using the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "161485ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationTab (generic function with 1 method)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, h, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    println(\"State Space Completed\")\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    policy = piPolicyExactCont(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Completed\")\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = policy[s]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6efb11fd",
   "metadata": {},
   "source": [
    "Returns an array of feasible actions for N-dim state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "c00dff36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enumerateFeasibleActions (generic function with 1 method)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function enumerateFeasibleActions(s,N)\n",
    "    actionSpace = []\n",
    "    if s == fill(3, N)\n",
    "        for i in 1:N\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            push!(actionSpace, a)\n",
    "        end\n",
    "        return actionSpace\n",
    "    end\n",
    "\n",
    "    push!(actionSpace, zeros(Int64,N))\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            push!(actionSpace, a)\n",
    "        end\n",
    "    end\n",
    "    return actionSpace\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884ee22f",
   "metadata": {},
   "source": [
    "# SMART Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc90c99f",
   "metadata": {},
   "source": [
    "Tabular SMART algorithm, associated gEvaluation function, and helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6bef550",
   "metadata": {},
   "source": [
    "Given a state and a q-table, return optimal action and associated Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "9b86ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actionFromQTab (generic function with 1 method)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function actionFromQTab(s, N, q)\n",
    "    feasibleActions = enumerateFeasibleActions(s,N)\n",
    "\n",
    "    #formulate action\n",
    "    optA = zeros(Int64, N)\n",
    "    if s == fill(3,N)\n",
    "        optA[1] = 1\n",
    "    end\n",
    "    optQ = q[s,optA]\n",
    "    for a in feasibleActions\n",
    "        testQ = q[s,a]\n",
    "        if testQ < optQ\n",
    "            optQ = testQ\n",
    "            optA = a\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optQ\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93d0c99a",
   "metadata": {},
   "source": [
    "Construct policy using above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "233657b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactContQ (generic function with 1 method)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piPolicyExactContQ(q, N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = actionFromQTab(s, N, q)[1]\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bcb0dda",
   "metadata": {},
   "source": [
    "Performs SMART, using a state-action trace and e-greedy action selection, where e can be chosen to depend on the state or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "1f7db3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartTab (generic function with 1 method)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smartTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    optAandQ = actionFromQTab(s, N, q)\n",
    "    optA = optAandQ[1]\n",
    "    optQ = optAandQ[2]\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        optFlag = true\n",
    "        \n",
    "        #choose e-greedy action\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            optQ = q[s,optA]\n",
    "            optFlag = false\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g if optimal action taken\n",
    "            if optFlag\n",
    "                runningTotal += c\n",
    "                timePassed += time\n",
    "                g = runningTotal/timePassed\n",
    "            end\n",
    "\n",
    "            #find next optimal action and q value\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb19c603",
   "metadata": {},
   "source": [
    "Taking a Q table as input, formulates the associated policy and simulates it to approximate g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "0943848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationTabQ (generic function with 1 method)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationTabQ(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, q; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    println(\"State Space Completed\")\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    policy = piPolicyExactContQ(q, N)\n",
    "    println(\"Policy Completed\")\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = policy[s]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15b7f9ed",
   "metadata": {},
   "source": [
    "On-Policy equivalent of SMART, using next chosen action instead of next optimal action for the update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "433d3d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartOnPolicyTab (generic function with 1 method)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smartOnPolicyTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #choose only action for s = s0\n",
    "    optA = zeros(Int64, N)\n",
    "    optQ = q[s,optA]\n",
    "    optFlag = true\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "\n",
    "            #find next e-greedy action and q value\n",
    "            #find optimal action and q-value\n",
    "            optFlag = true\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            #choose epsilon\n",
    "            epsilon = c/(c + n)\n",
    "            \n",
    "            if stateDepEpsilon\n",
    "                epsilon = c/(c + numVisits[sPrime])\n",
    "            end\n",
    "\n",
    "            #select random action with probability epsilon\n",
    "            if rand(Uniform(0,1)) < epsilon\n",
    "                nextOptA = randomAction(sPrime, N)\n",
    "                nextOptQ = q[sPrime,nextOptA]\n",
    "                optFlag = false\n",
    "            end\n",
    "            \n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            optFlag = true\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            #choose epsilon\n",
    "            epsilon = c/(c + n)\n",
    "            \n",
    "            if stateDepEpsilon\n",
    "                epsilon = c/(c + numVisits[sPrime])\n",
    "            end\n",
    "\n",
    "            #select random action with probability epsilon\n",
    "            if rand(Uniform(0,1)) < epsilon\n",
    "                nextOptA = randomAction(sPrime, N)\n",
    "                nextOptQ = q[sPrime,nextOptA]\n",
    "                optFlag = false\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70994d31",
   "metadata": {},
   "source": [
    "Version of SMART using moving average window to approximate g, discarding older data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "d8fa137b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartTabMA (generic function with 1 method)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SMART with an MA approximation for g\n",
    "function smartTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; window = 1000000, printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    totalCosts = [0.0]\n",
    "    lenTotalCosts = 1\n",
    "    totalTimes = [0.0]\n",
    "    lenTotal = 1\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    optAandQ = actionFromQTab(s, N, q)\n",
    "    optA = optAandQ[1]\n",
    "    optQ = optAandQ[2]\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        optFlag = true\n",
    "        \n",
    "        #choose e-greedy action\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            optQ = q[s,optA]\n",
    "            optFlag = false\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g if optimal action taken\n",
    "            if optFlag\n",
    "                runningTotal += c\n",
    "                timePassed += time\n",
    "                push!(totalCosts, runningTotal)               \n",
    "                push!(totalTimes, timePassed)\n",
    "                lenTotal += 1\n",
    "                if lenTotal <= window\n",
    "                    g = runningTotal/timePassed\n",
    "                else\n",
    "                    g = (runningTotal - totalCosts[lenTotal - window])/(timePassed - totalTimes[lenTotal - window])\n",
    "                end\n",
    "            end\n",
    "\n",
    "            #find next optimal action and q value\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "199fc408",
   "metadata": {},
   "source": [
    "# Tabular SMARPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "29ea47f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabST (generic function with 1 method)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SMARPE with tabular representation instead of VFA, and state trace \n",
    "function smarpeTabST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, hIn, g0, nMax, b; copyH = false, printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    policy = piPolicyExactCont(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Constructed\")\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        if copyH\n",
    "            h[s] = hIn[s]\n",
    "        else\n",
    "            h[s] = 0.0\n",
    "        end\n",
    "        \n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optA = policy[s]\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "9483ef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabStochST (generic function with 1 method)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses tabular representation instead of VFA, e-greedy action selection, and state trace \n",
    "function smarpeTabStochST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #choose random action\n",
    "        optA = randomAction(s,N)\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "2da93d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabST_epsSoft_onPolicy (generic function with 1 method)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tabular SMARPE with state trace using e-soft policy\n",
    "function smarpeTabST_epsSoft_onPolicy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, hIn, g0, nMax, b, c; copyH = false, printProgress = false, modCounter = 100000, stateDepEpsilon = true)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    policy = piPolicyExactCont(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Constructed\")\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        if copyH\n",
    "            h[s] = hIn[s]\n",
    "        else\n",
    "            h[s] = 0.0\n",
    "        end\n",
    "        \n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate action\n",
    "        optA = policy[s]\n",
    "        optV = 0.0\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        #e-greedy action\n",
    "        bestA = optA\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            bestA = randomAction(s,N)\n",
    "            if bestA == zeros(Int64, N)\n",
    "                t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            end\n",
    "        end\n",
    "\n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe61cfb6",
   "metadata": {},
   "source": [
    "# SMARVI - New VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "2411428a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subStates (generic function with 1 method)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns substates of edges in an array, and state of destination node\n",
    "function subStates(s, N, flows)\n",
    "    sis = []\n",
    "    sn = 0\n",
    "    for i in 1:N\n",
    "        if s[i] == 2 || s[i] == 3\n",
    "            push!(sis, s[i])\n",
    "        elseif flows[i] == 0\n",
    "            push!(sis, 0)\n",
    "        else\n",
    "            push!(sis, 1)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if flows == fill(0.0, N)\n",
    "        sn = 1\n",
    "    end\n",
    "    \n",
    "    return sis, sn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "b40410e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 4 methods)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates v from seperate ve and vn tables\n",
    "function v(s::Vector{Int64}, N::Int64, flows::Vector{Float64}, ve::Dict, vn::Dict; s0 = [])\n",
    "    if s == s0\n",
    "        return 0.0\n",
    "    end\n",
    "\n",
    "    substates = subStates(s, N, flows)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    v = 0.0\n",
    "    for i in 1:N\n",
    "        si = sis[i]\n",
    "        v += ve[i,si]\n",
    "    end\n",
    "\n",
    "    v += vn[sn]\n",
    "\n",
    "    return v\n",
    "end\n",
    "\n",
    "function v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn; s0 = [])\n",
    "    if s == s0\n",
    "        return 0.0\n",
    "    end\n",
    "    \n",
    "    flowsAndCost = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    return v(s, N, flowsAndCost[1], ve, vn)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "bdcbb3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and VFA from ve and vn tables\n",
    "function expectedNextValueContNewVFA(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn; s0 = [])\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn; s0 = s0)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn; s0 = s0)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] == 2\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 1\n",
    "            runningTotal += tau[k]*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn; s0 = s0)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "e63f3219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g; s0 = [])\n",
    "    #find optimal action\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContNewVFA(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn; s0 = s0) - g*t\n",
    "    zeroV = optV\n",
    "\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            \n",
    "            testV = v(s-a, N, flows, ve, vn; s0 = s0)\n",
    "            if testV <= optV\n",
    "                optV = testV\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    #Fix choose optimal non-passive action if state is [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optV = v(s-optA, N, flows, ve, vn; s0 = s0)\n",
    "            \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "\n",
    "                testV = v(s-a, N, flows, ve, vn; s0 = s0)\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV, zeroV\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "dee6e500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policyFromNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function policyFromNewVFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "        policy[s] = smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)[1]\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "ea55e06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromNewVFA_AC (generic function with 1 method)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromNewVFA_AC(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, gActor, gCritic; s0 = [])\n",
    "    #find optimal action using gActor to calculate Q(s,0)\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContNewVFA(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn; s0 = s0) - gActor*t\n",
    "    zeroV = optV + gActor*t - gCritic*t #for zeroV, replace gActor with gCritic for accurate evaluation value\n",
    "\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            \n",
    "            testV = v(s-a, N, flows, ve, vn; s0 = s0)\n",
    "            if testV <= optV\n",
    "                optV = testV\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    #Fix choose optimal non-passive action if state is [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optV = v(s-optA, N, flows, ve, vn; s0 = s0)\n",
    "            \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "\n",
    "                testV = v(s-a, N, flows, ve, vn; s0 = s0)\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #if passive action taken, set optV to zeroV for accurate evaluation\n",
    "    if optA == zeros(Int64, N)\n",
    "        optV = zeroV\n",
    "    end\n",
    "\n",
    "    return optA, optV, zeroV\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "66a7c3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateVFA! (generic function with 1 method)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#updates ve and vn\n",
    "function updateVFA!(s, substates, target, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, stepsize; s0 = [])\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn; s0 = s0)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += stepsize*(target - currentEst)\n",
    "    end\n",
    "\n",
    "    vn[sn] += stepsize*(target - currentEst)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "d260791b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mcclainStep (generic function with 1 method)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mcclainStep(current, limit)\n",
    "    return current/(1 + current - limit)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "c165d293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateVFA_mcclain! (generic function with 1 method)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateVFA_mcclain!(s, substates, target, ve, vn, veStep, vnStep, N, alpha_d, alpha_r, beta, tau, c0, c1, r, stepsize; s0 = [])\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn; s0 = s0)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += veStep[i, si]*(target - currentEst)\n",
    "        veStep[i, si] = mcclainStep(veStep[i, si], stepsize)\n",
    "    end\n",
    "\n",
    "    vn[sn] += vnStep[sn]*(target - currentEst)\n",
    "    vnStep[sn] = mcclainStep(vnStep[sn], stepsize)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "379e3f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviNewVFA_ST (generic function with 1 method)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses new VFA architecture, e-greedy action selection, and state trace \n",
    "#stepsizeType options: \n",
    "# - varyByNumVisits: uses stepsize b/(b + numVisits)\n",
    "# - varyByIteration: uses stepsize b/(b + n) where n is the iteration modCounter\n",
    "# - constant: uses stepsize b\n",
    "#c is used for calculating epsilon = c/(c + n)\n",
    "#d is used for the stepsize of g\n",
    "function smarviNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c, d; s = [1 for i in 1:N], stateTrace = [], vfaProvided = false, ve = Dict(), vn = Dict(), gActor = 0.0, gCritic, timePassed = 0.0, printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s0 = [1 for i in 1:N]\n",
    "    s3 = [3 for i in 1:N]\n",
    "    gActors = [gActor]\n",
    "    gCritics = [gCritic]\n",
    "    \n",
    "    #initialise ve and vn tables if needed\n",
    "    if !vfaProvided\n",
    "        for i in 1:N\n",
    "            for si in 0:3\n",
    "                ve[i,si] = 0.0\n",
    "            end\n",
    "        end\n",
    "\n",
    "        for i in 0:1\n",
    "            vn[i] = 0.0\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    vs0Hist = []\n",
    "    vs3Hist = []\n",
    "    \n",
    "    #initialise flows\n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    \n",
    "    flows0 = calculateFlows(s0,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    \n",
    "    flows3 = fill(0.0, N)\n",
    "\n",
    "    #do nMax iterations of SMARVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        substates = subStates(s, N, flows)\n",
    "        sis = substates[1]\n",
    "        sn = substates[2]\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action and v value\n",
    "        optAandV = smarActionAndVFromNewVFA_AC(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, gActor, gCritic)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        zeroV = optAandV[3]\n",
    "\n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "\n",
    "        #if random action chosen, choose action action and v value\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N) \n",
    "                optV = zeroV\n",
    "            else \n",
    "                optV = v(s - optA, N, flows, ve, vn)\n",
    "            end \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV \n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            updateVFA!(s, substates, bestV, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "            traceTarget = v(s, N, flows, ve, vn)\n",
    "            for st in stateTrace\n",
    "                updateVFA!(st, subStates(st, N, flows), traceTarget, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "\n",
    "                push!(vs0Hist, v(s0, N, flows0, ve, vn))\n",
    "                push!(vs3Hist, v(s3, N, flows3, ve, vn))\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            gCritic = (gCritic*timePassed + c)/(timePassed + time) #give gCritic full update\n",
    "            timePassed += time\n",
    "            gActor += d*(gCritic - gActor) #smaller update for gActor (1/n style updates might actually make sense in this case)\n",
    "            \n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gActors, gActor)\n",
    "        push!(gCritics, gCritic)\n",
    "\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #return all info\n",
    "    endState = Dict()\n",
    "    endState[\"ve\"]  = ve\n",
    "    endState[\"vn\"] = vn\n",
    "    endState[\"gActor\"] = gActor\n",
    "    endState[\"gCritic\"] = gCritic\n",
    "    endState[\"timePassed\"] = timePassed\n",
    "    endState[\"s\"] = s\n",
    "    endState[\"stateTrace\"] = stateTrace\n",
    "\n",
    "    hist = Dict()\n",
    "    hist[\"vs0\"] = vs0Hist\n",
    "    hist[\"vs3\"] = vs3Hist\n",
    "    hist[\"gCritics\"] = gCritics\n",
    "    hist[\"gActors\"] = gActors\n",
    "\n",
    "    return endState, hist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "fba49dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationNewVFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, ve, vn, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action and v value\n",
    "        optAandV = smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g0; s0 = s0)\n",
    "        optA = optAandV[1]\n",
    "\n",
    "        #update state, flows and g\n",
    "        if optA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - optA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9dd63ad",
   "metadata": {},
   "source": [
    "# SMARPI New VFA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "4f606ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromNewVFA_fa (generic function with 1 method)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromNewVFA_fa(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g; forceZeroVCalc = false, s0 = [])\n",
    "    #find optimal action\n",
    "    optA = faAction(s)\n",
    "    optV = 0.0\n",
    "    zeroV = 0.0\n",
    "    \n",
    "    #force calculation of zeroV if required\n",
    "    if forceZeroVCalc\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        zeroV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContNewVFA(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn; s0 = s0) - g*t\n",
    "    end\n",
    "\n",
    "    #handle different cases for action choice and forceZeroVCalc\n",
    "    if optA == zeros(Int64, N) && forceZeroVCalc\n",
    "        optV = zeroV\n",
    "    elseif optA == zeros(Int64, N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContNewVFA(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn; s0 = s0) - g*t\n",
    "    else\n",
    "        optV = v(s-optA, N, flows, ve, vn; s0 = s0)\n",
    "    end\n",
    "    \n",
    "    #return values based on forceZeroVCalc\n",
    "    if forceZeroVCalc\n",
    "        return optA, optV, zeroV\n",
    "    else\n",
    "        return optA, optV\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "3c1a505d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromNewVFA_DSP (generic function with 1 method)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromNewVFA_DSP(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g, dsPolicy; epsilon = -1.0, forceZeroVCalc = false, s0 = [])\n",
    "    #find action e-soft action\n",
    "    if rand() > epsilon\n",
    "        optA = fill(0,N)\n",
    "        for i in 1:N\n",
    "            if s[i] == 3 && dsPolicy[i] == 1\n",
    "                optA[i] = 1\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    else\n",
    "        optA = randomAction(s, N)\n",
    "    end \n",
    "    \n",
    "    optV = 0.0\n",
    "    zeroV = 0.0\n",
    "    \n",
    "    #force calculation of zeroV if required\n",
    "    if forceZeroVCalc\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        zeroV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContNewVFA(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn; s0 = s0) - g*t\n",
    "    end\n",
    "\n",
    "    #handle different cases for action choice and forceZeroVCalc\n",
    "    if optA == zeros(Int64, N) && forceZeroVCalc\n",
    "        optV = zeroV\n",
    "    elseif optA == zeros(Int64, N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContNewVFA(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn; s0 = s0) - g*t\n",
    "    else\n",
    "        optV = v(s-optA, N, flows, ve, vn; s0 = s0)\n",
    "    end\n",
    "    \n",
    "    #return values based on forceZeroVCalc\n",
    "    if forceZeroVCalc\n",
    "        return optA, optV, zeroV\n",
    "    else\n",
    "        return optA, optV\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "a95985e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeNewVFA_ST_fa (generic function with 1 method)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarpeNewVFA_ST_fa(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b; ve = Dict(), vn = Dict(), vfaProvided = false, g = 0.0, s = [1 for i in 1:N], stateTrace = [], timePassed = 0.0, printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s0 = [1 for i in 1:N]\n",
    "    s3 = [3 for i in 1:N]\n",
    "\n",
    "    flows = zeros(N)\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise ve and vn tables if needed\n",
    "    if !vfaProvided\n",
    "        for i in 1:N\n",
    "            for si in 0:3\n",
    "                ve[i,si] = 0.0\n",
    "            end\n",
    "        end\n",
    "\n",
    "        for i in 0:1\n",
    "            vn[i] = 0.0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    vs0Hist = []\n",
    "    vs3Hist = []\n",
    "\n",
    "    #initialise flows\n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    flows0 = calculateFlows(s0,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    flows3 = fill(0.0, N)\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        substates = subStates(s, N, flows)\n",
    "        sis = substates[1]\n",
    "        sn = substates[2]\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate fully active action and v value\n",
    "        optAandV = smarActionAndVFromNewVFA_fa(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV \n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            updateVFA!(s, substates, bestV, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "            traceTarget = v(s, N, flows, ve, vn)\n",
    "            for st in stateTrace\n",
    "                updateVFA!(st, subStates(st, N, flows), traceTarget, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "\n",
    "                push!(vs0Hist, v(s0, N, flows0, ve, vn))\n",
    "                push!(vs3Hist, v(s3, N, flows3, ve, vn))\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "        \n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            g = (g*timePassed + c)/(timePassed + time)\n",
    "            timePassed += time\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            show(n)\n",
    "            println()\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    endState = Dict()\n",
    "    endState[\"ve\"]  = ve\n",
    "    endState[\"vn\"] = vn\n",
    "    endState[\"g\"] = g\n",
    "    endState[\"timePassed\"] = timePassed\n",
    "    endState[\"s\"] = s\n",
    "    endState[\"stateTrace\"] = stateTrace\n",
    "\n",
    "    hist = Dict()\n",
    "    hist[\"vs0\"] = vs0Hist\n",
    "    hist[\"vs3\"] = vs3Hist\n",
    "    hist[\"g\"] = gs\n",
    "\n",
    "    return endState, hist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "2abc0b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeNewVFA_ST_DSP (generic function with 1 method)"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarpeNewVFA_ST_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, dsPolicy; epsilon = -1.0, ve = Dict(), vn = Dict(), vfaProvided = false, g = 0.0, gFixed = false, s = [1 for i in 1:N], stateTrace = [], timePassed = 0.0, printProgress = false, modCounter = 100000, printStates = false)\n",
    "    #initialise\n",
    "    s0 = [1 for i in 1:N]\n",
    "    s3 = [3 for i in 1:N]\n",
    "\n",
    "    flows = zeros(N)\n",
    "    gs = [g]\n",
    "    gInit = g\n",
    "    #initialise ve and vn tables if needed\n",
    "    if !vfaProvided\n",
    "        for i in 1:N\n",
    "            for si in 0:3\n",
    "                ve[i,si] = 0.0\n",
    "            end\n",
    "        end\n",
    "\n",
    "        for i in 0:1\n",
    "            vn[i] = 0.0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    vs0Hist = []\n",
    "    vs3Hist = []\n",
    "\n",
    "    #initialise flows\n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    flows0 = calculateFlows(s0,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    flows3 = fill(0.0, N)\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits \n",
    "        substates = subStates(s, N, flows)\n",
    "        push!(stateTrace, s)\n",
    "        \n",
    "        if gFixed\n",
    "            optAandV = smarActionAndVFromNewVFA_DSP(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, gInit, dsPolicy; epsilon = epsilon)\n",
    "        else\n",
    "            optAandV = smarActionAndVFromNewVFA_DSP(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g, dsPolicy; epsilon = epsilon)\n",
    "        end\n",
    "        \n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV \n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            updateVFA!(s, substates, bestV, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "            traceTarget = v(s, N, flows, ve, vn)\n",
    "            for st in stateTrace\n",
    "                updateVFA!(st, subStates(st, N, flows), traceTarget, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "                push!(vs0Hist, v(s0, N, flows0, ve, vn))\n",
    "                push!(vs3Hist, v(s3, N, flows3, ve, vn))\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "        \n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            g = (g*timePassed + c)/(timePassed + time)\n",
    "            timePassed += time\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "\n",
    "        if printStates \n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    endState = Dict()\n",
    "    endState[\"ve\"]  = ve\n",
    "    endState[\"vn\"] = vn\n",
    "    endState[\"g\"] = g\n",
    "    endState[\"timePassed\"] = timePassed\n",
    "    endState[\"s\"] = s\n",
    "    endState[\"stateTrace\"] = stateTrace\n",
    "\n",
    "    hist = Dict()\n",
    "    hist[\"vs0\"] = vs0Hist\n",
    "    hist[\"vs3\"] = vs3Hist\n",
    "    hist[\"g\"] = gs\n",
    "\n",
    "    return endState, hist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "86bcac58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeNewVFA_ST_DSP_mcclain (generic function with 1 method)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarpeNewVFA_ST_DSP_mcclain(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, minStep, dsPolicy; epsilon = -1.0, ve = Dict(), vn = Dict(), veStep = Dict(), vnStep = Dict(), vfaProvided = false, g = 0.0, gFixed = false, s = [1 for i in 1:N], stateTrace = [], timePassed = 0.0, printProgress = false, modCounter = 100000, printStates = false)\n",
    "    #initialise\n",
    "    s0 = [1 for i in 1:N]\n",
    "    s3 = [3 for i in 1:N]\n",
    "\n",
    "    flows = zeros(N)\n",
    "    gs = [g]\n",
    "    gInit = g\n",
    "    #initialise ve and vn tables if needed\n",
    "    if !vfaProvided\n",
    "        for i in 1:N\n",
    "            for si in 0:3\n",
    "                ve[i,si] = 0.0\n",
    "                veStep[i, si] = 1.0\n",
    "            end\n",
    "        end\n",
    "\n",
    "        for i in 0:1\n",
    "            vn[i] = 0.0\n",
    "            vnStep[i] = 1.0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    vs0Hist = []\n",
    "    vs3Hist = []\n",
    "\n",
    "    #initialise flows\n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    flows0 = calculateFlows(s0,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    flows3 = fill(0.0, N)\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits \n",
    "        substates = subStates(s, N, flows)\n",
    "        push!(stateTrace, s)\n",
    "        \n",
    "        if gFixed\n",
    "            optAandV = smarActionAndVFromNewVFA_DSP(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, gInit, dsPolicy; epsilon = epsilon)\n",
    "        else\n",
    "            optAandV = smarActionAndVFromNewVFA_DSP(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g, dsPolicy; epsilon = epsilon)\n",
    "        end\n",
    "        \n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV \n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            updateVFA_mcclain!(s, substates, bestV, ve, vn, veStep, vnStep, N, alpha_d, alpha_r, beta, tau, c0, c1, r, minStep)\n",
    "            traceTarget = v(s, N, flows, ve, vn)\n",
    "            for st in stateTrace\n",
    "                updateVFA_mcclain!(st, subStates(st, N, flows), traceTarget, ve, vn, veStep, vnStep, N, alpha_d, alpha_r, beta, tau, c0, c1, r, minStep)\n",
    "                push!(vs0Hist, v(s0, N, flows0, ve, vn))\n",
    "                push!(vs3Hist, v(s3, N, flows3, ve, vn))\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "        \n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            g = (g*timePassed + c)/(timePassed + time)\n",
    "            timePassed += time\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "\n",
    "        if printStates \n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    endState = Dict()\n",
    "    endState[\"ve\"]  = ve\n",
    "    endState[\"vn\"] = vn\n",
    "    endState[\"g\"] = g\n",
    "    endState[\"timePassed\"] = timePassed\n",
    "    endState[\"s\"] = s\n",
    "    endState[\"stateTrace\"] = stateTrace\n",
    "\n",
    "    hist = Dict()\n",
    "    hist[\"vs0\"] = vs0Hist\n",
    "    hist[\"vs3\"] = vs3Hist\n",
    "    hist[\"g\"] = gs\n",
    "\n",
    "    return endState, hist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "75cd5870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeNewVFA_fa (generic function with 1 method)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarpeNewVFA_fa(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, d; ve = Dict(), vn = Dict(), vfaProvided = false, g = 0.0, s = [1 for i in 1:N], stepsizeType = \"constant\", printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s0 = [1 for i in 1:N]\n",
    "    s3 = [3 for i in 1:N]\n",
    "\n",
    "    flows = zeros(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise ve and vn tables\n",
    "    numVisitsE = Dict()\n",
    "    for i in 1:N\n",
    "        for si in 0:3\n",
    "            if !vfaProvided\n",
    "                ve[i,si] = 0.0\n",
    "            end\n",
    "            numVisitsE[i,si] = 0\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    numVisitsN = Dict()\n",
    "    for i in 0:1\n",
    "        if !vfaProvided\n",
    "            vn[i] = 0.0\n",
    "        end\n",
    "        numVisitsN[i] = 0\n",
    "    end\n",
    "\n",
    "    numVisitsG = 0\n",
    "    vs0Hist = [0.0]\n",
    "    vs3Hist = [0.0]\n",
    "\n",
    "    #initialise flows\n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    flows0 = calculateFlows(s0,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    flows3 = fill(0.0, N)\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        substates = subStates(s, N, flows)\n",
    "        sis = substates[1]\n",
    "        sn = substates[2]\n",
    "        for i in 1:N\n",
    "            numVisitsE[i,sis[i]] += 1\n",
    "        end\n",
    "\n",
    "        numVisitsN[sn] += 1\n",
    "\n",
    "        #formulate fully active action and v value\n",
    "        optAandV = smarActionAndVFromNewVFA_fa(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV \n",
    "        \n",
    "        #update VFA\n",
    "        if stepsizeType == \"varyByNumVisits\"\n",
    "            ve,vn = updateVFA(s, substates, bestV, ve, vn, numVisitsE, numVisitsN, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "        elseif stepsizeType == \"constant\"\n",
    "            ve,vn = updateVFA(s, substates, bestV, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "        elseif stepsizeType == \"varyByIteration\"\n",
    "            ve,vn = updateVFA(s, substates, bestV, ve, vn, n, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "        else\n",
    "            println(\"Invalid stepsize rule\")\n",
    "            return 0\n",
    "        end\n",
    "\n",
    "        #add new estimates of v(s0) and v(s3) to their lists\n",
    "        push!(vs0Hist, v(s0, N, flows0, ve, vn))\n",
    "        push!(vs3Hist, v(s3, N, flows3, ve, vn))\n",
    "\n",
    "\n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont2(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            numVisitsG += 1\n",
    "            g = runningTotal/timePassed #use basic calculation for g due to stationary policy (could be replaced by slower version to reduce noise)\n",
    "            \n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            show(n)\n",
    "            println()\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return ve, vn, g, gs, vs0Hist, vs3Hist, s, stateTrace\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "c4a9d774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normaliseVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function normaliseVFA!(ve, vn, substates0)\n",
    "    sis0 = substates0[1]\n",
    "    sn0 = substates0[2]\n",
    "    for i in 1:N \n",
    "        i0 = sis0[i]\n",
    "        for ie in 0:3\n",
    "            ve[i,ie] -= ve[i, i0]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    vn[0] -= vn[sn0]\n",
    "    vn[1] -= vn[sn0]\n",
    "    \n",
    "end\n",
    "\n",
    "function normaliseVFA(ve,vn,substates0)\n",
    "    veC = copy(ve)\n",
    "    vnC = copy(vn)\n",
    "\n",
    "    normaliseVFA!(veC, vnC, substates0)\n",
    "\n",
    "    return ve, vn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "583177a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromNewVFA_PI (generic function with 1 method)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromNewVFA_PI(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve_in, vn_in, g_in, ve, vn, g; s0 = [])\n",
    "    #find optimal action\n",
    "    optA = smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve_in, vn_in, g_in; s0 = s0)[1]\n",
    "    optV = 0.0\n",
    "    if optA == zeros(Int64, N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContNewVFA(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn; s0 = s0) - g*t\n",
    "    else\n",
    "        optV = v(s - optA, N, flows, ve, vn; s0 = s0)\n",
    "    end\n",
    "\n",
    "    return optA, optV\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "8cebaeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpiNewVFA_ST (generic function with 1 method)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarpiNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, ve_in, vn_in, g_in, b; vfaProvided = false, ve = Dict(), vn = Dict(), g = 0.0, timePassed = 0.0, s = [1 for i in 1:N], stateTrace = [], stepsizeType = \"constant\", printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s0 = [1 for i in 1:N]\n",
    "    s3 = [3 for i in 1:N]\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise ve and vn tables\n",
    "    if !vfaProvided\n",
    "        for i in 1:N\n",
    "            for si in 0:3\n",
    "                ve[i,si] = 0.0\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        for i in 0:1\n",
    "            vn[i] = 0.0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    vs0Hist = []\n",
    "    vs3Hist = []\n",
    "\n",
    "    #initialise flows\n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    flows0 = calculateFlows(s0,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    flows3 = fill(0.0, N)\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        substates = subStates(s, N, flows)\n",
    "        sis = substates[1]\n",
    "        sn = substates[2]\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action and v value\n",
    "        optAandV = smarActionAndVFromNewVFA_PI(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve_in, vn_in, g_in, ve, vn, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV \n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            updateVFA!(s, substates, bestV, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "            traceTarget = v(s, N, flows, ve, vn)\n",
    "            for st in stateTrace\n",
    "                updateVFA!(st, subStates(st, N, flows), traceTarget, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "\n",
    "                push!(vs0Hist, v(s0, N, flows0, ve, vn))\n",
    "                push!(vs3Hist, v(s3, N, flows3, ve, vn))\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            g = (g*timePassed + c)/(timePassed + time)\n",
    "            timePassed += time\n",
    "            \n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #Wrap final internal state of algorithm in dictionary, and history of estimates for plots in another\n",
    "    endState = Dict()\n",
    "    endState[\"ve\"]  = ve\n",
    "    endState[\"vn\"] = vn\n",
    "    endState[\"g\"] = g\n",
    "    endState[\"timePassed\"] = timePassed\n",
    "    endState[\"s\"] = s\n",
    "    endState[\"stateTrace\"] = stateTrace\n",
    "\n",
    "    hist = Dict()\n",
    "    hist[\"vs0\"] = vs0Hist\n",
    "    hist[\"vs3\"] = vs3Hist\n",
    "    hist[\"g\"] = gs\n",
    "\n",
    "    return endState, hist\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "509e852e",
   "metadata": {},
   "source": [
    "# New Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "a990c3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "staticPolicyToDynamicPolicyLAS (generic function with 1 method)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function staticPolicyToDynamicPolicy(N, static)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    passiveAction = fill(0, N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        a = copy(passiveAction)\n",
    "        for i in 1:N\n",
    "            if s[i] == 3 && static[i] == 1\n",
    "                a[i] = 1\n",
    "            end\n",
    "        end\n",
    "\n",
    "        policy[s] = a\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end\n",
    "\n",
    "function staticPolicyToDynamicPolicyLAS(N, static)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    passiveAction = fill(0, N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        a = copy(passiveAction)\n",
    "        for i in 1:N\n",
    "            if s[i] == 3 && static[i] == 1\n",
    "                a[i] = 1\n",
    "                break \n",
    "            end\n",
    "        end\n",
    "\n",
    "        policy[s] = a\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "eb55928a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bestRIHeuristic (generic function with 1 method)"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try each removal/insertion in order, return the best improvement\n",
    "function bestRIHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r, dsp)\n",
    "    changeFlag = false\n",
    "    currentG = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, dsp)\n",
    "    bestDsp = dsp\n",
    "    bestG = currentG\n",
    "    for i in 1:N\n",
    "        newDsp = copy(dsp)\n",
    "        newDsp[i] = 1 - newDsp[i] #flip policy\n",
    "        thisG = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, newDsp)\n",
    "        if thisG < bestG\n",
    "            changeFlag = true\n",
    "            bestG = thisG\n",
    "            bestDsp = newDsp            \n",
    "        end\n",
    "    end\n",
    "\n",
    "    \n",
    "    return bestDsp, bestG, changeFlag\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "f018a216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bestSwapHeuristic (generic function with 1 method)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try all possible swaps, return the best improvement\n",
    "function bestSwapHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r, dsp)\n",
    "    changeFlag = false\n",
    "    currentG = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, dsp)\n",
    "    bestDsp = dsp\n",
    "    bestG = currentG\n",
    "    for i in 1:(N - 1)\n",
    "        for j in (i+1):N\n",
    "            if dsp[i]!=dsp[j]\n",
    "                newDsp = copy(dsp)\n",
    "                newDsp[i] = 1 - newDsp[i] #flip policy\n",
    "                newDsp[j] = 1 - newDsp[j]\n",
    "                thisG = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, newDsp)\n",
    "                if thisG < bestG\n",
    "                    changeFlag = true\n",
    "                    bestG = thisG\n",
    "                    bestDsp = newDsp  \n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return bestDsp, bestG, changeFlag\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "b0fc522f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dspHeuristic (generic function with 1 method)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeatedly apply Best RI Heuristic until policy cannot improve\n",
    "function dspHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r; dsp = fill(0, N))\n",
    "    changeFlag = true\n",
    "    while changeFlag\n",
    "        dsp, g, changeFlag = bestRIHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r, dsp)\n",
    "    end\n",
    "    return dsp, g\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "cbda6682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dspHeuristicWithSwaps (generic function with 1 method)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeatly apply Best RI Heuristic until policy cannot improve, then try all swaps. If a swap improves the policy, repeat the process\n",
    "function dspHeuristicWithSwaps(N, alpha_d, alpha_r, beta, tau, c0, c1, r; dsp = fill(0, N))\n",
    "    changeFlag = true\n",
    "    counter = 0\n",
    "    g = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, dsp)\n",
    "    while changeFlag\n",
    "        while changeFlag\n",
    "            dsp, g, changeFlag = bestRIHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r, dsp)\n",
    "            counter += 1\n",
    "        end\n",
    "        dsp, g, changeFlag = bestSwapHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r, dsp)\n",
    "        counter += 1\n",
    "    end\n",
    "    \n",
    "    return dsp, g, counter\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "ebf91771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bestThreeHeuristic (generic function with 1 method)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bestThreeHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    bestDsp = fill(0, N)\n",
    "    bestG = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, bestDsp)\n",
    "    for i in 1:(N - 2)\n",
    "        for j in (i + 1):(N - 1)\n",
    "            for k in (j + 1):N\n",
    "                thisDsp = fill(0, N)\n",
    "                thisDsp[i] = 1\n",
    "                thisDsp[j] = 1\n",
    "                thisDsp[k] = 1\n",
    "                thisG = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, thisDsp)\n",
    "\n",
    "                if thisG < bestG\n",
    "                    bestDsp = thisDsp\n",
    "                    bestG = thisG\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "    return bestDsp, bestG\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "f2f13841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bestTwoHeuristic (generic function with 1 method)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bestTwoHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    bestDsp = fill(0, N)\n",
    "    bestG = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, bestDsp)\n",
    "    for i in 1:(N - 1)\n",
    "        for j in (i + 1):N\n",
    "            thisDsp = fill(0, N)\n",
    "            thisDsp[i] = 1\n",
    "            thisDsp[j] = 1\n",
    "            thisG = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, thisDsp)\n",
    "\n",
    "            if thisG < bestG\n",
    "                bestDsp = thisDsp\n",
    "                bestG = thisG\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "    return bestDsp, bestG\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "9462942c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "best123Heuristic (generic function with 1 method)"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function best123Heuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    #initialise with passive policy\n",
    "    zeroG = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, fill(0, N))\n",
    "    gs = [zeroG]\n",
    "    dsps = [fill(0, N)]\n",
    "\n",
    "    #best one link policy\n",
    "    oneDsp = bestRIHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r, fill(0, N))[1]\n",
    "    oneG = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, oneDsp)\n",
    "    push!(gs, oneG)\n",
    "    push!(dsps, oneDsp)\n",
    "\n",
    "    #best two link policy\n",
    "    twoDsp, twoG = bestTwoHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    push!(gs, twoG)\n",
    "    push!(dsps, twoDsp)\n",
    "\n",
    "    #best three link policy\n",
    "    (threeDsp, threeG) = bestThreeHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    push!(gs, threeG)\n",
    "    push!(dsps, threeDsp)\n",
    "\n",
    "    bestDsp = dsps[1]\n",
    "    bestG = gs[1]\n",
    "    for i in 2:4\n",
    "        if gs[i] < bestG\n",
    "            bestDsp = dsps[i]\n",
    "            bestG = gs[i]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return bestDsp, bestG\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "04c29e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jointHeuristic (generic function with 1 method)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function jointHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    startDsp, startG = best123Heuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    return dspHeuristicWithSwaps(N, alpha_d, alpha_r, beta, tau, c0, c1, r; dsp = startDsp)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8d7c556",
   "metadata": {},
   "source": [
    "# Tests (Redo Tests to check for consistency and errors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3564b9b3",
   "metadata": {},
   "source": [
    "## Homog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1\n",
    "N = 5\n",
    "alpha_d = 0.0\n",
    "alpha_r = 0.1\n",
    "beta = 10.0\n",
    "c0 = 1.0\n",
    "c1 = 100.0\n",
    "r = 100.0\n",
    "\n",
    "epsilon = 0.0001\n",
    "res = rviHomog(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = N, forceActive = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "resG = calculateG_SSP_Homog(N,alpha_d, alpha_r, beta, fill(1.0, N), c0, c1, r, 1e10, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c643df30",
   "metadata": {},
   "source": [
    "## Inhomog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "6bcd6b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504.2142857142852, Dict{Any, Any}([2, 1, 3, 1, 1] => 247.6407721247002, [2, 3, 1, 2, 3] => 483.54669097951034, [2, 1, 2, 2, 2] => 514.317908886486, [3, 2, 1, 3, 3] => 428.31873820623764, [3, 2, 2, 2, 1] => 607.1799501476754, [3, 2, 2, 1, 3] => 518.130173754816, [1, 2, 3, 2, 3] => 254.2795290788786, [1, 2, 1, 1, 2] => 190.46946058901548, [1, 2, 2, 3, 1] => 218.9681079502508, [3, 1, 3, 3, 3] => 324.1621055531764…), 303, Dict{Any, Any}([2, 1, 3, 1, 1] => [0, 0, 0, 0, 0], [2, 3, 1, 2, 3] => [0, 0, 0, 0, 0], [2, 1, 2, 2, 2] => [0, 0, 0, 0, 0], [3, 2, 1, 3, 3] => [1, 0, 0, 0, 0], [3, 2, 2, 2, 1] => [1, 0, 0, 0, 0], [3, 2, 2, 1, 3] => [1, 0, 0, 0, 0], [1, 2, 3, 2, 3] => [0, 0, 0, 0, 0], [1, 2, 1, 1, 2] => [0, 0, 0, 0, 0], [1, 2, 2, 3, 1] => [0, 0, 0, 0, 0], [3, 1, 3, 3, 3] => [1, 0, 0, 0, 0]…))"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "tau = fill(1.0, N)\n",
    "alpha_d = fill(0.0, N)\n",
    "alpha_r = [1.0*i for i in 1:N]\n",
    "beta = 10.0\n",
    "c0 = [1.0*i for i in 1:N]\n",
    "c1 = 100.0\n",
    "r = [100.0*i for i in 1:N]\n",
    "\n",
    "epsilon = 1e-20\n",
    "resInhomog = rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = true, modCounter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece9ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tauArr = fill(1.0, N)\n",
    "lambda = fill(0.0, N)\n",
    "lambda[1:2] .= 1e200\n",
    "\n",
    "resInhomG = calculateG_SSP(N,alpha_d, alpha_r, beta, tauArr, c0, c1, r, lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a89cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateG_DSP(N,alpha_d, alpha_r, beta, tauArr, c0, c1, r, [1,1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "e724ba97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524.9999999999993, Dict{Any, Any}([2, 1, 3, 1, 1] => 223.2511606041013, [2, 3, 1, 2, 3] => 469.56979104773166, [2, 1, 2, 2, 2] => 489.5666863093327, [3, 2, 1, 3, 3] => 406.07373155167215, [3, 2, 2, 2, 1] => 583.1312761312753, [3, 2, 2, 1, 3] => 494.63592861386905, [1, 2, 3, 2, 3] => 249.21063631357742, [1, 2, 1, 1, 2] => 190.1410946080063, [1, 2, 2, 3, 1] => 215.69577154871268, [3, 1, 3, 3, 3] => 297.67095377389444…), 341)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = staticPolicyToDynamicPolicyLAS(N, [1,1,0,0,0])\n",
    "resPE = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = true, modCounter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resPI = rpiExact(N, resPE[2], alpha_d, alpha_r, beta, tau, c0, c1, r, 1e-10; nMax = 50000, delScale = 1, printProgress = true, modCounter = 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da388edd",
   "metadata": {},
   "source": [
    "## SMARPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nMax = 1000000\n",
    "b = 1e-2\n",
    "dsPolicy = [1,1,0,0,0]\n",
    "resultSMARPE = smarpeNewVFA_ST_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, dsPolicy; epsilon = 0.0, ve = Dict(), vn = Dict(), vfaProvided = false, g = 525.0, gFixed = true, s = [1 for i in 1:N], stateTrace = [], timePassed = 0.0, printProgress = true, modCounter = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58474eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = serialize(\"resultSMARPE.dat\", resultSMARPE[1])\n",
    "f = serialize(\"resultSMARPE.big\", resultSMARPE[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4039403",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultSMARPE_S = deserialize(\"resultSMARPE.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160aa65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(resultSMARPE[2][\"vs0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e04a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(resultSMARPE[2][\"vs3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b985ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantStates = []\n",
    "stateSpace = enumerateStates(N)\n",
    "for s in stateSpace\n",
    "    if s[3] == 3 && s[4] == 3 && s[5] == 3\n",
    "        push!(importantStates, s)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce8087",
   "metadata": {},
   "outputs": [],
   "source": [
    "ve = resultSMARPE_S[\"ve\"]\n",
    "vn = resultSMARPE_S[\"vn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trueValues = []\n",
    "approxValues = []\n",
    "for s in importantStates\n",
    "    push!(trueValues, resPE[2][s])\n",
    "    push!(approxValues, v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d41de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = mean(trueValues - approxValues)\n",
    "PyPlot.plot(trueValues .- gap, label = \"True\")\n",
    "PyPlot.plot(approxValues, label = \"Approx\")\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nMax = 1000000\n",
    "gs = gEvaluationDSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, [1,1,0,0,0]; s = fill(3, N), epsilon = 0.01, printProgress = true, modCounter = 100000, stateTrace = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "nMax = 10000000\n",
    "b = 1e-2\n",
    "dsPolicy = [1,1,0,0,0]\n",
    "resultSMARPEeSoft = smarpeNewVFA_ST_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, dsPolicy; epsilon = 0.01, ve = Dict(), vn = Dict(), vfaProvided = false, g = 528.0, gFixed = true, s = fill(3,N), stateTrace = [], timePassed = 0.0, printProgress = true, modCounter = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fda241",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = serialize(\"resultSMARPEesoft.dat\", resultSMARPEeSoft[1])\n",
    "f = serialize(\"resultSMARPEesoft.big\", resultSMARPEeSoft[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultSMARPEeSoft_S = deserialize(\"resultSMARPEesoft.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09302f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1\n",
    "finish = nMax - 2\n",
    "PyPlot.plot(resultSMARPEeSoft[2][\"vs0\"][start:finish])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(resultSMARPEeSoft[2][\"vs3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(resultSMARPEeSoft[2][\"vs3\"] - resultSMARPEeSoft[2][\"vs0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ve = resultSMARPEeSoft_S[\"ve\"]\n",
    "vn = resultSMARPEeSoft_S[\"vn\"]\n",
    "trueValues = []\n",
    "approxValues = []\n",
    "for s in importantStates\n",
    "    push!(trueValues, resPE[2][s])\n",
    "    push!(approxValues, v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = mean(trueValues - approxValues)\n",
    "PyPlot.plot(trueValues .- gap, label = \"True\")\n",
    "PyPlot.plot(approxValues, label = \"Approx\")\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7299acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(trueValues - approxValues .- gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e706ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalNMax = 2000000\n",
    "ve = resultSMARPEeSoft_S[\"ve\"]\n",
    "vn = resultSMARPEeSoft_S[\"vn\"]\n",
    "gsPI = gEvaluationNewVFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, evalNMax, ve, vn, 528.3, printProgress = true, modCounter = 100000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e19adb3c",
   "metadata": {},
   "source": [
    "Looks like it all works!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76e0ead9",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67433e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "numRuns = 1000\n",
    "N = 5\n",
    "alpha_d = fill(0.0, N)\n",
    "beta = 1.0\n",
    "epsilon = 1e-10\n",
    "LEN_FRAME = 125\n",
    "TAU_MAX = 1.0\n",
    "C0_MAX = 1.0\n",
    "C1_MAX = 1000\n",
    "R_MAX = 100\n",
    "nMax = 10000\n",
    "delScale = 1\n",
    "gaps = []\n",
    "for run in 1:numRuns\n",
    "    #Start frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "    print(\"Run: \")\n",
    "    println(run)\n",
    "    #generate parameters\n",
    "    tau = rand(N).*TAU_MAX #\n",
    "    print(\"tau: \")\n",
    "    println([round(tau[i], digits = 5) for i in 1:N])\n",
    "\n",
    "    alpha_r = rand(N).*tau\n",
    "    print(\"alpha_r: \")\n",
    "    println([round(alpha_r[i], digits = 5) for i in 1:N])\n",
    "\n",
    "    c0 = rand(N).*C0_MAX\n",
    "    print(\"c0: \")\n",
    "    println([round(c0[i], digits = 5) for i in 1:N])\n",
    "\n",
    "    c1 = C0_MAX + rand()*(C1_MAX - C0_MAX)\n",
    "    print(\"c1: \")\n",
    "    println(round(c1, digits = 5))\n",
    "\n",
    "    r = rand(N).*R_MAX\n",
    "    print(\"r: \")\n",
    "    println([round(r[i], digits = 5) for i in 1:N])\n",
    "\n",
    "    #find true optimal solution\n",
    "    resInhomog = rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    print(\"RVI: \")\n",
    "    print(round(resInhomog[1], digits = 5))\n",
    "    print(\" (Iterations: \")\n",
    "    print(resInhomog[3])\n",
    "    print(\")    \")\n",
    "\n",
    "    #find best DSP solution\n",
    "    dsps = enumerateActions(N)\n",
    "    bestDSP = dsps[1]\n",
    "    bestDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, bestDSP)\n",
    "    for i in 2:(2^N)\n",
    "        thisDSP = dsps[i]\n",
    "        thisDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, thisDSP)\n",
    "        if thisDSP_G < bestDSP_G\n",
    "            bestDSP = thisDSP\n",
    "            bestDSP_G = thisDSP_G\n",
    "        end\n",
    "    end\n",
    "\n",
    "    print(\"DSP Exact: \")\n",
    "    print(round(bestDSP_G, digits = 5))\n",
    "    print(\",    \")\n",
    "\n",
    "    #find PI on best DSP\n",
    "    policy = staticPolicyToDynamicPolicyLAS(N, bestDSP)\n",
    "    resPE = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    print(\"DSP LAS: \")\n",
    "    print(round(resPE[1], digits = 5))\n",
    "    print(\" (Iterations: \")\n",
    "    print(resPE[3])\n",
    "    print(\"),   \")\n",
    "    resPI = rpiExact(N, resPE[2], alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    print(\"PIH: \")\n",
    "    print(round(resPI[1], digits = 5))\n",
    "    print(\" (Iterations: \")\n",
    "    print(resPI[3])\n",
    "    println(\").\")\n",
    "    print(\"Optimality Gap: \")\n",
    "    gap = (1 - resInhomog[1]/resPI[1])*100\n",
    "    push!(gaps, gap)\n",
    "    print(round(gap, digits = 2))\n",
    "    println(\"%.\")\n",
    "    #end frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d164114",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b96b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "gapsPrime = [((1/(1 - (g/100)))-1)*100 for g in gaps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95556757",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(gapsPrime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(g < 1e-10 for g in gapsPrime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(g < 1.35 for g in gapsPrime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(gapsPrime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(gapsPrime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = serialize(\"gaps1.dat\", gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156513de",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sampleTruncExp(mean, upper)\n",
    "    while true\n",
    "        s = rand(Exponential(mean))\n",
    "        if s < upper\n",
    "            return s\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de3aab5a",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325540f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numRuns = 1000\n",
    "N = 5\n",
    "alpha_d = fill(0.0, N)\n",
    "beta = 1.0\n",
    "epsilon = 1e-10\n",
    "LEN_FRAME = 125\n",
    "TAU_MAX = 1.0\n",
    "C0_MAX = 1.0\n",
    "C1_MAX = 1000\n",
    "R_MAX = 100\n",
    "nMax = 10000\n",
    "delScale = 1\n",
    "taus = []\n",
    "alphas = []\n",
    "c0s = []\n",
    "c1s = []\n",
    "rs = []\n",
    "rviGs = []\n",
    "dsps = []\n",
    "dspGs = []\n",
    "dspPeGs = []\n",
    "piGs = []\n",
    "gaps2 = []\n",
    "for run in 1:numRuns\n",
    "    #Start frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "    print(\"Run: \")\n",
    "    println(run)\n",
    "    #generate parameters\n",
    "    tau = rand(N).*TAU_MAX #\n",
    "    print(\"tau: \")\n",
    "    println([round(tau[i], digits = 5) for i in 1:N])\n",
    "    push!(taus, tau)\n",
    "\n",
    "    alpha_r = [sampleTruncExp(tau[i]/2, tau[i]) for i in 1:N]\n",
    "    print(\"alpha_r: \")\n",
    "    println([round(alpha_r[i], digits = 5) for i in 1:N])\n",
    "    push!(alphas, alpha_r)\n",
    "\n",
    "    c0 = rand(N).*C0_MAX\n",
    "    c0 = sort(c0)\n",
    "    print(\"c0: \")\n",
    "    println([round(c0[i], digits = 5) for i in 1:N])\n",
    "    push!(c0s, c0)\n",
    "\n",
    "    c1 = C0_MAX + rand()*(C1_MAX - C0_MAX)\n",
    "    print(\"c1: \")\n",
    "    println(round(c1, digits = 5))\n",
    "    push!(c1s, c1)\n",
    "\n",
    "    r = rand(N).*R_MAX\n",
    "    print(\"r: \")\n",
    "    println([round(r[i], digits = 5) for i in 1:N])\n",
    "    push!(rs, r)\n",
    "\n",
    "    #find true optimal solution\n",
    "    resInhomog = rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    push!(rviGs, resInhomog[1])\n",
    "    print(\"RVI: \")\n",
    "    print(round(resInhomog[1], digits = 5))\n",
    "    print(\" (Iterations: \")\n",
    "    print(resInhomog[3])\n",
    "    print(\")    \")\n",
    "\n",
    "    #find best DSP solution\n",
    "    dsps = enumerateActions(N)\n",
    "    bestDSP = dsps[1]\n",
    "    bestDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, bestDSP)\n",
    "    for i in 2:(2^N)\n",
    "        thisDSP = dsps[i]\n",
    "        thisDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, thisDSP)\n",
    "        if thisDSP_G < bestDSP_G\n",
    "            bestDSP = thisDSP\n",
    "            bestDSP_G = thisDSP_G\n",
    "        end\n",
    "    end\n",
    "    push!(dsps, bestDSP)\n",
    "    push!(dspGs, bestDSP_G)\n",
    "    print(\"DSP Exact: \")\n",
    "    print(round(bestDSP_G, digits = 5))\n",
    "    print(\",    \")\n",
    "\n",
    "    #find PI on best DSP\n",
    "    policy = staticPolicyToDynamicPolicyLAS(N, bestDSP)\n",
    "    resPE = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    push!(dspPeGs, resPE[1])\n",
    "    print(\"DSP LAS: \")\n",
    "    print(round(resPE[1], digits = 5))\n",
    "    print(\" (Iterations: \")\n",
    "    print(resPE[3])\n",
    "    print(\"),   \")\n",
    "    resPI = rpiExact(N, resPE[2], alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    push!(piGs, resPI[1])\n",
    "    print(\"PIH: \")\n",
    "    print(round(resPI[1], digits = 5))\n",
    "    print(\" (Iterations: \")\n",
    "    print(resPI[3])\n",
    "    println(\").\")\n",
    "    print(\"DSP: \")\n",
    "    println(bestDSP)\n",
    "    print(\"Optimality Gap: \")\n",
    "    gap = (resPI[1]/resInhomog[1] - 1)*100\n",
    "    push!(gaps2, gap)\n",
    "    print(round(gap, digits = 2))\n",
    "    println(\"%.\")\n",
    "    #end frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultExperiment = Dict()\n",
    "resultExperiment[\"tau\"] = taus\n",
    "resultExperiment[\"alpha\"] = alphas\n",
    "resultExperiment[\"c0\"] = c0s\n",
    "resultExperiment[\"c1\"] = c1s\n",
    "resultExperiment[\"r\"] = rs \n",
    "resultExperiment[\"rvi\"] = rviGs\n",
    "resultExperiment[\"dsp\"] = dsps\n",
    "resultExperiment[\"dsp_g\"] = dspGs\n",
    "resultExperiment[\"dsp_pe\"] = dspPeGs\n",
    "resultExperiment[\"pi\"] = piGs\n",
    "resultExperiment[\"gaps\"] = gaps2\n",
    "f = serialize(\"experiment2.big\", resultExperiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ebbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(gaps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(gaps2, xlim = (0,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(gaps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "median(gaps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(gaps2[i] <= 10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile(gaps2, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ffd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(gaps2[i] < 1e-10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspGaps = (dspGs./rviGs .- 1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(dspGaps, xlim = (0,1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "median(dspGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps[i] < 1e-10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faed79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile(dspGaps, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc22cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "improvementPP = dspGaps - gaps2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c000a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist([i for i in improvementPP if i < 250], bins = [10*i for i in 0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb51c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(improvementPP, xlim = (0,350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [(dspGaps[i] - gaps2[i])/dspGaps[i] for i in 1:1000 if dspGaps[i] > 1e-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9841ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7514ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [s*(s >= 0 && s <= 1) + (s > 1) for s in score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07756cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(score, xlim = (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(score, bins = [0.05*i for i in 0:(ceil(Int64, 1/0.05) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "median(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7490db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b9acef2",
   "metadata": {},
   "source": [
    "## Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "numRuns = 1000\n",
    "N = 20\n",
    "alpha_d = fill(0.0, N)\n",
    "beta = 1.0\n",
    "epsilon = 1e-10\n",
    "LEN_FRAME = 75\n",
    "TAU_MAX = 1.0\n",
    "C0_MAX = 1.0\n",
    "C1_MAX = 1000\n",
    "R_MAX = 100\n",
    "nMax = 10000\n",
    "delScale = 1\n",
    "numSame = 0\n",
    "#taus = []\n",
    "#alphas = []\n",
    "#c0s = []\n",
    "#c1s = []\n",
    "#rs = []\n",
    "#rviGs = []\n",
    "#dsps = []\n",
    "#dspGs = []\n",
    "#dspPeGs = []\n",
    "#piGs = []\n",
    "#gaps2 = []\n",
    "for run in 1:numRuns\n",
    "    #Start frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "    print(\"Run: \")\n",
    "    println(run)\n",
    "    #generate parameters\n",
    "    tau = rand(N).*TAU_MAX \n",
    "    print(\"tau: \")\n",
    "    println([round(tau[i], digits = 5) for i in 1:N])\n",
    "    #push!(taus, tau)\n",
    "\n",
    "    alpha_r = [sampleTruncExp(tau[i]/2, tau[i]) for i in 1:N]\n",
    "    print(\"alpha_r: \")\n",
    "    println([round(alpha_r[i], digits = 5) for i in 1:N])\n",
    "    #push!(alphas, alpha_r)\n",
    "\n",
    "    c0 = rand(N).*C0_MAX\n",
    "    c0 = sort(c0)\n",
    "    print(\"c0: \")\n",
    "    println([round(c0[i], digits = 5) for i in 1:N])\n",
    "    #push!(c0s, c0)\n",
    "\n",
    "    c1 = C0_MAX + rand()*(C1_MAX - C0_MAX)\n",
    "    print(\"c1: \")\n",
    "    println(round(c1, digits = 5))\n",
    "    #push!(c1s, c1)\n",
    "\n",
    "    r = rand(N).*R_MAX\n",
    "    print(\"r: \")\n",
    "    println([round(r[i], digits = 5) for i in 1:N])\n",
    "    #push!(rs, r)\n",
    "\n",
    "    #find true optimal solution\n",
    "    #resInhomog = rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    #push!(rviGs, resInhomog[1])\n",
    "    #print(\"RVI: \")\n",
    "    #print(round(resInhomog[1], digits = 5))\n",
    "    #print(\" (Iterations: \")\n",
    "    #print(resInhomog[3])\n",
    "    #print(\")    \")\n",
    "\n",
    "    #find best DSP solution\n",
    "    dsps = enumerateActions(N)\n",
    "    bestDSP = dsps[1]\n",
    "    bestDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, bestDSP)\n",
    "    for i in 2:(2^N)\n",
    "        thisDSP = dsps[i]\n",
    "        thisDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, thisDSP)\n",
    "        if thisDSP_G < bestDSP_G\n",
    "            bestDSP = thisDSP\n",
    "            bestDSP_G = thisDSP_G\n",
    "        end\n",
    "    end\n",
    "    #push!(dsps, bestDSP)\n",
    "    #push!(dspGs, bestDSP_G)\n",
    "    print(\"DSP Exact: \")\n",
    "    print(round(bestDSP_G, digits = 5))\n",
    "    print(\",    \")\n",
    "\n",
    "\n",
    "    #find PI on best DSP\n",
    "    #policy = staticPolicyToDynamicPolicyLAS(N, bestDSP)\n",
    "    #resPE = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    #push!(dspPeGs, resPE[1])\n",
    "    #print(\"DSP LAS: \")\n",
    "    #print(round(resPE[1], digits = 5))\n",
    "    #print(\" (Iterations: \")\n",
    "    #print(resPE[3])\n",
    "    #print(\"),   \")\n",
    "    #resPI = rpiExact(N, resPE[2], alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    #push!(piGs, resPI[1])\n",
    "    #print(\"PIH: \")\n",
    "    #print(round(resPI[1], digits = 5))\n",
    "    #print(\" (Iterations: \")\n",
    "    #print(resPI[3])\n",
    "    #println(\").\")\n",
    "\n",
    "    #print DSP\n",
    "    print(\"DSP: \")\n",
    "    println(bestDSP)\n",
    "\n",
    "    #Give heuristic DSP\n",
    "    dspH, dspHG counter = jointHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    print(\"Heuristic DSP: \")\n",
    "    print(dspH)\n",
    "    print(\": \")\n",
    "    println(dspHG)\n",
    "    if dspH == bestDSP\n",
    "        numSame += 1\n",
    "    end\n",
    "    #print optimality gap\n",
    "    #print(\"Optimality Gap: \")\n",
    "    #gap = (resPI[1]/resInhomog[1] - 1)*100\n",
    "    #push!(gaps2, gap)\n",
    "    #print(round(gap, digits = 2))\n",
    "    #println(\"%.\")\n",
    "    #end frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e925d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numSame/numRuns) #0.9918 vs 0.9984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r; dsp = fill(1, N))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52e113b4",
   "metadata": {},
   "source": [
    "## Experiment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e446837",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(12345)\n",
    "numRuns = 1000\n",
    "N = 20\n",
    "alpha_d = fill(0.0, N)\n",
    "beta = 1.0\n",
    "epsilon = 1e-10\n",
    "LEN_FRAME = 75\n",
    "TAU_MAX = 1.0\n",
    "C0_MAX = 1.0\n",
    "C1_MAX = 1000\n",
    "R_MAX = 100\n",
    "nMax = 10000\n",
    "delScale = 1\n",
    "numSame = 0\n",
    "funList = []\n",
    "for run in 1:numRuns\n",
    "    #Start frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "    print(\"Run: \")\n",
    "    println(run)\n",
    "    #generate parameters\n",
    "    tau = rand(N).*TAU_MAX \n",
    "    print(\"tau: \")\n",
    "    println([round(tau[i], digits = 5) for i in 1:N])\n",
    "    #push!(taus, tau)\n",
    "\n",
    "    alpha_r = [sampleTruncExp(tau[i]/2, tau[i]) for i in 1:N]\n",
    "    print(\"alpha_r: \")\n",
    "    println([round(alpha_r[i], digits = 5) for i in 1:N])\n",
    "    #push!(alphas, alpha_r)\n",
    "\n",
    "    c0 = rand(N).*C0_MAX\n",
    "    c0 = sort(c0)\n",
    "    print(\"c0: \")\n",
    "    println([round(c0[i], digits = 5) for i in 1:N])\n",
    "    #push!(c0s, c0)\n",
    "\n",
    "    c1 = C0_MAX + rand()*(C1_MAX - C0_MAX)\n",
    "    print(\"c1: \")\n",
    "    println(round(c1, digits = 5))\n",
    "    #push!(c1s, c1)\n",
    "\n",
    "    r = rand(N).*R_MAX\n",
    "    print(\"r: \")\n",
    "    println([round(r[i], digits = 5) for i in 1:N])\n",
    "    #push!(rs, r)\n",
    "\n",
    "    #Give heuristic DSP\n",
    "    dspH, dspHG, counter = jointHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    if counter > 2\n",
    "        push!(funList, run)\n",
    "    end\n",
    "    print(\"Heuristic DSP: \")\n",
    "    print(dspH)\n",
    "    print(\": \")\n",
    "    print(dspHG)\n",
    "    print(\". Iterations: \")\n",
    "    println(counter)\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3991c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "funList"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78687327",
   "metadata": {},
   "source": [
    "## Experiment 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb420745",
   "metadata": {},
   "source": [
    "Counting number of links in optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "numRuns = 1000\n",
    "N = 20\n",
    "alpha_d = fill(0.0, N)\n",
    "beta = 1.0\n",
    "epsilon = 1e-10\n",
    "LEN_FRAME = 75\n",
    "TAU_MAX = 1.0\n",
    "C0_MAX = 1.0\n",
    "C1_MAX = 1000\n",
    "R_MAX = 100\n",
    "nMax = 10000\n",
    "delScale = 1\n",
    "numSame = 0\n",
    "linkCount = fill(0, 21)\n",
    "#taus = []\n",
    "#alphas = []\n",
    "#c0s = []\n",
    "#c1s = []\n",
    "#rs = []\n",
    "#rviGs = []\n",
    "#dsps = []\n",
    "#dspGs = []\n",
    "#dspPeGs = []\n",
    "#piGs = []\n",
    "#gaps2 = []\n",
    "for run in 1:numRuns\n",
    "    #Start frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "    print(\"Run: \")\n",
    "    println(run)\n",
    "    #generate parameters\n",
    "    tau = rand(N).*TAU_MAX \n",
    "    print(\"tau: \")\n",
    "    println([round(tau[i], digits = 5) for i in 1:N])\n",
    "    #push!(taus, tau)\n",
    "\n",
    "    alpha_r = [sampleTruncExp(tau[i]/2, tau[i]) for i in 1:N]\n",
    "    print(\"alpha_r: \")\n",
    "    println([round(alpha_r[i], digits = 5) for i in 1:N])\n",
    "    #push!(alphas, alpha_r)\n",
    "\n",
    "    c0 = rand(N).*C0_MAX\n",
    "    c0 = sort(c0)\n",
    "    print(\"c0: \")\n",
    "    println([round(c0[i], digits = 5) for i in 1:N])\n",
    "    #push!(c0s, c0)\n",
    "\n",
    "    c1 = C0_MAX + rand()*(C1_MAX - C0_MAX)\n",
    "    print(\"c1: \")\n",
    "    println(round(c1, digits = 5))\n",
    "    #push!(c1s, c1)\n",
    "\n",
    "    r = rand(N).*R_MAX\n",
    "    print(\"r: \")\n",
    "    println([round(r[i], digits = 5) for i in 1:N])\n",
    "    #push!(rs, r)\n",
    "\n",
    "    #find true optimal solution\n",
    "    #resInhomog = rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    #push!(rviGs, resInhomog[1])\n",
    "    #print(\"RVI: \")\n",
    "    #print(round(resInhomog[1], digits = 5))\n",
    "    #print(\" (Iterations: \")\n",
    "    #print(resInhomog[3])\n",
    "    #print(\")    \")\n",
    "\n",
    "    #find best DSP solution\n",
    "    dsps = enumerateActions(N)\n",
    "    bestDSP = dsps[1]\n",
    "    bestDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, bestDSP)\n",
    "    for i in 2:(2^N)\n",
    "        thisDSP = dsps[i]\n",
    "        thisDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, thisDSP)\n",
    "        if thisDSP_G < bestDSP_G\n",
    "            bestDSP = thisDSP\n",
    "            bestDSP_G = thisDSP_G\n",
    "        end\n",
    "    end\n",
    "    #push!(dsps, bestDSP)\n",
    "    #push!(dspGs, bestDSP_G)\n",
    "    print(\"DSP Exact: \")\n",
    "    print(round(bestDSP_G, digits = 5))\n",
    "    print(\",    \")\n",
    "\n",
    "\n",
    "    #find PI on best DSP\n",
    "    #policy = staticPolicyToDynamicPolicyLAS(N, bestDSP)\n",
    "    #resPE = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    #push!(dspPeGs, resPE[1])\n",
    "    #print(\"DSP LAS: \")\n",
    "    #print(round(resPE[1], digits = 5))\n",
    "    #print(\" (Iterations: \")\n",
    "    #print(resPE[3])\n",
    "    #print(\"),   \")\n",
    "    #resPI = rpiExact(N, resPE[2], alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    #push!(piGs, resPI[1])\n",
    "    #print(\"PIH: \")\n",
    "    #print(round(resPI[1], digits = 5))\n",
    "    #print(\" (Iterations: \")\n",
    "    #print(resPI[3])\n",
    "    #println(\").\")\n",
    "\n",
    "    #print DSP\n",
    "    print(\"DSP: \")\n",
    "    println(bestDSP)\n",
    "\n",
    "    numLinks = sum(bestDSP)\n",
    "    linkCount[numLinks + 1] += 1\n",
    "\n",
    "    #Give heuristic DSP\n",
    "    dspH, dspHG, counter = jointHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    print(\"Heuristic DSP: \")\n",
    "    print(dspH)\n",
    "    print(\": \")\n",
    "    println(dspHG)\n",
    "    if dspH == bestDSP\n",
    "        numSame += 1\n",
    "    end\n",
    "    #print optimality gap\n",
    "    #print(\"Optimality Gap: \")\n",
    "    #gap = (resPI[1]/resInhomog[1] - 1)*100\n",
    "    #push!(gaps2, gap)\n",
    "    #print(round(gap, digits = 2))\n",
    "    #println(\"%.\")\n",
    "    #end frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkCount"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90b5a5f1",
   "metadata": {},
   "source": [
    "## Experiment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c59b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(12345)\n",
    "numRuns = 1000\n",
    "N = 5\n",
    "alpha_d = fill(0.0, N)\n",
    "beta = 1.0\n",
    "epsilon = 1e-15\n",
    "LEN_FRAME = 75\n",
    "TAU_MAX = 1.0\n",
    "C0_MAX = 1.0\n",
    "C1_MAX = 1000\n",
    "R_MAX = 100\n",
    "nMax = 50000\n",
    "delScale = 1\n",
    "taus = []\n",
    "alphas = []\n",
    "c0s = []\n",
    "c1s = []\n",
    "rs = []\n",
    "rviGs = []\n",
    "optDsps = []\n",
    "optDspGs = []\n",
    "heurDsps = []\n",
    "heurDspGs = []\n",
    "optPiGs = []\n",
    "heurPiGs = []\n",
    "optGaps = []\n",
    "heurGaps = []\n",
    "for run in 1:numRuns\n",
    "    #Start frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "    print(\"Run: \")\n",
    "    println(run)\n",
    "\n",
    "    #generate parameters\n",
    "    tau = rand(N).*TAU_MAX #\n",
    "    print(\"tau: \")\n",
    "    println([round(tau[i], digits = 5) for i in 1:N])\n",
    "    push!(taus, tau)\n",
    "\n",
    "    alpha_r = [sampleTruncExp(minimum(tau)/2, minimum(tau)) for i in 1:N]\n",
    "    print(\"alpha_r: \")\n",
    "    println([round(alpha_r[i], digits = 5) for i in 1:N])\n",
    "    push!(alphas, alpha_r)\n",
    "\n",
    "    c0 = rand(N).*C0_MAX\n",
    "    c0 = sort(c0)\n",
    "    print(\"c0: \")\n",
    "    println([round(c0[i], digits = 5) for i in 1:N])\n",
    "    push!(c0s, c0)\n",
    "\n",
    "    c1 = C0_MAX + rand()*(C1_MAX - C0_MAX)\n",
    "    print(\"c1: \")\n",
    "    println(round(c1, digits = 5))\n",
    "    push!(c1s, c1)\n",
    "\n",
    "    r = rand(N).*R_MAX\n",
    "    print(\"r: \")\n",
    "    println([round(r[i], digits = 5) for i in 1:N])\n",
    "    push!(rs, r)\n",
    "\n",
    "    #find true optimal solution\n",
    "    resRVI= rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 100)\n",
    "    push!(rviGs, resRVI[1])\n",
    "    print(\"RVI: \")\n",
    "    print(round(resRVI[1], digits = 5))\n",
    "    print(\" (Iterations: \")\n",
    "    print(resRVI[3])\n",
    "    println(\")    \")\n",
    "\n",
    "    #find best DSP solution\n",
    "    dsps = enumerateActions(N)\n",
    "    bestDSP = dsps[1]\n",
    "    bestDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, bestDSP)\n",
    "    for i in 2:(2^N)\n",
    "        thisDSP = dsps[i]\n",
    "        thisDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, thisDSP)\n",
    "        if thisDSP_G < bestDSP_G\n",
    "            bestDSP = thisDSP\n",
    "            bestDSP_G = thisDSP_G\n",
    "        end\n",
    "    end\n",
    "    push!(optDsps, bestDSP)\n",
    "    push!(optDspGs, bestDSP_G)\n",
    "    print(\"Optimal DSP: \")\n",
    "    print(bestDSP)\n",
    "    print(\" \")\n",
    "    print(round(bestDSP_G, digits = 5))\n",
    "    print(\",    \")\n",
    "\n",
    "    #find PE of best DSP\n",
    "    policy = staticPolicyToDynamicPolicyLAS(N, bestDSP)\n",
    "    resPE = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    print(\"PE Iterations: \")\n",
    "    print(resPE[3])\n",
    "    print(\",   \")\n",
    "\n",
    "    #do PI on best DSP\n",
    "    resPI = rpiExact(N, resPE[2], alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    push!(optPiGs, resPI[1])\n",
    "    print(\"PIH: \")\n",
    "    print(round(resPI[1], digits = 5))\n",
    "    print(\" (Iterations: \")\n",
    "    print(resPI[3])\n",
    "    println(\").\")\n",
    "\n",
    "    #Print optimality gap\n",
    "    print(\"Optimality Gap: \")\n",
    "    gap = (resPI[1]/resRVI[1] - 1)*100\n",
    "    push!(optGaps, gap)\n",
    "    print(round(gap, digits = 2))\n",
    "    println(\"%.\")\n",
    "\n",
    "    #find heuristic DSP solution\n",
    "    heurDsp, heurDspG, counter = jointHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    push!(heurDsps, heurDsp)\n",
    "    push!(heurDspGs)\n",
    "    if heurDsp == bestDSP\n",
    "        println(\"Heuristic DSP same as Optimal DSP.\")\n",
    "        push!(heurGaps, gap)\n",
    "    else\n",
    "        #print heuristic DSP and g\n",
    "        print(\"Heuristic DSP: \")\n",
    "        print(heurDsp)\n",
    "        print(\" \")\n",
    "        print(round(heurDspG, digits = 5))\n",
    "\n",
    "        #find PE of heuristic DSP\n",
    "        policy = staticPolicyToDynamicPolicyLAS(N, heurDsp)\n",
    "        resPE = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "        print(\"PE Iterations: \")\n",
    "        print(resPE[3])\n",
    "        print(\"),   \")\n",
    "\n",
    "        #do PI on heuristic DSP\n",
    "        resPI = rpiExact(N, resPE[2], alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "        push!(heurPiGs, resPI[1])\n",
    "        print(\"PIH: \")\n",
    "        print(round(resPI[1], digits = 5))\n",
    "        print(\" (Iterations: \")\n",
    "        print(resPI[3])\n",
    "        println(\").\")\n",
    "\n",
    "        #Print optimality gap\n",
    "        print(\"Optimality Gap: \")\n",
    "        gap = (resPI[1]/resRVI[1] - 1)*100\n",
    "        push!(heurGaps, gap)\n",
    "        print(round(gap, digits = 2))\n",
    "        println(\"%.\")\n",
    "    end\n",
    "    #end frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2108638",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsExp6 = Dict()\n",
    "resultsExp6[\"taus\"] = taus \n",
    "resultsExp6[\"alphas\"] = alphas\n",
    "resultsExp6[\"c0s\"] = c1s \n",
    "resultsExp6[\"rs\"] = rs \n",
    "resultsExp6[\"rviGs\"] = rviGs\n",
    "resultsExp6[\"optDsps\"] = optDsps \n",
    "resultsExp6[\"optDspGs\"] = optDspGs \n",
    "resultsExp6[\"heurDsps\"] = heurDsps\n",
    "resultsExp6[\"heurDspGs\"] = heurDspGs\n",
    "resultsExp6[\"optPiGs\"] = optPiGs \n",
    "resultsExp6[\"heurPiGs\"] = heurPiGs\n",
    "resultsExp6[\"optGaps\"] = optGaps \n",
    "resultsExp6[\"heurGaps\"] = heurGaps\n",
    "\n",
    "f = serialize(\"resultsExp6.big\", resultsExp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdee3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(optGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum(optGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(optGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "median(optGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < 1e-10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9cbbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < -1e-5 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699ba9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < -1e-10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc3d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < 1 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeec0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < 5 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < 10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(heurDsps[i] == optDsps[i] for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(optDsps[i]) == 3 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspGaps = (optDspGs./rviGs .-1).*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(dspGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5944bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(dspGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abadc1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "median(dspGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab62523",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps[i] < 1e-10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3932db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps[i] < -1e-10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5463a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < -1e-10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154640ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] for i in 1:1000 if dspGaps[i] < 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps[i] for i in 1:1000 if dspGaps[i] < 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(optGaps[i] for i in 1:1000 if optGaps[i] > -0.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [(dspGaps[i] - optGaps[i])/dspGaps[i] for i in 1:1000 if dspGaps[i] > 1e-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd20457",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [s*(s <= 1) + 1*(s > 1) for s in score if s >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(score, xlim = (0,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c73f6a77",
   "metadata": {},
   "source": [
    "## Experiment 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d29b5f8a",
   "metadata": {},
   "source": [
    "Similar to above, but smaller epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4093260",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(12345)\n",
    "numRuns = 1000\n",
    "N = 5\n",
    "alpha_d = fill(0.0, N)\n",
    "beta = 1.0\n",
    "epsilon = 1e-10\n",
    "LEN_FRAME = 75\n",
    "TAU_MAX = 1.0\n",
    "C0_MAX = 1.0\n",
    "C1_MAX = 1000\n",
    "R_MAX = 100\n",
    "nMax = 50000\n",
    "delScale = 1\n",
    "taus = []\n",
    "alphas = []\n",
    "c0s = []\n",
    "c1s = []\n",
    "rs = []\n",
    "rviGs = []\n",
    "optDsps = []\n",
    "optDspGs = []\n",
    "heurDsps = []\n",
    "heurDspGs = []\n",
    "optPiGs = []\n",
    "heurPiGs = []\n",
    "optGaps = []\n",
    "heurGaps = []\n",
    "for run in 1:numRuns\n",
    "    #Start frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "    print(\"Run: \")\n",
    "    println(run)\n",
    "\n",
    "    #generate parameters\n",
    "    tau = rand(N).*TAU_MAX #\n",
    "    print(\"tau: \")\n",
    "    println([round(tau[i], digits = 5) for i in 1:N])\n",
    "    push!(taus, tau)\n",
    "\n",
    "    alpha_r = [sampleTruncExp(minimum(tau)/2, minimum(tau)) for i in 1:N]\n",
    "    print(\"alpha_r: \")\n",
    "    println([round(alpha_r[i], digits = 5) for i in 1:N])\n",
    "    push!(alphas, alpha_r)\n",
    "\n",
    "    c0 = rand(N).*C0_MAX\n",
    "    c0 = sort(c0)\n",
    "    print(\"c0: \")\n",
    "    println([round(c0[i], digits = 5) for i in 1:N])\n",
    "    push!(c0s, c0)\n",
    "\n",
    "    c1 = C0_MAX + rand()*(C1_MAX - C0_MAX)\n",
    "    print(\"c1: \")\n",
    "    println(round(c1, digits = 5))\n",
    "    push!(c1s, c1)\n",
    "\n",
    "    r = rand(N).*R_MAX\n",
    "    print(\"r: \")\n",
    "    println([round(r[i], digits = 5) for i in 1:N])\n",
    "    push!(rs, r)\n",
    "\n",
    "    #find true optimal solution\n",
    "    resRVI= rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 100)\n",
    "\n",
    "    #evaluate true optimal solution\n",
    "    resRVI_PE = rpe(N, resRVI[4], alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    push!(rviGs, resRVI_PE[1])\n",
    "    print(\"RVI: \")\n",
    "    print(round(resRVI_PE[1], digits = 5))\n",
    "    print(\" (RVI Iterations: \")\n",
    "    print(resRVI[3])\n",
    "    print(\", PE Iterations: \")\n",
    "    print(resRVI_PE[3])\n",
    "    println(\")    \")\n",
    "\n",
    "    #find best DSP solution\n",
    "    dsps = enumerateActions(N)\n",
    "    bestDSP = dsps[1]\n",
    "    bestDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, bestDSP)\n",
    "    for i in 2:(2^N)\n",
    "        thisDSP = dsps[i]\n",
    "        thisDSP_G = calculateG_DSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, thisDSP)\n",
    "        if thisDSP_G < bestDSP_G\n",
    "            bestDSP = thisDSP\n",
    "            bestDSP_G = thisDSP_G\n",
    "        end\n",
    "    end\n",
    "    push!(optDsps, bestDSP)\n",
    "    push!(optDspGs, bestDSP_G)\n",
    "    print(\"Optimal DSP: \")\n",
    "    print(bestDSP)\n",
    "    print(\" \")\n",
    "    print(round(bestDSP_G, digits = 5))\n",
    "    print(\",    \")\n",
    "\n",
    "    #find PE of best DSP\n",
    "    policy = staticPolicyToDynamicPolicyLAS(N, bestDSP)\n",
    "    resPE = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    print(\"PE Iterations: \")\n",
    "    print(resPE[3])\n",
    "    print(\",   \")\n",
    "\n",
    "    #do PI on best DSP\n",
    "    resPI = rpiExact(N, resPE[2], alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    push!(optPiGs, resPI[1])\n",
    "    print(\"PIH: \")\n",
    "    print(round(resPI[1], digits = 5))\n",
    "    print(\" (Iterations: \")\n",
    "    print(resPI[3])\n",
    "    println(\").\")\n",
    "\n",
    "    #Print optimality gap\n",
    "    print(\"Optimality Gap: \")\n",
    "    gap = (resPI[1]/resRVI_PE[1] - 1)*100\n",
    "    push!(optGaps, gap)\n",
    "    print(round(gap, digits = 2))\n",
    "    println(\"%.\")\n",
    "\n",
    "    #find heuristic DSP solution\n",
    "    heurDsp, heurDspG, counter = jointHeuristic(N, alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    push!(heurDsps, heurDsp)\n",
    "    push!(heurDspGs)\n",
    "    if heurDsp == bestDSP\n",
    "        println(\"Heuristic DSP same as Optimal DSP.\")\n",
    "        push!(heurGaps, gap)\n",
    "    else\n",
    "        #print heuristic DSP and g\n",
    "        print(\"Heuristic DSP: \")\n",
    "        print(heurDsp)\n",
    "        print(\" \")\n",
    "        print(round(heurDspG, digits = 5))\n",
    "\n",
    "        #find PE of heuristic DSP\n",
    "        policy = staticPolicyToDynamicPolicyLAS(N, heurDsp)\n",
    "        resPE = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "        print(\"PE Iterations: \")\n",
    "        print(resPE[3])\n",
    "        print(\"),   \")\n",
    "\n",
    "        #do PI on heuristic DSP\n",
    "        resPI = rpiExact(N, resPE[2], alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "        push!(heurPiGs, resPI[1])\n",
    "        print(\"PIH: \")\n",
    "        print(round(resPI[1], digits = 5))\n",
    "        print(\" (Iterations: \")\n",
    "        print(resPI[3])\n",
    "        println(\").\")\n",
    "\n",
    "        #Print optimality gap\n",
    "        print(\"Optimality Gap: \")\n",
    "        gap = (resPI[1]/resRVI_PE[1] - 1)*100\n",
    "        push!(heurGaps, gap)\n",
    "        print(round(gap, digits = 2))\n",
    "        println(\"%.\")\n",
    "    end\n",
    "    #end frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ed756",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsExp7 = Dict()\n",
    "resultsExp7[\"taus\"] = taus \n",
    "resultsExp7[\"alphas\"] = alphas\n",
    "resultsExp7[\"c0s\"] = c1s \n",
    "resultsExp7[\"rs\"] = rs \n",
    "resultsExp7[\"rviGs\"] = rviGs\n",
    "resultsExp7[\"optDsps\"] = optDsps \n",
    "resultsExp7[\"optDspGs\"] = optDspGs \n",
    "resultsExp7[\"heurDsps\"] = heurDsps\n",
    "resultsExp7[\"heurDspGs\"] = heurDspGs\n",
    "resultsExp7[\"optPiGs\"] = optPiGs \n",
    "resultsExp7[\"heurPiGs\"] = heurPiGs\n",
    "resultsExp7[\"optGaps\"] = optGaps \n",
    "resultsExp7[\"heurGaps\"] = heurGaps\n",
    "\n",
    "f = serialize(\"resultsExp7.big\", resultsExp7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(optGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum(optGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(optGaps, xlim = (0,25), label = \"PI Optimality Gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(optGaps, density= true, bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1539c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(optGaps, xlim = (10, 25), ylim = (0, 0.008))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < 1e-10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a54309",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < 0.1 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedda4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < 1.0 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps[i] < 10.0 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a16073",
   "metadata": {},
   "outputs": [],
   "source": [
    "median(optGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(optGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82772b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile(optGaps, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386235d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile(optGaps, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspGaps = (optDspGs./rviGs .-1).*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19be9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "density(dspGaps, xlim = (0.0, maximum(dspGaps)), label = \"DSP Optimality Gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90143a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(dspGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum(dspGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb312a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9126d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "median(dspGaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps[i] < 1e-10 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps[i] < 0.1 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf359e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps[i] < 1.0 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dspGaps[i] < 10.0 for i in 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile(dspGaps, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34969138",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile(dspGaps, 0.99)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73a7fe26",
   "metadata": {},
   "source": [
    "## Experiment 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "30308f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Run: 1\n",
      "tau: [0.9447905649425166, 0.866894886438913, 0.3396115028483221, 0.13611651795784874, 0.3225010866963668]\n",
      "alpha: [0.008970179158651559, 0.036935810010869013, 0.059888020871150055, 0.03663215704203369, 0.0737056992956364]\n",
      "c0: [0.09261145157968753, 0.11848853125656122, 0.26755021483368013, 0.8104947638539546, 0.8681535487443706]\n",
      "c1: 853.4190042384113\n",
      "r: [99.56147215735662, 82.000516419209, 83.4463081073842, 73.46937216084547, 38.36128790136257]\n",
      "\n",
      "RVI: 4.70822, PI Heuristic: 4.70822, Gap: -0.0\n",
      "(Simulated) DSP g: 4.71124, e-soft DSP g: 4.94448\n",
      "Simulated PI g: 4.7069, Exact PI g: 4.70822, Optimality Gap: -0.0\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Run: 2\n",
      "tau: [0.9026092458595942, 0.33079891601205613, 0.13162494417125792, 0.1735229272258466, 0.13882745373884087]\n",
      "alpha: [0.03063031765318111, 0.045966658334234294, 0.007767668179646831, 0.002744059925049017, 0.014014082661568152]\n",
      "c0: [0.031060918612698662, 0.47502901483951, 0.6338479453532103, 0.8686618392461037, 0.8999510969166794]\n",
      "c1: 886.4052903510224\n",
      "r: [2.2917203156524124, 74.74303803715054, 92.61105979360987, 24.71873940882312, 49.33692564620821]\n",
      "\n",
      "RVI: 0.95977, PI Heuristic: 0.95977, Gap: 0.0\n",
      "(Simulated) DSP g: 0.96504, e-soft DSP g: 1.28256\n",
      "Simulated PI g: 0.9757, Exact PI g: 0.97105, Optimality Gap: 1.18\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Run: 3\n",
      "tau: [0.022975827157776574, 0.5819438542193595, 0.28519068774898626, 0.8401967208497753, 0.1706197629133105]\n",
      "alpha: [0.010343463158472604, 0.022534434095330277, 0.00297784571145519, 0.01564425158696812, 0.0088073736937561]\n",
      "c0: [0.03823088323229851, 0.08118340510081057, 0.48103483781166023, 0.6652328434047942, 0.6831432889802543]\n",
      "c1: 299.0424552332221\n",
      "r: [48.852596224220115, 5.6513539628750165, 18.79230864601834, 89.16133717717167, 83.94541316242426]\n",
      "\n",
      "RVI: 0.59994, PI Heuristic: 0.60328, Gap: 0.56\n",
      "(Simulated) DSP g: 0.61556, e-soft DSP g: 0.98288\n",
      "Simulated PI g: 0.61464, Exact PI g: 0.61598, Optimality Gap: 2.67\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Run: 4\n",
      "tau: [0.5257954524244801, 0.6846826944853885, 0.6299157011197652, 0.12180466275080659, 0.22714523410904908]\n",
      "alpha: [0.10793481201011335, 0.07928947327031104, 0.06441516474737297, 0.1038830297062849, 0.046310822473277766]\n",
      "c0: [0.04556691727491835, 0.2569855620998601, 0.7641765093049641, 0.8355328219650928, 0.9247865497253934]\n",
      "c1: 215.51526875094254\n",
      "r: [18.11552569034297, 65.83241974888779, 73.72969029596533, 69.25855091220197, 8.926233453609667]\n",
      "\n",
      "RVI: 9.08295, PI Heuristic: 9.33284, Gap: 2.75\n",
      "(Simulated) DSP g: 10.97675, e-soft DSP g: 11.32879\n",
      "Simulated PI g: 10.92873, Exact PI g: 10.98198, Optimality Gap: 20.91\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Run: 5\n",
      "tau: [0.745158656317059, 0.06716015502195471, 0.9766402666233881, 0.7678286947665199, 0.9259791273188613]\n",
      "alpha: [0.004550703832405469, 0.02918758748451868, 0.004692436761172917, 0.04218837255365773, 0.026556668445700402]\n",
      "c0: [0.014706365774695529, 0.310238593662026, 0.45867544991454623, 0.497571749691327, 0.7897714145236052]\n",
      "c1: 142.54375963933796\n",
      "r: [76.48717254657191, 69.05450320280814, 13.795756870611886, 50.037855003798214, 89.39004629330798]\n",
      "\n",
      "RVI: 0.55177, PI Heuristic: 0.55177, Gap: -0.0\n",
      "(Simulated) DSP g: 0.55191, e-soft DSP g: 0.61175\n",
      "Simulated PI g: 0.55186, Exact PI g: 0.55177, Optimality Gap: -0.0\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Run: 6\n",
      "tau: [0.3152421869370332, 0.0775328897574149, 0.8668001780368776, 0.635247162882394, 0.37101097421566187]\n",
      "alpha: [0.02209608772152969, 0.03497173817099469, 0.006111187881283514, 0.04762558624167467, 0.006563205112644]\n",
      "c0: [0.1876429469318034, 0.484814030223487, 0.5136160809618052, 0.6045241487477916, 0.8827443952651307]\n",
      "c1: 297.3222012571138\n",
      "r: [25.844439089895545, 32.364983460388174, 54.91785936768554, 55.003683540638825, 40.79891902418276]\n",
      "\n",
      "RVI: 1.64594, PI Heuristic: 1.64594, Gap: 0.0\n",
      "(Simulated) DSP g: 1.64614, e-soft DSP g: 1.68781\n",
      "Simulated PI g: 1.6456, Exact PI g: 1.64594, Optimality Gap: 0.0\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Run: 7\n",
      "tau: [0.9401692702576502, 0.12253520319473654, 0.9804835488409009, 0.059976101651463054, 0.21479246476141112]\n",
      "alpha: [0.0028901349510305242, 0.005098753385585487, 0.012999539284461492, 0.040165092523188686, 0.034670073449654844]\n",
      "c0: [0.3812291267547717, 0.539344466494275, 0.5926193129911952, 0.6550375788867486, 0.9808109904430782]\n",
      "c1: 583.5296460777939\n",
      "r: [43.67261437144428, 55.22601284843039, 68.23433195558535, 12.229945769185345, 81.37025017657913]\n",
      "\n",
      "RVI: 1.43193, PI Heuristic: 1.43193, Gap: 0.0\n",
      "(Simulated) DSP g: 1.43153, e-soft DSP g: 1.53135\n",
      "Simulated PI g: 1.40399, Exact PI g: 1.71245, Optimality Gap: 19.59\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Run: 8\n",
      "tau: [0.9456921874808961, 0.9497919748832513, 0.45925544940841956, 0.6821525081196905, 0.6664253465715614]\n",
      "alpha: [0.11423733862082462, 0.15286560502573487, 0.19254088803642913, 0.10758051686472457, 0.3975377781371666]\n",
      "c0: [0.22682668576626408, 0.29561830474399775, 0.30707213452019777, 0.606623172832839, 0.8840887409951093]\n",
      "c1: 220.45421939847725\n",
      "r: [68.41606204680438, 1.5736189890959706, 77.60279058551623, 16.20470452457906, 9.30788711684144]\n",
      "\n",
      "RVI: 6.30142, PI Heuristic: 6.42945, Gap: 2.03\n",
      "(Simulated) DSP g: 6.90283, e-soft DSP g: 7.18593\n",
      "Simulated PI g: 6.87231, Exact PI g: 6.91623, Optimality Gap: 9.76\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Run: 9\n",
      "tau: [0.8683757466953818, 0.05771875062539511, 0.8684883538634811, 0.08192290025835924, 0.7900357372232398]\n",
      "alpha: [0.016811561518671525, 0.008752819399673536, 0.007510710372060958, 0.011454595638471272, 0.00412955700132952]\n",
      "c0: [0.23637668396750444, 0.2710937732136022, 0.30684075929588006, 0.5125292450208497, 0.9456116870596813]\n",
      "c1: 684.4743327959106\n",
      "r: [25.679854650523204, 30.18057448831498, 2.355462804770092, 56.79002499090413, 72.16649536424083]\n",
      "\n",
      "RVI: 0.73465, PI Heuristic: 0.73538, Gap: 0.1\n",
      "(Simulated) DSP g: 0.73841, e-soft DSP g: 0.83694\n",
      "Simulated PI g: 0.73879, Exact PI g: 0.73824, Optimality Gap: 0.49\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Run: 10\n",
      "tau: [0.9012147830891933, 0.46122675648518374, 0.31544803282459943, 0.37435245804088535, 0.5685754198997256]\n",
      "alpha: [0.11961157271271257, 0.14914615582631915, 0.1755868021738314, 0.04575225990932379, 0.09233917901748587]\n",
      "c0: [0.12638796304645084, 0.18494113302675308, 0.3323566671401027, 0.36708612741863855, 0.6844497859519026]\n",
      "c1: 676.2840045976853\n",
      "r: [86.76798774391285, 62.572808673000694, 61.677208836541666, 49.901638315862705, 39.01371187257606]\n",
      "\n",
      "RVI: 17.78518, PI Heuristic: 19.32581, Gap: 8.66\n",
      "(Simulated) DSP g: 21.53591, e-soft DSP g: 21.71494\n",
      "Simulated PI g: 21.45121, Exact PI g: 21.56686, Optimality Gap: 21.26\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Run: 11\n",
      "tau: [0.022522076981925765, 0.8228275977023196, 0.8693865393448893, 0.12311714526487239, 0.012322728543585404]\n",
      "alpha: [0.008910573509950405, 0.0026667237295539908, 0.0030445495105178165, 0.0007028947516503511, 0.003795975923588317]\n",
      "c0: [0.2337230656462589, 0.3225747463161558, 0.5681099185151535, 0.8105455050815257, 0.9314719369533632]\n",
      "c1: 725.1320617048058\n",
      "r: [68.40749403376859, 99.58718455087417, 92.94873577891161, 34.380836023438455, 29.013577427125238]\n",
      "\n",
      "RVI: 0.85149, PI Heuristic: 0.85203, Gap: 0.06\n",
      "(Simulated) DSP g: 0.85367, e-soft DSP g: 0.95877\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] Array",
      "    @ ./boot.jl:459 [inlined]",
      "  [2] Array",
      "    @ ./boot.jl:468 [inlined]",
      "  [3] Array",
      "    @ ./boot.jl:476 [inlined]",
      "  [4] similar",
      "    @ ./abstractarray.jl:841 [inlined]",
      "  [5] similar",
      "    @ ./abstractarray.jl:840 [inlined]",
      "  [6] similar",
      "    @ ./broadcast.jl:212 [inlined]",
      "  [7] similar",
      "    @ ./broadcast.jl:211 [inlined]",
      "  [8] copy",
      "    @ ./broadcast.jl:885 [inlined]",
      "  [9] materialize",
      "    @ ./broadcast.jl:860 [inlined]",
      " [10] broadcast_preserving_zero_d",
      "    @ ./broadcast.jl:849 [inlined]",
      " [11] -(A::Vector{Int64}, B::Vector{Int64})",
      "    @ Base ./arraymath.jl:8",
      " [12] sojournTime(s::Vector{Int64}, a::Vector{Int64}, flows::Vector{Float64}, N::Int64, alpha_d::Vector{Float64}, alpha_r::Vector{Float64}, beta::Float64, tau::Vector{Float64})",
      "    @ Main ./In[376]:3",
      " [13] #instantCostCont#660",
      "    @ ./In[377]:4 [inlined]",
      " [14] instantCostCont",
      "    @ ./In[377]:2 [inlined]",
      " [15] smarActionAndVFromNewVFA(s::Vector{Int64}, flows::Vector{Float64}, N::Int64, alpha_d::Vector{Float64}, alpha_r::Vector{Float64}, beta::Float64, tau::Vector{Float64}, c0::Vector{Float64}, c1::Float64, r::Vector{Float64}, ve::Dict{Any, Any}, vn::Dict{Any, Any}, g::Float64; s0::Vector{Int64})",
      "    @ Main ./In[425]:5",
      " [16] gEvaluationNewVFA(N::Int64, alpha_d::Vector{Float64}, alpha_r::Vector{Float64}, beta::Float64, tau::Vector{Float64}, c0::Vector{Float64}, c1::Float64, r::Vector{Float64}, nMax::Int64, ve::Dict{Any, Any}, vn::Dict{Any, Any}, g0::Float64; printProgress::Bool, modCounter::Int64)",
      "    @ Main ./In[432]:28",
      " [17] top-level scope",
      "    @ ./In[477]:89"
     ]
    }
   ],
   "source": [
    "numRuns = 1000\n",
    "N = 5\n",
    "alpha_d = fill(0.0, N)\n",
    "beta = 1.0\n",
    "epsilon = 1e-10\n",
    "LEN_FRAME = 75\n",
    "nMaxgEval = 2000000\n",
    "nMaxSmarpe = 2000000\n",
    "nMaxPE = 50000\n",
    "delScale = 1\n",
    "stepsize = 0.0\n",
    "esoft = 1e-3\n",
    "\n",
    "ves = []\n",
    "vns = []\n",
    "simDspG = []\n",
    "simSoftDspG = []\n",
    "simApproxPIG = []\n",
    "exactApproxPIG = []\n",
    "optGapsApprox = []\n",
    "for run in 1:numRuns\n",
    "    #Start frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "    print(\"Run: \")\n",
    "    println(run)\n",
    "\n",
    "    #get parameters\n",
    "    tau = taus[run]\n",
    "    print(\"tau: \")\n",
    "    println(tau)\n",
    "\n",
    "    alpha_r = alphas[run]\n",
    "    print(\"alpha: \")\n",
    "    println(alpha_r)\n",
    "\n",
    "    c0 = c0s[run]\n",
    "    print(\"c0: \")\n",
    "    println(c0)\n",
    "\n",
    "    c1 = c1s[run]\n",
    "    print(\"c1: \")\n",
    "    println(c1)\n",
    "\n",
    "    r = rs[run]\n",
    "    print(\"r: \")\n",
    "    println(r)\n",
    "\n",
    "    println()\n",
    "\n",
    "    #print rvi result \n",
    "    print(\"RVI: \")\n",
    "    print(round(rviGs[run], digits = 5))\n",
    "\n",
    "    #print exact heuristic result\n",
    "    print(\", PI Heuristic: \")\n",
    "    print(round(optPiGs[run], digits = 5))\n",
    "\n",
    "    print(\", Gap: \")\n",
    "    println(round(optGaps[run], digits = 2))\n",
    "\n",
    "    #simulate g of exact dsp\n",
    "    dsp = optDsps[run]\n",
    "    Random.seed!(12345)\n",
    "    gsExact = gEvaluationDSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMaxgEval, dsp; s = fill(1, N), epsilon = 0.0, printProgress = false, modCounter = 100000, stateTrace = false)\n",
    "    print(\"(Simulated) DSP g: \")\n",
    "    print(round(gsExact[1], digits = 5))\n",
    "    push!(simDspG, gsExact[1])\n",
    "\n",
    "    #simulate g of e-soft dsp\n",
    "    Random.seed!(12345)\n",
    "    gs = gEvaluationDSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMaxgEval, dsp; s = fill(1, N), epsilon = 0.01, printProgress = false, modCounter = 100000, stateTrace = false)\n",
    "    g = gs[1]\n",
    "    print(\", e-soft DSP g: \")\n",
    "    println(round(gs[1], digits = 5))\n",
    "    push!(simSoftDspG, gs[1])\n",
    "\n",
    "    #find smarpe vfa of dsp\n",
    "    Random.seed!(12345)\n",
    "    resSMARPE = smarpeNewVFA_ST_DSP_mcclain(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMaxSmarpe, stepsize, dsp; epsilon = esoft, ve = Dict(), vn = Dict(), vfaProvided = false, g = g, gFixed = true, s = fill(1,N), stateTrace = [], timePassed = 0.0, printProgress = false, modCounter = 1000000)\n",
    "    ve = resSMARPE[1][\"ve\"]\n",
    "    vn = resSMARPE[1][\"vn\"]\n",
    "    push!(ves, ve)\n",
    "    push!(vns, vn)\n",
    "\n",
    "    #evaluate PI policy from smarpe vfa\n",
    "    policy = policyFromNewVFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "    resPI = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMaxPE, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "    Random.seed!(12345)\n",
    "    gsPI = gEvaluationNewVFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMaxgEval, ve, vn, g, printProgress = false, modCounter = 100000)\n",
    "    push!(simApproxPIG, gsPI[1])\n",
    "    push!(exactApproxPIG, resPI[1])\n",
    "\n",
    "    print(\"Simulated PI g: \")\n",
    "    print(round(gsPI[1], digits = 5))\n",
    "    print(\", Exact PI g: \")\n",
    "    print(round(resPI[1], digits = 5))\n",
    "    print(\", Optimality Gap: \")\n",
    "    gap = (resPI[1]/rviGs[run] - 1)*100\n",
    "    push!(optGapsApprox, gap)\n",
    "    println(round(gap, digits = 2))\n",
    "    #end frame\n",
    "    println(join(fill(\"-\",LEN_FRAME)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d021ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultExp8 = Dict()\n",
    "resultExp8[\"ves\"] = ves \n",
    "resultExp8[\"vns\"] = vns \n",
    "resultExp8[\"simDspG\"] = simDspG \n",
    "resultExp8[\"simSoftDspG\"] = simSoftDspG \n",
    "resultExp8[\"simApproxPIG\"] = simApproxPIG \n",
    "resultExp8[\"exactApproxPIG\"] = exactApproxPIG \n",
    "resultExp8[\"optGapsApprox\"] = optGapsApprox \n",
    "f = serialize(\"resultExp8.dat\", resultExp8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "1b41aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Run: 1\n",
      "tau: [0.9447905649425166, 0.866894886438913, 0.3396115028483221, 0.13611651795784874, 0.3225010866963668]\n",
      "alpha: [0.008970179158651559, 0.036935810010869013, 0.059888020871150055, 0.03663215704203369, 0.0737056992956364]\n",
      "c0: [0.09261145157968753, 0.11848853125656122, 0.26755021483368013, 0.8104947638539546, 0.8681535487443706]\n",
      "c1: 853.4190042384113\n",
      "r: [99.56147215735662, 82.000516419209, 83.4463081073842, 73.46937216084547, 38.36128790136257]\n",
      "\n",
      "RVI: 4.70822, PI Heuristic: 4.70822, Gap: -0.0\n",
      "(Simulated) DSP g: 4.70998, e-soft DSP g: 4.94791\n",
      "Simulated PI g: 4.70748, Exact PI g: 4.70822, Optimality Gap: 0.0\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "run = 1\n",
    "\n",
    "#Start frame\n",
    "println(join(fill(\"-\",LEN_FRAME)))\n",
    "print(\"Run: \")\n",
    "println(run)\n",
    "\n",
    "#get parameters\n",
    "tau = taus[run]\n",
    "print(\"tau: \")\n",
    "println(tau)\n",
    "\n",
    "alpha_r = alphas[run]\n",
    "print(\"alpha: \")\n",
    "println(alpha_r)\n",
    "\n",
    "c0 = c0s[run]\n",
    "print(\"c0: \")\n",
    "println(c0)\n",
    "\n",
    "c1 = c1s[run]\n",
    "print(\"c1: \")\n",
    "println(c1)\n",
    "\n",
    "r = rs[run]\n",
    "print(\"r: \")\n",
    "println(r)\n",
    "\n",
    "println()\n",
    "\n",
    "#print rvi result \n",
    "print(\"RVI: \")\n",
    "print(round(rviGs[run], digits = 5))\n",
    "\n",
    "#print exact heuristic result\n",
    "print(\", PI Heuristic: \")\n",
    "print(round(optPiGs[run], digits = 5))\n",
    "\n",
    "print(\", Gap: \")\n",
    "println(round(optGaps[run], digits = 2))\n",
    "\n",
    "#simulate g of exact dsp\n",
    "dsp = optDsps[run]\n",
    "Random.seed!(12345)\n",
    "gsExact = gEvaluationDSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMaxgEval, dsp; s = fill(1, N), epsilon = 0.0, printProgress = false, modCounter = 100000, stateTrace = false)\n",
    "print(\"(Simulated) DSP g: \")\n",
    "print(round(gsExact[1], digits = 5))\n",
    "push!(simDspG, gsExact[1])\n",
    "\n",
    "#rpe of dsp\n",
    "policy = staticPolicyToDynamicPolicyLAS(N, dsp)\n",
    "resPE = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = 1, printProgress = false, modCounter = 1000)\n",
    "\n",
    "#simulate g of e-soft dsp\n",
    "Random.seed!(12345)\n",
    "gs = gEvaluationDSP(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMaxgEval, dsp; s = fill(1, N), epsilon = 0.01, printProgress = false, modCounter = 100000, stateTrace = false)\n",
    "g = gs[1]\n",
    "print(\", e-soft DSP g: \")\n",
    "println(round(gs[1], digits = 5))\n",
    "push!(simSoftDspG, gs[1])\n",
    "\n",
    "#find smarpe vfa of dsp\n",
    "Random.seed!(12345)\n",
    "resSMARPE = smarpeNewVFA_ST_DSP_mcclain(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMaxSmarpe, 0.0, dsp; epsilon = esoft, ve = Dict(), vn = Dict(), vfaProvided = false, g = g, gFixed = true, s = fill(1,N), stateTrace = [], timePassed = 0.0, printProgress = false, modCounter = 1000000)\n",
    "ve = resSMARPE[1][\"ve\"]\n",
    "vn = resSMARPE[1][\"vn\"]\n",
    "push!(ves, ve)\n",
    "push!(vns, vn)\n",
    "\n",
    "#evaluate PI policy from smarpe vfa\n",
    "policy = policyFromNewVFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "resPI = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMaxPE, delScale = delScale, printProgress = false, modCounter = 1000)\n",
    "Random.seed!(12345)\n",
    "gsPI = gEvaluationNewVFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMaxgEval, ve, vn, g, printProgress = false, modCounter = 100000)\n",
    "push!(simApproxPIG, gsPI[1])\n",
    "push!(exactApproxPIG, resPI[1])\n",
    "\n",
    "print(\"Simulated PI g: \")\n",
    "print(round(gsPI[1], digits = 5))\n",
    "print(\", Exact PI g: \")\n",
    "print(round(resPI[1], digits = 5))\n",
    "print(\", Optimality Gap: \")\n",
    "gap = (resPI[1]/rviGs[run] - 1)*100\n",
    "push!(optGapsApprox, gap)\n",
    "println(round(gap, digits = 2))\n",
    "#end frame\n",
    "println(join(fill(\"-\",LEN_FRAME)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "2117b4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243-element Vector{Any}:\n",
       " [1, 1, 1, 1, 1]\n",
       " [2, 1, 1, 1, 1]\n",
       " [3, 1, 1, 1, 1]\n",
       " [1, 2, 1, 1, 1]\n",
       " [2, 2, 1, 1, 1]\n",
       " [3, 2, 1, 1, 1]\n",
       " [1, 3, 1, 1, 1]\n",
       " [2, 3, 1, 1, 1]\n",
       " [3, 3, 1, 1, 1]\n",
       " [1, 1, 2, 1, 1]\n",
       " [2, 1, 2, 1, 1]\n",
       " [3, 1, 2, 1, 1]\n",
       " [1, 2, 2, 1, 1]\n",
       " ⋮\n",
       " [1, 3, 2, 3, 3]\n",
       " [2, 3, 2, 3, 3]\n",
       " [3, 3, 2, 3, 3]\n",
       " [1, 1, 3, 3, 3]\n",
       " [2, 1, 3, 3, 3]\n",
       " [3, 1, 3, 3, 3]\n",
       " [1, 2, 3, 3, 3]\n",
       " [2, 2, 3, 3, 3]\n",
       " [3, 2, 3, 3, 3]\n",
       " [1, 3, 3, 3, 3]\n",
       " [2, 3, 3, 3, 3]\n",
       " [3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stateSpace = enumerateStates(N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "1f1af851",
   "metadata": {},
   "outputs": [],
   "source": [
    "trueValues = []\n",
    "approxValues = []\n",
    "for s in stateSpace\n",
    "    push!(trueValues, resPE[2][s])\n",
    "    push!(approxValues, v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, resSMARPE[1][\"ve\"], resSMARPE[1][\"vn\"]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "68faca0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243-element Vector{Any}:\n",
       " [1, 1, 1, 1, 1]\n",
       " [1, 1, 1, 1, 3]\n",
       " [1, 1, 3, 1, 1]\n",
       " [1, 1, 3, 1, 3]\n",
       " [1, 1, 1, 3, 1]\n",
       " [1, 1, 1, 3, 3]\n",
       " [1, 1, 3, 3, 1]\n",
       " [1, 1, 3, 3, 3]\n",
       " [1, 2, 1, 1, 1]\n",
       " [1, 3, 1, 1, 1]\n",
       " [1, 2, 1, 1, 3]\n",
       " [1, 3, 1, 1, 3]\n",
       " [1, 2, 3, 1, 1]\n",
       " ⋮\n",
       " [3, 2, 3, 2, 2]\n",
       " [2, 3, 3, 2, 2]\n",
       " [3, 3, 3, 2, 3]\n",
       " [2, 2, 2, 2, 3]\n",
       " [3, 2, 2, 2, 3]\n",
       " [2, 3, 2, 2, 3]\n",
       " [3, 3, 3, 2, 2]\n",
       " [2, 2, 2, 2, 2]\n",
       " [3, 2, 2, 2, 2]\n",
       " [2, 3, 2, 2, 2]\n",
       " [3, 3, 2, 2, 3]\n",
       " [3, 3, 2, 2, 2]"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = sortperm(trueValues); trueValues .= trueValues[p]; approxValues .= approxValues[p]; stateSpace .= stateSpace[p] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "0542b9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 750x500 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(figsize=((3/2)*5, 5))\n",
    "gap = mean(trueValues - approxValues)\n",
    "ptSize = 2\n",
    "PyPlot.plot(trueValues , \"bo\", label = \"True\", ms = ptSize)\n",
    "PyPlot.plot(approxValues.+ gap, \"ro\", label = \"Approx\", ms = ptSize)\n",
    "legend()\n",
    "PyPlot.savefig(\"vfaVsTrue.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "932a0fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243-element Vector{Float64}:\n",
       " -107.91452235625638\n",
       " -160.1638461793782\n",
       " -147.5386114938192\n",
       " -199.27772737635678\n",
       " -176.32527267371552\n",
       " -227.55930381080907\n",
       " -214.51606529204673\n",
       " -263.7499698549036\n",
       " -114.01780754407446\n",
       " -114.85389441289954\n",
       " -166.23899958084166\n",
       " -167.07508644966674\n",
       " -153.60382219609892\n",
       "    ⋮\n",
       " -164.22171972924298\n",
       " -159.68105742185617\n",
       "  -44.693636879538644\n",
       " -163.4345998019071\n",
       " -168.81134897811808\n",
       " -164.27068667073218\n",
       "  -50.60275740594261\n",
       " -135.9210992438757\n",
       " -141.29784842008667\n",
       " -136.75718611270077\n",
       "  -56.24260738178964\n",
       "  -42.19548828237748"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaps = trueValues - approxValues .- gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "03587505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 1, 1, 1]\n",
      "[2, 2, 1, 1, 1]\n",
      "[3, 2, 1, 1, 1]\n",
      "[2, 3, 1, 1, 1]\n",
      "[3, 3, 1, 1, 3]\n",
      "[2, 2, 1, 1, 3]\n",
      "[3, 2, 1, 1, 3]\n",
      "[2, 3, 1, 1, 3]\n",
      "[3, 3, 3, 1, 1]\n",
      "[2, 2, 3, 1, 1]\n",
      "[3, 2, 3, 1, 1]\n",
      "[2, 3, 3, 1, 1]\n",
      "[3, 3, 1, 3, 1]\n",
      "[2, 2, 1, 3, 1]\n",
      "[3, 2, 1, 3, 1]\n",
      "[2, 3, 1, 3, 1]\n",
      "[2, 2, 3, 1, 3]\n",
      "[3, 2, 3, 1, 3]\n",
      "[2, 3, 3, 1, 3]\n",
      "[3, 3, 3, 1, 3]\n",
      "[2, 2, 1, 3, 3]\n",
      "[3, 2, 1, 3, 3]\n",
      "[2, 3, 1, 3, 3]\n",
      "[3, 3, 1, 3, 3]\n",
      "[2, 2, 3, 3, 1]\n",
      "[3, 2, 3, 3, 1]\n",
      "[2, 3, 3, 3, 1]\n",
      "[3, 3, 3, 3, 1]\n",
      "[3, 3, 1, 1, 2]\n",
      "[2, 2, 1, 1, 2]\n",
      "[3, 2, 1, 1, 2]\n",
      "[2, 3, 1, 1, 2]\n",
      "[2, 2, 3, 1, 2]\n",
      "[3, 2, 3, 1, 2]\n",
      "[2, 3, 3, 1, 2]\n",
      "[3, 3, 3, 1, 2]\n",
      "[2, 2, 1, 3, 2]\n",
      "[3, 2, 1, 3, 2]\n",
      "[2, 3, 1, 3, 2]\n",
      "[3, 3, 1, 3, 2]\n",
      "[3, 3, 2, 1, 1]\n",
      "[2, 2, 2, 1, 1]\n",
      "[3, 2, 2, 1, 1]\n",
      "[2, 3, 2, 1, 1]\n",
      "[2, 2, 2, 1, 3]\n",
      "[3, 2, 2, 1, 3]\n",
      "[2, 3, 2, 1, 3]\n",
      "[3, 3, 2, 1, 3]\n",
      "[2, 2, 2, 3, 1]\n",
      "[3, 2, 2, 3, 1]\n",
      "[2, 3, 2, 3, 1]\n",
      "[3, 3, 2, 3, 1]\n",
      "[2, 2, 2, 1, 2]\n",
      "[3, 2, 2, 1, 2]\n",
      "[2, 3, 2, 1, 2]\n",
      "[3, 3, 2, 1, 2]\n",
      "[3, 3, 1, 2, 1]\n",
      "[2, 2, 1, 2, 1]\n",
      "[3, 2, 1, 2, 1]\n",
      "[2, 3, 1, 2, 1]\n",
      "[2, 2, 1, 2, 3]\n",
      "[3, 2, 1, 2, 3]\n",
      "[2, 3, 1, 2, 3]\n",
      "[3, 3, 1, 2, 3]\n",
      "[2, 2, 3, 2, 1]\n",
      "[3, 2, 3, 2, 1]\n",
      "[2, 3, 3, 2, 1]\n",
      "[3, 3, 3, 2, 1]\n",
      "[2, 2, 1, 2, 2]\n",
      "[3, 2, 1, 2, 2]\n",
      "[2, 3, 1, 2, 2]\n",
      "[3, 3, 1, 2, 2]\n",
      "[2, 2, 2, 2, 1]\n",
      "[3, 2, 2, 2, 1]\n",
      "[2, 3, 2, 2, 1]\n",
      "[3, 3, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "test = [stateSpace[i] for i in 1:243 if gaps[i] >= 100]\n",
    "for t in test\n",
    "    println(t)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "267cbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = []\n",
    "for t in test\n",
    "    cand = t[3:5]\n",
    "    unique = true\n",
    "    for t2 in test2\n",
    "        if cand == t2\n",
    "            unique = false\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if unique\n",
    "        push!(test2, cand)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "d4b3a917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19-element Vector{Any}:\n",
       " [1, 1, 1]\n",
       " [1, 1, 3]\n",
       " [3, 1, 1]\n",
       " [1, 3, 1]\n",
       " [3, 1, 3]\n",
       " [1, 3, 3]\n",
       " [3, 3, 1]\n",
       " [1, 1, 2]\n",
       " [3, 1, 2]\n",
       " [1, 3, 2]\n",
       " [2, 1, 1]\n",
       " [2, 1, 3]\n",
       " [2, 3, 1]\n",
       " [2, 1, 2]\n",
       " [1, 2, 1]\n",
       " [1, 2, 3]\n",
       " [3, 2, 1]\n",
       " [1, 2, 2]\n",
       " [2, 2, 1]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "4af67b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = enumerateStates(3)\n",
    "options2 = []\n",
    "for o in options\n",
    "    found = false\n",
    "    for t2 in test2\n",
    "        if o == t2\n",
    "            found = true\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if !found\n",
    "        push!(options2, o)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "9f4d8f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Any}:\n",
       " [2, 2, 2]\n",
       " [3, 2, 2]\n",
       " [2, 3, 2]\n",
       " [3, 3, 2]\n",
       " [2, 2, 3]\n",
       " [3, 2, 3]\n",
       " [2, 3, 3]\n",
       " [3, 3, 3]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb42e594",
   "metadata": {},
   "source": [
    "All poor estimates have DIP links either damaged or repairing, and at least 1 non-DIP link healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "128343dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x7f54e358fe50>"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPlot.plot(gaps .+ 150, \"bo\", ms =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "7e1472cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Int64}:\n",
       " 1\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importantStates[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012972a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aab027",
   "metadata": {},
   "outputs": [],
   "source": [
    "median(optGapsApprox)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
