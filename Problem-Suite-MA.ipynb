{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c967fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using Random\n",
    "using Plots\n",
    "using PyPlot\n",
    "using StatsBase\n",
    "using StatsPlots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b1ef523",
   "metadata": {},
   "source": [
    "# Problem-Suite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebbc7791",
   "metadata": {},
   "source": [
    "Problem-Suite is a large structured notebook containing all of the functions created so far for this project.\n",
    "\n",
    "Sections:\n",
    "\n",
    "-[Miscellaneous Functions](#Miscellaneous-Functions)\n",
    "\n",
    "-[Pre-requisite functions for uniformised AVI](#Pre-requisite-functions-for-uniformised-AVI)\n",
    "\n",
    "-[Uniformised AVI functions](#Uniformised-AVI-functions)\n",
    "\n",
    "-[Pre-requisite functions for SMARVI](#Pre-requisite-functions-for-SMARVI)\n",
    "\n",
    "-[SMARVI Functions](#SMARVI-Functions)\n",
    "\n",
    "-[Pre-requisite Functions for Exact DP on Homogeneous Problems](#Pre-requisite-Functions-for-Exact-DP-on-Homogeneous-Problems)\n",
    "\n",
    "-[Exact DP on Homogeneous Problems (RVIA and PE/PI)](#Exact-DP-for-Homogeneous-problem)\n",
    "\n",
    "-[Pre-requisite Functions for Exact DP on Inhomogeneous Problems](#Pre-requisite-Functions-for-Exact-DP-on-Inhomogeneous-Problems)\n",
    "\n",
    "-[Exact DP on Inhomogeneous Problems (RVIA and PE/PI)](#Exact-DP-for-Inhomogeneous-Problem-(using-exact-h-or-VFA))\n",
    "\n",
    "-[Evaluation via simulation](#Evaluation-via-simulation)\n",
    "\n",
    "-[APE on Fully Active Policy](#APE-on-Fully-Active-Policy)\n",
    "\n",
    "-[SMARPE](#SMARPE)\n",
    "\n",
    "-[Tabular SMARVI and gEval](#tabular-smarvi-and-geval)\n",
    "\n",
    "-[SMART Functions](#SMART-Functions)\n",
    "\n",
    "-[New Functions](#new-functions)\n",
    "\n",
    "-[Tests](#Tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12fe12",
   "metadata": {},
   "source": [
    "# Miscellaneous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2556d9f",
   "metadata": {},
   "source": [
    "-Functions for enumerating state and action spaces\n",
    "\n",
    "-Functions for calculating flows given a state or state-action pair\n",
    "\n",
    "-Function for evaluating a VFA at a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a0da98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arrayToString (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#produce an array of array representations of all possible states\n",
    "function enumerateStates(N::Int64)\n",
    "    if N==1\n",
    "        return [[1],[2],[3]]\n",
    "    end\n",
    "    \n",
    "    output = []\n",
    "    lower = enumerateStates(N-1)\n",
    "    for s in lower\n",
    "        new1 = append!([1],s)\n",
    "        new2 = append!([2],s)\n",
    "        new3 = append!([3],s)\n",
    "        append!(output,[new1])\n",
    "        append!(output,[new2])\n",
    "        append!(output,[new3])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "#produce an array of array representations of all possible actions\n",
    "function enumerateActions(N::Int64)\n",
    "    if N==1\n",
    "        return [[0],[1]]\n",
    "    end\n",
    "    \n",
    "    output = []\n",
    "    lower = enumerateActions(N-1)\n",
    "    for a in lower\n",
    "        new1 = append!([0],a)\n",
    "        new2 = append!([1],a)\n",
    "        append!(output,[new1])\n",
    "        append!(output,[new2])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end    \n",
    "\n",
    "#produce array of array representations of all restricted, or single-repair, actions\n",
    "function enumerateRestrictedActions(N::Int64)\n",
    "    if N==1\n",
    "        return [[0],[1]]\n",
    "    end\n",
    "    \n",
    "    output = [zeros(Int64,N)]\n",
    "    for i in 1:N\n",
    "        temp = zeros(N)\n",
    "        temp[i] = 1\n",
    "        append!(output,[temp])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "#convert all array elements to string, then concatanate all elements (DEPRECATED AS DICTS CAN TAKE ARRAYS AS KEYS)\n",
    "function arrayToString(x)\n",
    "    return join(string.(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ba543e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateFlows (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function for calculating the flows given a state\n",
    "function calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    #update flows\n",
    "    flows = zeros(N)\n",
    "    healthy = sum(i == 1 for i in s)\n",
    "    \n",
    "    #if no links are healthy, return \n",
    "    if healthy == 0\n",
    "        return flows, c1\n",
    "    end\n",
    "    \n",
    "    #otherwise, find best route, and return\n",
    "    bestCost = maximum(c0) + 1\n",
    "    usedLink = 0\n",
    "    for k in 1:N\n",
    "        if s[k] == 1 && c0[k] < bestCost\n",
    "            bestCost = c0[k]\n",
    "            usedLink = k\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[usedLink] = beta\n",
    "    \n",
    "    return flows, bestCost\n",
    "end\n",
    "\n",
    "#function for calculating the flows given a state-action pair\n",
    "function calculateFlows(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    sPrime = s - a\n",
    "    return calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127d3f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate a VFA at a given state\n",
    "function v(s, params, features)\n",
    "    numFeatures = length(features)\n",
    "    return params[1] + sum(params[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960bf4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 2 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version of v that takes flows for the features\n",
    "function v(s, flows, params, features)\n",
    "    N = length(params)\n",
    "    return params[1] + sum(params[i]*features[i-1](s, flows) for i in 2:N)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05d0fc",
   "metadata": {},
   "source": [
    "# Pre-requisite functions for uniformised AVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c56072f",
   "metadata": {},
   "source": [
    "This section contains functions used within the AVI algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "476755cd",
   "metadata": {},
   "source": [
    "Given a state-action pair, return the next random pre-decision state, the instant cost, and the updated flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d157b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateStateAndFlowsUnif (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateStateAndFlowsUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #observe exogenous information\n",
    "    w = rand(Uniform(0, 1))\n",
    "    \n",
    "    #interpret exog info: is it a demand deg, rare deg, or completed repair \n",
    "    found = false\n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        if runningTotal <= w <= runningTotal + flows[k]*alpha_d[k]*del\n",
    "            found = true\n",
    "            sPrime[k] = 3\n",
    "            #println(\"Demand Deg at \"*string.(k))\n",
    "            break\n",
    "        end\n",
    "        runningTotal = runningTotal + flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    if found == false\n",
    "        for k in 1:N\n",
    "            if runningTotal <= w <= runningTotal + alpha_r[k]*del\n",
    "                found = true\n",
    "                sPrime[k] = 3\n",
    "                #println(\"Rare Deg at \"*string.(k))\n",
    "                break\n",
    "            end\n",
    "            runningTotal = runningTotal + alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if found == false && repair > 0\n",
    "        if runningTotal <= w <= runningTotal + tau(repair)*del\n",
    "            found = true\n",
    "            #find all repairing links\n",
    "            repairing = []\n",
    "            for k in 1:N\n",
    "                if sPrime[k] == 2\n",
    "                    append!(repairing,[k])\n",
    "                end\n",
    "            end\n",
    "            repaired = sample(repairing)\n",
    "            sPrime[repaired] = 1\n",
    "            #println(\"Repair completed at \"*string.(repaired))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if found == false\n",
    "        #println(\"No Event\")\n",
    "    end\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    \n",
    "    return sPrime, (beta*bestCost + sum(r[k]*(sPrime[k]==2) for k in 1:N))*del, newFlows\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "836f070e",
   "metadata": {},
   "source": [
    "Given a state action pair, return the instant cost over the delta timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "685eb320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostUnif (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instant cost over the timestep\n",
    "function instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(sPrime[i] == 1 for i in 1:N)\n",
    "    repair = sum(sPrime[i] == 2 for i in 1:N)\n",
    "    damaged = sum(sPrime[i] == 3 for i in 1:N)\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    \n",
    "    return (beta*bestCost + sum(r[k]*(sPrime[k]==2) for k in 1:N))*del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9603d5cd",
   "metadata": {},
   "source": [
    "Given a state-action pair and a VFA, calculate the expected value of the value function after one timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0a307f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueUnif (generic function with 2 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h. Also used in Exact PE/PI when using a VFA\n",
    "#One version takes flows as an argument, the other calculates the flows\n",
    "function expectedNextValueUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*v(sPrime, vParams, features)\n",
    "end  \n",
    "\n",
    "function expectedNextValueUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, vParams, features)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    flows = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*v(sPrime, vParams, features)\n",
    "end  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3920172",
   "metadata": {},
   "source": [
    "# Uniformised AVI functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80cbe1e5",
   "metadata": {},
   "source": [
    "Algorithms that perform RAVI on the uniformised version of the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5310d49c",
   "metadata": {},
   "source": [
    "Given some parallel link problem and VFA architecture, perform RAVI, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f83714d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in uniformised setting, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state\n",
    "function aviApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - g\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + v(sPrime, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "718d21d7",
   "metadata": {},
   "source": [
    "Given some parallel link problem and VFA architecture, perform RAVI, using a full expectation for update targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fb36c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviFull (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function aviFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - g\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d76fa41",
   "metadata": {},
   "source": [
    "Similar to above, but only uses the Binary Action Space (BAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ebdac30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviUnifBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI with BAS in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function aviUnifBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        vs0 = v(s0, vParams, features)\n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - vs0\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        testV = instantCostUnif(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - vs0\n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA = testA\n",
    "            optV = testV\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f587fd5b",
   "metadata": {},
   "source": [
    "# Pre-requisite functions for SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2008a2c4",
   "metadata": {},
   "source": [
    "Helper functions for the SMARVI algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1f36cda",
   "metadata": {},
   "source": [
    "Given a state-action pair and pre-calculated flows, return the expected sojourn time for the state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1506350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sojournTime (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the expected sojourn time of a state-action pair\n",
    "function sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    s = s - a\n",
    "    if s == fill(3,N)\n",
    "        return 1/(beta*sum(alpha_d) + sum(alpha_r) + tau(N))\n",
    "    end\n",
    "    \n",
    "    numRep = sum(i == 2 for i in s)\n",
    "    cumulativeRate = 0.0\n",
    "    for i in 1:N\n",
    "        if s[i] == 1\n",
    "            cumulativeRate += flows[i]*alpha_d[i] + alpha_r[i]\n",
    "        elseif s[i] == 2\n",
    "            cumulativeRate += alpha_r[i] + tau(numRep)/numRep\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return 1/cumulativeRate\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb2c6d2b",
   "metadata": {},
   "source": [
    "Given a state-action pair and flows, calculate the expected cost accumulated until a transition occurs, or calculate the simulated cost accumulated over a simulated time del."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66c5307b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostCont (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the expected cost accumulated until a transition \n",
    "function instantCostCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows; del = 0)\n",
    "    if del == 0\n",
    "        del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    end\n",
    "    \n",
    "    return instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ecc13f2",
   "metadata": {},
   "source": [
    "Given a state-action pair, return the next random pre-decision state, the cost accumulated over the sojourn time, and the updated flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75587424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateStateAndFlowsCont (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given a state-action pair, return the next random pre-decision state, the cost accumulated over the sojourn time, and the updated flows\n",
    "function updateStateAndFlowsCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    actualTime = rand(Exponential(del))\n",
    "    result = updateStateAndFlowsUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "    return result[1], instantCostCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows; del = actualTime), result[3], actualTime\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae4efc43",
   "metadata": {},
   "source": [
    "Given a state-action pair, precalculated flows, and a VFA, return the expected value of the VFA after a transition has occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60cda4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueCont (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h\n",
    "function expectedNextValueCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime,vParams,features)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc5b77a7",
   "metadata": {},
   "source": [
    "Similar to above, but assumes the features of the VFA take precalcuated flows as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "488f16a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContFlows (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h\n",
    "#Assumes the VFA is constructed with features taking arguments (s, flows), so flows are precalculated\n",
    "function expectedNextValueContFlows(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime, flows, vParams,features)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, flowsNext, vParams, features)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, flowsNext, vParams, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, flowsNext, vParams, features)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a2e614d",
   "metadata": {},
   "source": [
    "Given a state, flows, and a VFA-g pair, return the optimal action and associated V value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b01347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "    #formulate optimal action and calculate optV\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "    \n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            testV = v(s-a, vParams, features)\n",
    "            if testV <= optV\n",
    "                optV = testV\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #If wanted, force a repair if optA is passive for [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA = zeros(Int64,N)\n",
    "        optA[1] = 1\n",
    "        optV = v(s-optA, vParams, features)\n",
    "        \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                testV = v(s-a, vParams, features)\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f824e",
   "metadata": {},
   "source": [
    "# SMARVI Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dbccab0",
   "metadata": {},
   "source": [
    "Variety of functions which perform the SMARVI algorithm, with different additional features.\n",
    "For clarity, the \"state-trace\" is a method which collects a sequence of states connected by instantaneous actions together, and ensures they all have the same update target. For example, if in state s we take action a!=0, and in the resulting state s+a we take action 0, both s and s+a will have update target (c + E(V(s')) - gt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39c05112",
   "metadata": {},
   "source": [
    "Given a problem and a VFA architecture, perform SMARVI, with no e-greedy action selection or state trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea289cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs SMARVI\n",
    "function smarvi(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action and calculate optV\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = optV - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5c6e48e",
   "metadata": {},
   "source": [
    "Perform SMARVI with a state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2005234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviST (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in continuous time setting, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "function smarviST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    \n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    actionFlag = false\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update stateTrace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate optimal action and calculate optV\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #if optimal action is passive, update VFA for all states in the stateTrace, and simulate the next state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            \n",
    "            #find simulated next state\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "        \n",
    "            bestV = optV - v(s0, vParams, features)\n",
    "            \n",
    "            #update VFA\n",
    "            traceLength = length(stateTrace)\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            #reset stateTrace\n",
    "            stateTrace = []\n",
    "            \n",
    "            #update flows and average\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #if some action is optimal, simply update the state\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58458323",
   "metadata": {},
   "source": [
    "Performs SMARVI with a given fixed value of g0 for action selection. This prevents bad initial estimates of g from severely impacting the algorithm, but restricts the policy space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1840b7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions, and controlling action selection using some fixed g0\n",
    "function smarvi_g0(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g0)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV + g0*t - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25861988",
   "metadata": {},
   "source": [
    "Similar to regular SMARVI, but only using the BAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "318f0267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI with BAS in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "function smarviBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        tPassive = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tPassive\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        tActive = sojournTime(s, testA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        testV = instantCostCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tActive\n",
    "        \n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        #Ignore passive action for broken network\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d439d00",
   "metadata": {},
   "source": [
    "Similar to regular SMARVI, but where flows are assumes to be passed to the VFA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0daaef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviFlows (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version of smarvi where flows are passed to features\n",
    "function smarviFlows(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    flows0 = copy(flows)\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContFlows(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                \n",
    "                if vParams[1] + sum(vParams[i+1]*features[i](s-a, flows) for i in 1:numFeatures) <= optV\n",
    "                    optV = vParams[1] + sum(vParams[i+1]*features[i](s-a, flows) for i in 1:numFeatures)\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, flows, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    testV = v(s-a, flows, vParams, features)\n",
    "                    if testV <= optV\n",
    "                        optV = testV\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - v(s0, flows0, vParams,features)\n",
    "        else\n",
    "            bestV = optV - v(s0, flows0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s, flows) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s, flows) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8365b78",
   "metadata": {},
   "source": [
    "## e-greedy SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f3cba33",
   "metadata": {},
   "source": [
    "Helper functions and main algorithm for e-greedy SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fd55c88",
   "metadata": {},
   "source": [
    "Samples a random feasible action for a state s of length N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2fd2e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randomAction (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate random action\n",
    "function randomAction(s,N)\n",
    "    #deal with all-damaged case\n",
    "    if s == fill(3,N)\n",
    "        return randomActionAllDamaged(N)\n",
    "    end\n",
    "\n",
    "    damaged = [0]\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            append!(damaged, [i])\n",
    "        end\n",
    "    end\n",
    "            \n",
    "    choice = sample(damaged)\n",
    "    optA = zeros(Int64, N)\n",
    "    if choice == 0\n",
    "        return optA\n",
    "    else\n",
    "        optA[choice] = 1\n",
    "        return optA\n",
    "    end\n",
    "end\n",
    "\n",
    "#calculate random action for [3,3...,3] state\n",
    "function randomActionAllDamaged(N)\n",
    "    choice = sample(1:N)\n",
    "    a = zeros(Int64, N)\n",
    "    a[choice] = 1\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48f72466",
   "metadata": {},
   "source": [
    "Performs an e-greedy version of SMARVI, with e_n = b/b+n for some given b. Allows SMARVI to perform some exploratory actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a38d016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi_epsGreedy (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#actions are choosen via e-greedy action selection, where a random action is chosen with probability b/b+n\n",
    "function smarvi_epsGreedy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; b = 1.0, printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate e-greedy action\n",
    "        if rand(Uniform(0,1)) <= b/(b + n) \n",
    "            optA = randomAction(s,N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "            else\n",
    "                optV = v(s-optA, vParams, features)\n",
    "            end\n",
    "        else                \n",
    "            optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "            optA = optAandV[1]\n",
    "            optV = optAandV[2]\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eaa235",
   "metadata": {},
   "source": [
    "# Pre-requisite Functions for Exact DP on Homogeneous Problems "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7b7c104",
   "metadata": {},
   "source": [
    "Helper functions for Homogeneous Exact DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1801225",
   "metadata": {},
   "source": [
    "Calculates instant cost for homogeneous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22678b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instant cost function strictly for homogeneous problem\n",
    "function instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    i1Prime = i1 - a\n",
    "    i2Prime = i2 + a\n",
    "    \n",
    "    #if no links are healthy, return \n",
    "    if N - i1 - i2 == 0\n",
    "        return (beta*c1 + r*i2Prime)*del\n",
    "    end\n",
    "    \n",
    "    \n",
    "    return (beta*c0 + r*i2Prime)*del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94e504b7",
   "metadata": {},
   "source": [
    "Given state (i_1, i_2) and action a, calculates the expected next value function after one timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a17eacf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates E(h(s')) from s,a strictly for a homogeneous problem\n",
    "function expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    i1Prime = i1 - a\n",
    "    i2Prime = i2 + a\n",
    "    thisH = h[i1+1,i2+1]\n",
    "    \n",
    "    #if all are damaged\n",
    "    if i1Prime == N\n",
    "        return thisH\n",
    "    end\n",
    "    \n",
    "    #if none are healthy\n",
    "    if N - i1 - i2 == 0\n",
    "        return thisH + tau(i2Prime)*del*(h[i1Prime+1,i2Prime-1+1] - thisH) + i2Prime*del*alpha_r*(h[i1Prime+1+1, i2Prime-1+1] - thisH)\n",
    "    end\n",
    "    \n",
    "    #if none are repairing\n",
    "    if i2Prime == 0\n",
    "        return thisH + (beta*alpha_d + (N - i1 - i2)*alpha_r)*del*(h[i1Prime+1+1,i2Prime+1] - thisH)\n",
    "    end\n",
    "    \n",
    "    return thisH + (beta*alpha_d + (N - i1 - i2)*alpha_r)*del*(h[i1Prime+1+1,i2Prime+1] - thisH) + i2Prime*alpha_r*del*(h[i1Prime+1+1,i2Prime-1+1] - thisH) + tau(i2Prime)*del*(h[i1Prime+1,i2Prime-1+1] - thisH)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68c7af12",
   "metadata": {},
   "source": [
    "Given state (i_1,i_2) and value function h, calculates and returns the best action for the state using full expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beeb2b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the exact PI action strictly for a homogeneous problem\n",
    "function piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "    if i1 == 0\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    if i1 == N && forceActive\n",
    "        optA = 1\n",
    "        optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        for a in 2:i1\n",
    "            testH = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            if testH <= optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "        return optA\n",
    "    end\n",
    "    \n",
    "    optA = 0\n",
    "    optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    for a in 1:i1\n",
    "        testH = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        if testH <= optH\n",
    "            optA = a\n",
    "            optH = testH\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71928099",
   "metadata": {},
   "source": [
    "Similar to above, but uses the approximation Q(s,a) = h(s+a) for a!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b58a1177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the approx PI action based on instantaneous approximation, strictly for a homogeneous problem\n",
    "function piActionHomogApprox(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = false)\n",
    "    #deal with \"nothing damaged\" edge case\n",
    "    if i1 == 0\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    #deal with \"everything damaged\" edge case\n",
    "    if i1 == N && forceActive\n",
    "        optA = 1\n",
    "        optH = h[i1-optA+1,i2+optA+1]\n",
    "        for a in 2:i1\n",
    "            testH = h[i1-a+1,i2+a+1]\n",
    "            if testH <= optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "        return optA\n",
    "    end\n",
    "    \n",
    "    optA = 0\n",
    "    optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) - g*del\n",
    "    for a in 1:i1\n",
    "        testH = h[i1-a+1,i2+a+1]\n",
    "        if testH <= optH\n",
    "            optA = a\n",
    "            optH = testH\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8ab05cf",
   "metadata": {},
   "source": [
    "Given h, constructs optimal policy using exact PI method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6335a065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the exact PI policy strictly for a homogeneous problem\n",
    "function piPolicyHomog(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "    policy = zeros(Int64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            policy[i1+1,i2+1] = piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34525984",
   "metadata": {},
   "source": [
    "Given h, constructs optimal policy using Q(s,a) = h(s,a) for a!=0 approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "404bfec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the approx PI policy based on instantaneous approximation, strictly for a homogeneous problem\n",
    "function piPolicyHomogApprox(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = false)\n",
    "    policy = zeros(Int64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            policy[i1+1,i2+1] = piActionHomogApprox(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = forceActive)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905b77bf",
   "metadata": {},
   "source": [
    "Constructs a h table from a VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42156f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hFromVFAHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hFromVFAHomog(N, params, features)\n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return hIn\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c11abe",
   "metadata": {},
   "source": [
    "# Exact DP for Homogeneous problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fc32308",
   "metadata": {},
   "source": [
    "Actual DP algorithms "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99f7e6cf",
   "metadata": {},
   "source": [
    "Given a h table, performs PE on PI policy derived from h, and returns g, h, n (number of iterations), and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc148338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given exact h function, strictly for a homogeneous problem \n",
    "function rpiHomog(N::Int64, hIn, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    policy = piPolicyHomog(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = policy[i1+1,i2+1]\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b8046e6",
   "metadata": {},
   "source": [
    "Performs PE on the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e39012f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpeFAHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates the fully active policy, strictly for a homogeneous problem \n",
    "function rpeFAHomog(N::Int64, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = i1\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c051660",
   "metadata": {},
   "source": [
    "Similar to rpiHomog, but uses Q(s,a) = h(s,a) approximation for PI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "964c01ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates an approximate PI policy based on a given exact h function and instananeous actions, strictly for a homogeneous problem \n",
    "function rpiHomogApprox(N::Int64, hIn, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64, g::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    policy = piPolicyHomogApprox(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = forceActive)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = policy[i1+1,i2+1]\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e1c1de",
   "metadata": {},
   "source": [
    "Similar to above, but uses VFA as input h, and uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7146cc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogVFAApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given VFA, using instananeous approximation, strictly for a homogeneous problem \n",
    "function rpiHomogVFAApprox(N::Int64, params, features, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64, g::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    \n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #run standard function\n",
    "    return rpiHomogApprox(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = nMax, delScale = delScale, forceActive = forceActive)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54d455db",
   "metadata": {},
   "source": [
    "Similar to above, WITHOUT Q(s,a) = h(s+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04dc9704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given VFA, strictly for a homogeneous problem \n",
    "function rpiHomogVFA(N::Int64, params, features, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    \n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #run standard function\n",
    "    return rpiHomog(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, forceActive = forceActive)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "690683ed",
   "metadata": {},
   "source": [
    "Performs RVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939c3f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rviHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs RVIA \n",
    "function rviHomog(N::Int64, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b3849",
   "metadata": {},
   "source": [
    "# Pre-requisite Functions for Exact DP on Inhomogeneous Problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1e53ece",
   "metadata": {},
   "source": [
    "Helper functions for inhomogeneous exact DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14ea586f",
   "metadata": {},
   "source": [
    "Given a state-action pair and h, calculates the expected next value of the value function after one timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d45adbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueExact (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates E(h(s')) from s,a using exact h table\n",
    "function expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    \n",
    "    flows = zeros(Float64, N)\n",
    "    if healthy > 0\n",
    "        #otherwise, find best route, and return\n",
    "        bestCost = maximum(c0) + 1\n",
    "        usedLink = 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 1 && c0[k] < bestCost\n",
    "                bestCost = c0[k]\n",
    "                usedLink = k\n",
    "            end \n",
    "        end\n",
    "        \n",
    "        flows[usedLink] = beta\n",
    "    end\n",
    "    \n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*h[sNext]\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*h[sNext]\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*h[sNext]\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*h[sPrime]\n",
    "end "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f2dad95",
   "metadata": {},
   "source": [
    "Given a state and a h table, calculates the PI action for s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b71c013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExact (generic function with 1 method)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table\n",
    "function piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2cb0d43",
   "metadata": {},
   "source": [
    "Similar to above, but uses the approximation Q(s,a) = h(s+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17b1bbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table, based off instananeous actions\n",
    "function piActionExactInstant(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h) - g*del\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = h[s-a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7eb2f91a",
   "metadata": {},
   "source": [
    "Similar to above, but uses a VFA instead of a h table, WITHOUT Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2bab1aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using a VFA\n",
    "function piActionVFA(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4096d38",
   "metadata": {},
   "source": [
    "Similar to above, WITH Q(s,a) = h(s,a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "604bf237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using a VFA and instananeous actions\n",
    "function piActionVFAInstant(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features) - g*del\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = v(s-a,params,features)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884618d7",
   "metadata": {},
   "source": [
    "Constructs PI policy using h table and no approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1dfe8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExact (generic function with 1 method)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy using exact h table\n",
    "function piPolicyExact(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8842b6de",
   "metadata": {},
   "source": [
    "Constructs PI policy using h table and Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfb9dcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy using exact h table, interpretting h with instant actions\n",
    "function piPolicyExactInstant(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactInstant(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d2fa28",
   "metadata": {},
   "source": [
    "Constructs PI policy using VFA and no approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72a89a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy from a VFA\n",
    "function piPolicyVFA(params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionVFA(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d72cd38",
   "metadata": {},
   "source": [
    "Constructs PI policy using VFA and Q(s,a) = h(s+a) approximation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14bba9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy from a VFA, using instant actions to interpret h\n",
    "function piPolicyVFAInstant(params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionVFAInstant(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f8cc379",
   "metadata": {},
   "source": [
    "Constructs h table from VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function hFromVFAInhomog(N, params, features)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = v(s, params. features)\n",
    "    end\n",
    "\n",
    "    return h\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7172ee4",
   "metadata": {},
   "source": [
    "# Exact DP for Inhomogeneous Problem (using exact h or VFA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43e2ee1b",
   "metadata": {},
   "source": [
    "DP algorithms for inhomogeneous problem\n",
    "\n",
    "Note that throughout when we talk of a Q(s,a) = h(s+a) approximation, this only refers to action selection and not update rules, and excludes a=0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2225adc2",
   "metadata": {},
   "source": [
    "Given an explicit policy table, performs PE, returns g, h and n (# of iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b118e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpe (generic function with 1 method)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs PE using exact policy table\n",
    "function rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    h = Dict()\n",
    "    w = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        w[s] = 0.0\n",
    "    end\n",
    "    s0  = fill(1, N)\n",
    "    n = 0\n",
    "    \n",
    "    #do until max iterations met or epsilon convergence\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #find updates for every state\n",
    "        for s in stateSpace\n",
    "            a = policy[s]\n",
    "            w[s] = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "        end\n",
    "        \n",
    "        #calculate relative values and delta\n",
    "        delta = 0\n",
    "        for s in stateSpace\n",
    "            update = w[s] - w[s0]\n",
    "            if delta < update - h[s] || delta == 0\n",
    "                delta = update - h[s]\n",
    "            end\n",
    "            \n",
    "            h[s] = update\n",
    "        end\n",
    "        \n",
    "        #stopping condition\n",
    "        if delta < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if printProgress && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    a = zeros(Int64, N)\n",
    "    g = instantCostUnif(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h) - h[s0]\n",
    "    \n",
    "    return g/del, h, n\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1ad5a2b",
   "metadata": {},
   "source": [
    "Given a h table, constructs PI policy and performs PE, returning g, h, n and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c57636f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiExact (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using exact h table\n",
    "function rpiExact(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    policy = piPolicyExact(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    output = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "    return output[1], output[2], output[3], policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec78eb5a",
   "metadata": {},
   "source": [
    "Similar to above, but uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266027ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using exact h table, using instant actions to interpet h\n",
    "function rpiExactInstant(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    policy = piPolicyExactInstant(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    output = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "    return output[1], output[2], output[3], policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa6660e",
   "metadata": {},
   "source": [
    "Performs PE on the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f773f8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpeFA (generic function with 1 method)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs exact PE on the fully-active policy\n",
    "function rpeFA(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = faAction(s)\n",
    "    end\n",
    "    \n",
    "    return rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c325a927",
   "metadata": {},
   "source": [
    "Performs PE on fully passive policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d304fb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpePassive (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs exact PE on the passive policy\n",
    "function rpePassive(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    return rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52c45280",
   "metadata": {},
   "source": [
    "Given a VFA, constructs PI policy and performs PE, returning g, h, n and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ec6cacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using VFA\n",
    "function rpiVFA(N, params, features, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    hIn = hFromVFAInhomog(N, params, features)\n",
    "    return rpiExact(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68209e10",
   "metadata": {},
   "source": [
    "Similar to above, but uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a19903b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using VFA and instantaneous actions to interpret h\n",
    "function rpiVFAInstant(N, params, features, g, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    hIn = hFromVFAInhomog(N, params, features)\n",
    "    return rpiExactInstant(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b0b6c10",
   "metadata": {},
   "source": [
    "Performs RVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d38fbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rvi (generic function with 1 method)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs RVIA\n",
    "function rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    h = Dict()\n",
    "    w = Dict()\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        w[s] = 0.0\n",
    "        policy[s] = zeros(Int64,N)\n",
    "    end\n",
    "    s0  = fill(1, N)\n",
    "    n = 0\n",
    "    \n",
    "    #do until max iterations met or epsilon convergence\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #find updates for every state\n",
    "        for s in stateSpace\n",
    "            a = piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            w[s] = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "        end\n",
    "        \n",
    "        #calculate relative values and delta\n",
    "        delta = 0\n",
    "        for s in stateSpace\n",
    "            update = w[s] - w[s0]\n",
    "            if delta < update - h[s] || delta == 0\n",
    "                delta = update - h[s]\n",
    "            end\n",
    "            \n",
    "            h[s] = update\n",
    "        end\n",
    "        \n",
    "        #stopping condition\n",
    "        if delta < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if printProgress && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    a = zeros(Int64, N)\n",
    "    g = instantCostUnif(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h) - h[s0]\n",
    "    \n",
    "    return g/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb424a1e",
   "metadata": {},
   "source": [
    "# Evaluation via simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bcbbb37",
   "metadata": {},
   "source": [
    "Various evaluation functions for approximating g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ef631d9",
   "metadata": {},
   "source": [
    "Takes a trained VFA and learns g, using g also for control, starting from state s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8261907a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA via PI using simulation\n",
    "function gEvaluation(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, printState = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if printState\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "320c63e1",
   "metadata": {},
   "source": [
    "Similar to above, but starting from a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fd18438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFromS (generic function with 1 method)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluationFromS(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    flowResult = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    flows = flowResult[1]\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end \n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ceb6a98",
   "metadata": {},
   "source": [
    "Similar to gEvaluation, but uses a fixed g0 for control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba5ce5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluation_g0(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3cd3eae",
   "metadata": {},
   "source": [
    "Similar to above, but starts from a given state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b85a37ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFromS_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluationFromS_g0(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    flowResult = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    flows = flowResult[1]\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end \n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6636431",
   "metadata": {},
   "source": [
    "Finds the g of the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33be91d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFA (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates the FA policy\n",
    "function gEvaluationFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate FA action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d651dbd0",
   "metadata": {},
   "source": [
    "Similar to gEvaluation, but only uses the BAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8ba1023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA via PI using simulation\n",
    "function gEvaluationBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        tPassive = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tPassive\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        tActive = sojournTime(s, testA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        testV = instantCostCont(s,testA, N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tActive\n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA = testA\n",
    "            optV = testV\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            append!(runningTotals, [runningTotal])\n",
    "            timePassed += time\n",
    "            append!(times,[timePassed])\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d81acf",
   "metadata": {},
   "source": [
    "Similar to gEvaluation_g0, but assumes that flows are passed to the VFA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "862863bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation_g0_flows (generic function with 1 method)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluation_g0_flows(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContFlows(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, flows, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, flows, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, flows, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88307ffe",
   "metadata": {},
   "source": [
    "# APE on Fully Active Policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7749e9f",
   "metadata": {},
   "source": [
    "Performs APE on the Fully Active Policy using each of the four approaches to estimating a VFA (mixes of uniform/smar and simulated-next-state/expectation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "666815fa",
   "metadata": {},
   "source": [
    "Returns the FA action for a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1319dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "faAction (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computes the Fully Active action for a given state s\n",
    "function faAction(s)\n",
    "    N = length(s)\n",
    "    a = zeros(Int64,N)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a[i] = 1\n",
    "        end\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9168d0b",
   "metadata": {},
   "source": [
    "Evaluates the FA policy using a VFA and uniformisation, and update targets c + V(s') - gt, where s' is simulated next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e894ba39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAUnifApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE of FA policy in uniformised setting, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state\n",
    "function apeFAUnifApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + v(sPrime, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e96581e",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation for updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ccde3d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAUnifFull (generic function with 1 method)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE of FA policy in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function apeFAUnifFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + expectedNextValueUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d7bdaa9",
   "metadata": {},
   "source": [
    "Performs SMARPE on FA policy, with update target c + V(s') - gt where s' is the next simulated state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd7df03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAContApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs SMARPE on FA policy, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "function smarpeFAApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + v(sPrime, vParams, features) - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af073bb3",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation in update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8865800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAContFull (generic function with 1 method)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarpeFAFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + expectedNextValueCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ceece53",
   "metadata": {},
   "source": [
    "Similar to smarpeFAApprox, but incorporates state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b68eb6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAContApproxST (generic function with 1 method)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE on FA policy in continuous time setting, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "#Also incorporates the state trace when actions are taken\n",
    "function smarpeFAApproxST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update state trace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #for passive action, do proper update\n",
    "        if bestA == zeros(Int64,N)\n",
    "            #find value of v^n\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + v(sPrime, vParams, features) - g*t - v(s0, vParams,features)\n",
    "\n",
    "            #update VFA\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            stateTrace = []\n",
    "            \n",
    "            #update g, state, and flows\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #for other action, simply update state and move on\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c19d280d",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation for update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47d8caaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAContFullST (generic function with 1 method)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE on FA policy in continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Also incorporates the state trace when actions are taken\n",
    "function smarpeFAFullST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update state trace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #for passive action, do proper update\n",
    "        if bestA == zeros(Int64,N)\n",
    "            #find value of v^n\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + expectedNextValueCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t - v(s0, vParams,features)\n",
    "\n",
    "            #update VFA\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            stateTrace = []\n",
    "            \n",
    "            #update g, state, and flows\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #for other action, simply update state and move on\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f5b89",
   "metadata": {},
   "source": [
    "# SMARPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54992054",
   "metadata": {},
   "source": [
    "## Semi-Markov Approximate Relative Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be11b69",
   "metadata": {},
   "source": [
    "- SMARPE takes some trained VFA as input, and seeks to learn the associated long run cost g and the value function of the policy derived from the given VFA\n",
    "\n",
    "- Standard SMARPE uses the online training value of g for action selection, allowing the policy to vary throughout training\n",
    "\n",
    "- SMARPE_g0 takes a pre-learned value of g0 to be used for action selection, keeping the policy constant throughout. This value of g0 might be taken directly from SMARVI or from some gEval function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffc082",
   "metadata": {},
   "source": [
    "Worth also discussing is the exact behaviour of the gEval functions.\n",
    "\n",
    "- Standard gEval simply evaluates a VFA, and learns g throughout. In turn, this value of g is used for action selection, so the policy may vary throughout evaluation.\n",
    "\n",
    "- gEval_g0 evaluates a VFA-g0 pair, keeping the policy constant throughout. It may be good practice to always follow standard gEval with gEval_g0, due to the lack of policy variability.\n",
    "\n",
    "gEval functions are the part that actually calculate the PI actions based on the VFAs derived from SMARPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a0e5de3",
   "metadata": {},
   "source": [
    "Given a VFA-g pair, evaluates the PI policy derived from the pair via a new VFA with the same architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6082e341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpe_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE in the continuous time setting, approximating E(h(s')) using all possible transitions, and with a fixed g0 for action selection\n",
    "function smarpe(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, paramsIn, paramsOut, features, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [paramsOut]\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, paramsIn, features, g0)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #recalculate optA in terms of new VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, paramsOut, features) - g*t\n",
    "        else\n",
    "            optV = v(s - bestA, paramsOut, features)\n",
    "        end \n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, paramsOut ,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, paramsOut, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        paramsOut = paramsOut + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[paramsOut])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs, [g])\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return paramsOut, paramHist, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d24567",
   "metadata": {},
   "source": [
    "# Tabular SMARVI and gEval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b101ab9",
   "metadata": {},
   "source": [
    "Tabular SMARVI algorithms (non e-greedy, e-greedy, and e-greedt with state trace), associated gEval function, and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d500f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromTable (generic function with 1 method)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        \n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "               \n",
    "            if h[s-a] <= optV\n",
    "                optV = h[s-a]\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    #Fix random link if optA is passive for [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optV = h[s-optA]\n",
    "            \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                testV = h[s-a]\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd0e5b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContTab (generic function with 1 method)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and tabular h\n",
    "function expectedNextValueContTab(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return h[sPrime]\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*h[sNext]\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*h[sNext]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*h[sNext]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6447836b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab (generic function with 1 method)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Uses tabular representation instead of VFA\n",
    "function smarviTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - h[s0]\n",
    "        else\n",
    "            bestV = optV - h[s0]\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = h[s]\n",
    "        h[s] += (b/(b + numVisits[s]))*(bestV - currentEst)\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf860122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab_epsGreedy (generic function with 1 method)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Uses tabular representation instead of VFA and e-greedy action selection\n",
    "function smarviTab_epsGreedy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = h[s]\n",
    "        h[s] += (b/(b + numVisits[s]))*(bestV - currentEst)\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "de3e948e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab_epsGreedyST (generic function with 1 method)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Uses tabular representation instead of VFA, e-greedy action selection, and state trace \n",
    "function smarviTab_epsGreedyST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        #choose actual action\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0f86bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTabMA (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tabular SMARVI with moving average online approximation for g, e-greedy action selection, and state trace\n",
    "function smarviTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; window = 2500000, printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    totalCosts = [0.0]\n",
    "    timePassed = 0.0\n",
    "    totalTimes = [0.0]\n",
    "    lenTotals = 1\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        #choose actual action\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            push!(totalCosts, runningTotal)\n",
    "            push!(totalTimes, timePassed)\n",
    "            lenTotals += 1\n",
    "            if lenTotals <= window\n",
    "                g = runningTotal/timePassed\n",
    "            else\n",
    "                g = (runningTotal - totalCosts[lenTotals - window])/(timePassed - totalTimes[lenTotals - window])\n",
    "            end\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a89e1d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactCont (generic function with 1 method)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table, based off continuous model\n",
    "function piActionExactCont(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optH = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows,h) - g*t\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = h[s-a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optH = h[s - optA]\n",
    "\n",
    "        for i in 2:N\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            testH = h[s - a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47ec161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactCont (generic function with 1 method)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piPolicyExactCont(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactCont(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "161485ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationTab (generic function with 1 method)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, h, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    println(\"State Space Completed\")\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    policy = piPolicyExactCont(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Completed\")\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = policy[s]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, policy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c00dff36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enumerateFeasibleActions (generic function with 1 method)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function enumerateFeasibleActions(s,N)\n",
    "    actionSpace = []\n",
    "    if s == fill(3, N)\n",
    "        for i in 1:N\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            push!(actionSpace, a)\n",
    "        end\n",
    "        return actionSpace\n",
    "    end\n",
    "\n",
    "    push!(actionSpace, zeros(Int64,N))\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            push!(actionSpace, a)\n",
    "        end\n",
    "    end\n",
    "    return actionSpace\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884ee22f",
   "metadata": {},
   "source": [
    "# SMART Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc90c99f",
   "metadata": {},
   "source": [
    "Tabular SMART algorithm, associated gEvaluation function, and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "233657b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactContQ (generic function with 1 method)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piPolicyExactContQ(q, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactContQ(s, q, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9b86ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actionFromQTab (generic function with 1 method)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function actionFromQTab(s, N, q)\n",
    "    #deal with all-damaged case\n",
    "    if s == fill(3,N)\n",
    "        return actionFromQTabAllDamaged(N, q)\n",
    "    end\n",
    "\n",
    "    feasibleActions = enumerateFeasibleActions(s,N)\n",
    "\n",
    "    #formulate action\n",
    "    optA = zeros(Int64, N)\n",
    "    optQ = q[s,optA]\n",
    "    for a in feasibleActions\n",
    "        testQ = q[s,a]\n",
    "        if testQ < optQ\n",
    "            optQ = testQ\n",
    "            optA = a\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optQ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b7fa239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actionFromQTabAllDamaged (generic function with 1 method)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function actionFromQTabAllDamaged(N, q)\n",
    "    s = fill(3, N)\n",
    "    feasibleActions = enumerateFeasibleActions(s,N)\n",
    "\n",
    "    optA = zeros(Int64, N)\n",
    "    optA[1] = 1\n",
    "    optQ = q[s, optA]\n",
    "    for a in feasibleActions\n",
    "        testQ = q[s,a]\n",
    "        if testQ < optQ\n",
    "            optQ = testQ\n",
    "            optA = a\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optQ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af60e3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactContQ (generic function with 1 method)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piActionExactContQ(s, q, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    feasibleActions = enumerateFeasibleActions(s,N)\n",
    "    optA = feasibleActions[1]\n",
    "    optQ = q[s, optA]\n",
    "    for a in feasibleActions\n",
    "        testQ = q[s,a]\n",
    "        if testQ < optQ\n",
    "            optQ = testQ\n",
    "            optA = a\n",
    "        end\n",
    "    end \n",
    "\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1f7db3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartTab (generic function with 1 method)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smartTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    optAandQ = actionFromQTab(s, N, q)\n",
    "    optA = optAandQ[1]\n",
    "    optQ = optAandQ[2]\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        optFlag = true\n",
    "        \n",
    "        #choose e-greedy action\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            optQ = q[s,optA]\n",
    "            optFlag = false\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g if optimal action taken\n",
    "            if optFlag\n",
    "                runningTotal += c\n",
    "                timePassed += time\n",
    "                g = runningTotal/timePassed\n",
    "            end\n",
    "\n",
    "            #find next optimal action and q value\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0943848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationTabQ (generic function with 1 method)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationTabQ(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, q, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    println(\"State Space Completed\")\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    policy = piPolicyExactContQ(q, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Completed\")\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = policy[s]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, policy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "433d3d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartOnPolicyTab (generic function with 1 method)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smartOnPolicyTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #choose only action for s = s0\n",
    "    optA = zeros(Int64, N)\n",
    "    optQ = q[s,optA]\n",
    "    optFlag = true\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "\n",
    "            #find next e-greedy action and q value\n",
    "            #find optimal action and q-value\n",
    "            optFlag = true\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            #choose epsilon\n",
    "            epsilon = c/(c + n)\n",
    "            \n",
    "            if stateDepEpsilon\n",
    "                epsilon = c/(c + numVisits[sPrime])\n",
    "            end\n",
    "\n",
    "            #select random action with probability epsilon\n",
    "            if rand(Uniform(0,1)) < epsilon\n",
    "                nextOptA = randomAction(sPrime, N)\n",
    "                nextOptQ = q[sPrime,nextOptA]\n",
    "                optFlag = false\n",
    "            end\n",
    "            \n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            optFlag = true\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            #choose epsilon\n",
    "            epsilon = c/(c + n)\n",
    "            \n",
    "            if stateDepEpsilon\n",
    "                epsilon = c/(c + numVisits[sPrime])\n",
    "            end\n",
    "\n",
    "            #select random action with probability epsilon\n",
    "            if rand(Uniform(0,1)) < epsilon\n",
    "                nextOptA = randomAction(sPrime, N)\n",
    "                nextOptQ = q[sPrime,nextOptA]\n",
    "                optFlag = false\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d8fa137b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartTabMA (generic function with 1 method)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SMART with an MA approximation for g\n",
    "function smartTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; window = 1000000, printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    totalCosts = [0.0]\n",
    "    lenTotalCosts = 1\n",
    "    totalTimes = [0.0]\n",
    "    lenTotal = 1\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    optAandQ = actionFromQTab(s, N, q)\n",
    "    optA = optAandQ[1]\n",
    "    optQ = optAandQ[2]\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        optFlag = true\n",
    "        \n",
    "        #choose e-greedy action\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            optQ = q[s,optA]\n",
    "            optFlag = false\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g if optimal action taken\n",
    "            if optFlag\n",
    "                runningTotal += c\n",
    "                timePassed += time\n",
    "                push!(totalCosts, runningTotal)               \n",
    "                push!(totalTimes, timePassed)\n",
    "                lenTotal += 1\n",
    "                if lenTotal <= window\n",
    "                    g = runningTotal/timePassed\n",
    "                else\n",
    "                    g = (runningTotal - totalCosts[lenTotal - window])/(timePassed - totalTimes[lenTotal - window])\n",
    "                end\n",
    "            end\n",
    "\n",
    "            #find next optimal action and q value\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "199fc408",
   "metadata": {},
   "source": [
    "# New Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b30394b",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "198bda8f",
   "metadata": {},
   "source": [
    "## SMART-MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a4578bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 4\n",
    "function tau(x)\n",
    "    return x\n",
    "end\n",
    "\n",
    "alpha_d = [0.000001*i for i in 1:N]\n",
    "alpha_r = [0.000001*i for i in 1:N] \n",
    "beta=10.0\n",
    "c0=[1.0*i for i in 1:N] \n",
    "c1=100.0\n",
    "r=[100.0*i for i in 1:N]\n",
    "nMax = 20000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e89d5e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "10.003393960244903\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(12345)\n",
    "resultSmartMA = smartTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, 1.0, 1.0; window = 2500000, printProgress = true, modCounter = 1000000, stateDepEpsilon = true)\n",
    "println(resultSmartMA[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e169deb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x0000000067F08310>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = resultSmartMA[3]\n",
    "gLen = length(gs)\n",
    "PyPlot.plot(gs[1:gLen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1d511fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space Completed\n",
      "Policy Completed\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.00410269210177"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalNMax = 2000000\n",
    "gs = gEvaluationTabQ(N,alpha_d, alpha_r, beta, tau, c0, c1, r, evalNMax, resultSmartMA[1], resultSmartMA[2]; printProgress = true, modCounter = 100000)\n",
    "gs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da92163e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x00000000680BC1C0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPlot.plot(gs[2][200000:evalNMax])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bd6c84b",
   "metadata": {},
   "source": [
    "## SMARVI-MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "73b1758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "10.374760631450307\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(12345)\n",
    "resultSmarviMA = smarviTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, 20000000, 1.0, 100.0; window = 100000, printProgress = true, modCounter = 1000000, stateDepEpsilon = true)\n",
    "println(resultSmarviMA[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9d369330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x000000008E718CD0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = resultSmarviMA[3]\n",
    "gLen = length(gs)\n",
    "ylim(0,16)\n",
    "PyPlot.plot(gs[1:gLen])\n",
    "PyPlot.plot(fill(10,20000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "394dffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space Completed\n",
      "Policy Completed\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.00412513687461"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalNMax = 2000000\n",
    "gs = gEvaluationTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, evalNMax, resultSmarviMA[1], resultSmarviMA[2]; printProgress = true, modCounter = 100000)\n",
    "gs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "94ef749d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x000000008E7186D0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPlot.plot(gs[2][1:evalNMax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc666e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
