{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c967fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using Random\n",
    "using Plots\n",
    "using PyPlot\n",
    "using StatsBase\n",
    "using StatsPlots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b1ef523",
   "metadata": {},
   "source": [
    "# Problem-Suite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebbc7791",
   "metadata": {},
   "source": [
    "Problem-Suite is a large structured notebook containing all of the functions created so far for this project.\n",
    "\n",
    "Sections:\n",
    "\n",
    "-[Miscellaneous Functions](#Miscellaneous-Functions)\n",
    "\n",
    "-[Pre-requisite functions for uniformised AVI](#Pre-requisite-functions-for-uniformised-AVI)\n",
    "\n",
    "-[Uniformised AVI functions](#Uniformised-AVI-functions)\n",
    "\n",
    "-[Pre-requisite functions for SMARVI](#Pre-requisite-functions-for-SMARVI)\n",
    "\n",
    "-[SMARVI Functions](#SMARVI-Functions)\n",
    "\n",
    "-[Pre-requisite Functions for Exact DP on Homogeneous Problems](#Pre-requisite-Functions-for-Exact-DP-on-Homogeneous-Problems)\n",
    "\n",
    "-[Exact DP on Homogeneous Problems (RVIA and PE/PI)](#Exact-DP-for-Homogeneous-problem)\n",
    "\n",
    "-[Pre-requisite Functions for Exact DP on Inhomogeneous Problems](#Pre-requisite-Functions-for-Exact-DP-on-Inhomogeneous-Problems)\n",
    "\n",
    "-[Exact DP on Inhomogeneous Problems (RVIA and PE/PI)](#Exact-DP-for-Inhomogeneous-Problem-(using-exact-h-or-VFA))\n",
    "\n",
    "-[Evaluation via simulation](#Evaluation-via-simulation)\n",
    "\n",
    "-[APE on Fully Active Policy](#APE-on-Fully-Active-Policy)\n",
    "\n",
    "-[SMARPE](#SMARPE)\n",
    "\n",
    "-[Tabular SMARVI and gEval](#tabular-smarvi-and-geval)\n",
    "\n",
    "-[SMART Functions](#SMART-Functions)\n",
    "\n",
    "-[New Functions](#new-functions)\n",
    "\n",
    "-[Tests](#Tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12fe12",
   "metadata": {},
   "source": [
    "# Miscellaneous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2556d9f",
   "metadata": {},
   "source": [
    "-Functions for enumerating state and action spaces\n",
    "\n",
    "-Functions for calculating flows given a state or state-action pair\n",
    "\n",
    "-Function for evaluating a VFA at a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a0da98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arrayToString (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#produce an array of array representations of all possible states\n",
    "function enumerateStates(N::Int64)\n",
    "    if N==1\n",
    "        return [[1],[2],[3]]\n",
    "    end\n",
    "    \n",
    "    output = []\n",
    "    lower = enumerateStates(N-1)\n",
    "    for s in lower\n",
    "        new1 = append!([1],s)\n",
    "        new2 = append!([2],s)\n",
    "        new3 = append!([3],s)\n",
    "        append!(output,[new1])\n",
    "        append!(output,[new2])\n",
    "        append!(output,[new3])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "#produce an array of array representations of all possible actions\n",
    "function enumerateActions(N::Int64)\n",
    "    if N==1\n",
    "        return [[0],[1]]\n",
    "    end\n",
    "    \n",
    "    output = []\n",
    "    lower = enumerateActions(N-1)\n",
    "    for a in lower\n",
    "        new1 = append!([0],a)\n",
    "        new2 = append!([1],a)\n",
    "        append!(output,[new1])\n",
    "        append!(output,[new2])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end    \n",
    "\n",
    "#produce array of array representations of all restricted, or single-repair, actions\n",
    "function enumerateRestrictedActions(N::Int64)\n",
    "    if N==1\n",
    "        return [[0],[1]]\n",
    "    end\n",
    "    \n",
    "    output = [zeros(Int64,N)]\n",
    "    for i in 1:N\n",
    "        temp = zeros(N)\n",
    "        temp[i] = 1\n",
    "        append!(output,[temp])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "#convert all array elements to string, then concatanate all elements (DEPRECATED AS DICTS CAN TAKE ARRAYS AS KEYS)\n",
    "function arrayToString(x)\n",
    "    return join(string.(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ba543e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateFlows (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function for calculating the flows given a state\n",
    "function calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    #update flows\n",
    "    flows = zeros(N)\n",
    "    healthy = sum(i == 1 for i in s)\n",
    "    \n",
    "    #if no links are healthy, return \n",
    "    if healthy == 0\n",
    "        return flows, c1\n",
    "    end\n",
    "    \n",
    "    #otherwise, find best route, and return\n",
    "    bestCost = maximum(c0) + 1\n",
    "    usedLink = 0\n",
    "    for k in 1:N\n",
    "        if s[k] == 1 && c0[k] < bestCost\n",
    "            bestCost = c0[k]\n",
    "            usedLink = k\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[usedLink] = beta\n",
    "    \n",
    "    return flows, bestCost\n",
    "end\n",
    "\n",
    "#function for calculating the flows given a state-action pair\n",
    "function calculateFlows(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    sPrime = s - a\n",
    "    return calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127d3f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate a VFA at a given state\n",
    "function v(s::Vector{Int64}, params::Vector{Float64}, features::Vector{Function})\n",
    "    numFeatures = length(features)\n",
    "    return params[1] + sum(params[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960bf4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version of v that takes flows for the features\n",
    "function v(s::Vector{Int64}, flows::Vector{Float64}, params::Vector{Float64}, features::Vector{Function})\n",
    "    N = length(params)\n",
    "    return params[1] + sum(params[i]*features[i-1](s, flows) for i in 2:N)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05d0fc",
   "metadata": {},
   "source": [
    "# Pre-requisite functions for uniformised AVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c56072f",
   "metadata": {},
   "source": [
    "This section contains functions used within the AVI algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "476755cd",
   "metadata": {},
   "source": [
    "Given a state-action pair, return the next random pre-decision state, the instant cost, and the updated flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d157b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateStateAndFlowsUnif (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateStateAndFlowsUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #observe exogenous information\n",
    "    w = rand(Uniform(0, 1))\n",
    "    \n",
    "    #interpret exog info: is it a demand deg, rare deg, or completed repair \n",
    "    found = false\n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        if runningTotal <= w <= runningTotal + flows[k]*alpha_d[k]*del\n",
    "            found = true\n",
    "            sPrime[k] = 3\n",
    "            #println(\"Demand Deg at \"*string.(k))\n",
    "            break\n",
    "        end\n",
    "        runningTotal = runningTotal + flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    if found == false\n",
    "        for k in 1:N\n",
    "            if runningTotal <= w <= runningTotal + alpha_r[k]*del\n",
    "                found = true\n",
    "                sPrime[k] = 3\n",
    "                #println(\"Rare Deg at \"*string.(k))\n",
    "                break\n",
    "            end\n",
    "            runningTotal = runningTotal + alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if found == false && repair > 0\n",
    "        if runningTotal <= w <= runningTotal + tau(repair)*del\n",
    "            found = true\n",
    "            #find all repairing links\n",
    "            repairing = []\n",
    "            for k in 1:N\n",
    "                if sPrime[k] == 2\n",
    "                    append!(repairing,[k])\n",
    "                end\n",
    "            end\n",
    "            repaired = sample(repairing)\n",
    "            sPrime[repaired] = 1\n",
    "            #println(\"Repair completed at \"*string.(repaired))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if found == false\n",
    "        #println(\"No Event\")\n",
    "    end\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    \n",
    "    return sPrime, (beta*bestCost + sum(r[k]*(sPrime[k]==2) for k in 1:N))*del, newFlows\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "836f070e",
   "metadata": {},
   "source": [
    "Given a state action pair, return the instant cost over the delta timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "685eb320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostUnif (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instant cost over the timestep\n",
    "function instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(sPrime[i] == 1 for i in 1:N)\n",
    "    repair = sum(sPrime[i] == 2 for i in 1:N)\n",
    "    damaged = sum(sPrime[i] == 3 for i in 1:N)\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    \n",
    "    return (beta*bestCost + sum(r[k]*(sPrime[k]==2) for k in 1:N))*del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9603d5cd",
   "metadata": {},
   "source": [
    "Given a state-action pair and a VFA, calculate the expected value of the value function after one timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f0a307f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueUnif (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h. Also used in Exact PE/PI when using a VFA\n",
    "#One version takes flows as an argument, the other calculates the flows\n",
    "function expectedNextValueUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*v(sPrime, vParams, features)\n",
    "end  \n",
    "\n",
    "function expectedNextValueUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, vParams, features)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    flows = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*v(sPrime, vParams, features)\n",
    "end  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3920172",
   "metadata": {},
   "source": [
    "# Uniformised AVI functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80cbe1e5",
   "metadata": {},
   "source": [
    "Algorithms that perform RAVI on the uniformised version of the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5310d49c",
   "metadata": {},
   "source": [
    "Given some parallel link problem and VFA architecture, perform RAVI, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f83714d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in uniformised setting, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state\n",
    "function aviApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - g\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + v(sPrime, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "718d21d7",
   "metadata": {},
   "source": [
    "Given some parallel link problem and VFA architecture, perform RAVI, using a full expectation for update targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb36c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviFull (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function aviFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - g\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d76fa41",
   "metadata": {},
   "source": [
    "Similar to above, but only uses the Binary Action Space (BAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ebdac30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviUnifBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI with BAS in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function aviUnifBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        vs0 = v(s0, vParams, features)\n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - vs0\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        testV = instantCostUnif(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - vs0\n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA = testA\n",
    "            optV = testV\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f587fd5b",
   "metadata": {},
   "source": [
    "# Pre-requisite functions for SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2008a2c4",
   "metadata": {},
   "source": [
    "Helper functions for the SMARVI algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1f36cda",
   "metadata": {},
   "source": [
    "Given a state-action pair and pre-calculated flows, return the expected sojourn time for the state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1506350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sojournTime (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the expected sojourn time of a state-action pair\n",
    "function sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    s = s - a\n",
    "    if s == fill(3,N)\n",
    "        return 1/(beta*sum(alpha_d) + sum(alpha_r) + tau(N))\n",
    "    end\n",
    "    \n",
    "    numRep = sum(i == 2 for i in s)\n",
    "    cumulativeRate = 0.0\n",
    "    for i in 1:N\n",
    "        if s[i] == 1\n",
    "            cumulativeRate += flows[i]*alpha_d[i] + alpha_r[i]\n",
    "        elseif s[i] == 2\n",
    "            cumulativeRate += alpha_r[i] + tau(numRep)/numRep\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return 1/cumulativeRate\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb2c6d2b",
   "metadata": {},
   "source": [
    "Given a state-action pair and flows, calculate the expected cost accumulated until a transition occurs, or calculate the simulated cost accumulated over a simulated time del."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c5307b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostCont (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the expected cost accumulated until a transition \n",
    "function instantCostCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows; del = 0)\n",
    "    if del == 0\n",
    "        del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    end\n",
    "    \n",
    "    return instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ecc13f2",
   "metadata": {},
   "source": [
    "Given a state-action pair, return the next random pre-decision state, the cost accumulated over the sojourn time, and the updated flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75587424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateStateAndFlowsCont (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given a state-action pair, return the next random pre-decision state, the cost accumulated over the sojourn time, and the updated flows\n",
    "function updateStateAndFlowsCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    actualTime = rand(Exponential(del))\n",
    "    result = updateStateAndFlowsUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "    return result[1], instantCostCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows; del = actualTime), result[3], actualTime\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae4efc43",
   "metadata": {},
   "source": [
    "Given a state-action pair, precalculated flows, and a VFA, return the expected value of the VFA after a transition has occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60cda4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueCont (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h\n",
    "function expectedNextValueCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime,vParams,features)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc5b77a7",
   "metadata": {},
   "source": [
    "Similar to above, but assumes the features of the VFA take precalcuated flows as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "488f16a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContFlows (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h\n",
    "#Assumes the VFA is constructed with features taking arguments (s, flows), so flows are precalculated\n",
    "function expectedNextValueContFlows(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime, flows, vParams,features)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, flowsNext, vParams, features)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, flowsNext, vParams, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, flowsNext, vParams, features)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a2e614d",
   "metadata": {},
   "source": [
    "Given a state, flows, and a VFA-g pair, return the optimal action and associated V value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b01347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "    #formulate optimal action and calculate optV\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "    \n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            testV = v(s-a, vParams, features)\n",
    "            if testV <= optV\n",
    "                optV = testV\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #If wanted, force a repair if optA is passive for [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA = zeros(Int64,N)\n",
    "        optA[1] = 1\n",
    "        optV = v(s-optA, vParams, features)\n",
    "        \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                testV = v(s-a, vParams, features)\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f824e",
   "metadata": {},
   "source": [
    "# SMARVI Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dbccab0",
   "metadata": {},
   "source": [
    "Variety of functions which perform the SMARVI algorithm, with different additional features.\n",
    "For clarity, the \"state-trace\" is a method which collects a sequence of states connected by instantaneous actions together, and ensures they all have the same update target. For example, if in state s we take action a!=0, and in the resulting state s+a we take action 0, both s and s+a will have update target (c + E(V(s')) - gt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39c05112",
   "metadata": {},
   "source": [
    "Given a problem and a VFA architecture, perform SMARVI, with no e-greedy action selection or state trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea289cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs SMARVI\n",
    "function smarvi(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action and calculate optV\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = optV - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5c6e48e",
   "metadata": {},
   "source": [
    "Perform SMARVI with a state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2005234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviST (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in continuous time setting, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "function smarviST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    \n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    actionFlag = false\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update stateTrace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate optimal action and calculate optV\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #if optimal action is passive, update VFA for all states in the stateTrace, and simulate the next state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            \n",
    "            #find simulated next state\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "        \n",
    "            bestV = optV - v(s0, vParams, features)\n",
    "            \n",
    "            #update VFA\n",
    "            traceLength = length(stateTrace)\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            #reset stateTrace\n",
    "            stateTrace = []\n",
    "            \n",
    "            #update flows and average\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #if some action is optimal, simply update the state\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58458323",
   "metadata": {},
   "source": [
    "Performs SMARVI with a given fixed value of g0 for action selection. This prevents bad initial estimates of g from severely impacting the algorithm, but restricts the policy space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1840b7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions, and controlling action selection using some fixed g0\n",
    "function smarvi_g0(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g0)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV + g0*t - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25861988",
   "metadata": {},
   "source": [
    "Similar to regular SMARVI, but only using the BAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "318f0267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI with BAS in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "function smarviBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        tPassive = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tPassive\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        tActive = sojournTime(s, testA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        testV = instantCostCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tActive\n",
    "        \n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        #Ignore passive action for broken network\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d439d00",
   "metadata": {},
   "source": [
    "Similar to regular SMARVI, but where flows are assumes to be passed to the VFA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0daaef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviFlows (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version of smarvi where flows are passed to features\n",
    "function smarviFlows(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    flows0 = copy(flows)\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContFlows(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                \n",
    "                if vParams[1] + sum(vParams[i+1]*features[i](s-a, flows) for i in 1:numFeatures) <= optV\n",
    "                    optV = vParams[1] + sum(vParams[i+1]*features[i](s-a, flows) for i in 1:numFeatures)\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, flows, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    testV = v(s-a, flows, vParams, features)\n",
    "                    if testV <= optV\n",
    "                        optV = testV\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - v(s0, flows0, vParams,features)\n",
    "        else\n",
    "            bestV = optV - v(s0, flows0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s, flows) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s, flows) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8365b78",
   "metadata": {},
   "source": [
    "## e-greedy SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f3cba33",
   "metadata": {},
   "source": [
    "Helper functions and main algorithm for e-greedy SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fd55c88",
   "metadata": {},
   "source": [
    "Samples a random feasible action for a state s of length N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2fd2e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randomActionAllDamaged (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate random action\n",
    "function randomAction(s,N)\n",
    "    #deal with all-damaged case\n",
    "    if s == fill(3,N)\n",
    "        return randomActionAllDamaged(N)\n",
    "    end\n",
    "\n",
    "    damaged = [0]\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            append!(damaged, [i])\n",
    "        end\n",
    "    end\n",
    "            \n",
    "    choice = sample(damaged)\n",
    "    optA = zeros(Int64, N)\n",
    "    if choice == 0\n",
    "        return optA\n",
    "    else\n",
    "        optA[choice] = 1\n",
    "        return optA\n",
    "    end\n",
    "end\n",
    "\n",
    "#calculate random action for [3,3...,3] state\n",
    "function randomActionAllDamaged(N)\n",
    "    choice = sample(1:N)\n",
    "    a = zeros(Int64, N)\n",
    "    a[choice] = 1\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48f72466",
   "metadata": {},
   "source": [
    "Performs an e-greedy version of SMARVI, with e_n = b/b+n for some given b. Allows SMARVI to perform some exploratory actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a38d016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi_epsGreedy (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#actions are choosen via e-greedy action selection, where a random action is chosen with probability b/b+n\n",
    "function smarvi_epsGreedy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; b = 1.0, printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate e-greedy action\n",
    "        if rand(Uniform(0,1)) <= b/(b + n) \n",
    "            optA = randomAction(s,N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "            else\n",
    "                optV = v(s-optA, vParams, features)\n",
    "            end\n",
    "        else                \n",
    "            optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "            optA = optAandV[1]\n",
    "            optV = optAandV[2]\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eaa235",
   "metadata": {},
   "source": [
    "# Pre-requisite Functions for Exact DP on Homogeneous Problems "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7b7c104",
   "metadata": {},
   "source": [
    "Helper functions for Homogeneous Exact DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1801225",
   "metadata": {},
   "source": [
    "Calculates instant cost for homogeneous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22678b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instant cost function strictly for homogeneous problem\n",
    "function instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    i1Prime = i1 - a\n",
    "    i2Prime = i2 + a\n",
    "    \n",
    "    #if no links are healthy, return \n",
    "    if N - i1 - i2 == 0\n",
    "        return (beta*c1 + r*i2Prime)*del\n",
    "    end\n",
    "    \n",
    "    \n",
    "    return (beta*c0 + r*i2Prime)*del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94e504b7",
   "metadata": {},
   "source": [
    "Given state (i_1, i_2) and action a, calculates the expected next value function after one timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a17eacf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates E(h(s')) from s,a strictly for a homogeneous problem\n",
    "function expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    i1Prime = i1 - a\n",
    "    i2Prime = i2 + a\n",
    "    thisH = h[i1+1,i2+1]\n",
    "    \n",
    "    #if all are damaged\n",
    "    if i1Prime == N\n",
    "        return thisH\n",
    "    end\n",
    "    \n",
    "    #if none are healthy\n",
    "    if N - i1 - i2 == 0\n",
    "        return thisH + tau(i2Prime)*del*(h[i1Prime+1,i2Prime-1+1] - thisH) + i2Prime*del*alpha_r*(h[i1Prime+1+1, i2Prime-1+1] - thisH)\n",
    "    end\n",
    "    \n",
    "    #if none are repairing\n",
    "    if i2Prime == 0\n",
    "        return thisH + (beta*alpha_d + (N - i1 - i2)*alpha_r)*del*(h[i1Prime+1+1,i2Prime+1] - thisH)\n",
    "    end\n",
    "    \n",
    "    return thisH + (beta*alpha_d + (N - i1 - i2)*alpha_r)*del*(h[i1Prime+1+1,i2Prime+1] - thisH) + i2Prime*alpha_r*del*(h[i1Prime+1+1,i2Prime-1+1] - thisH) + tau(i2Prime)*del*(h[i1Prime+1,i2Prime-1+1] - thisH)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68c7af12",
   "metadata": {},
   "source": [
    "Given state (i_1,i_2) and value function h, calculates and returns the best action for the state using full expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "beeb2b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the exact PI action strictly for a homogeneous problem\n",
    "function piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "    if i1 == 0\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    if i1 == N && forceActive\n",
    "        optA = 1\n",
    "        optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        for a in 2:i1\n",
    "            testH = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            if testH <= optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "        return optA\n",
    "    end\n",
    "    \n",
    "    optA = 0\n",
    "    optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    for a in 1:i1\n",
    "        testH = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        if testH <= optH\n",
    "            optA = a\n",
    "            optH = testH\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71928099",
   "metadata": {},
   "source": [
    "Similar to above, but uses the approximation Q(s,a) = h(s+a) for a!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b58a1177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the approx PI action based on instantaneous approximation, strictly for a homogeneous problem\n",
    "function piActionHomogApprox(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = false)\n",
    "    #deal with \"nothing damaged\" edge case\n",
    "    if i1 == 0\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    #deal with \"everything damaged\" edge case\n",
    "    if i1 == N && forceActive\n",
    "        optA = 1\n",
    "        optH = h[i1-optA+1,i2+optA+1]\n",
    "        for a in 2:i1\n",
    "            testH = h[i1-a+1,i2+a+1]\n",
    "            if testH <= optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "        return optA\n",
    "    end\n",
    "    \n",
    "    optA = 0\n",
    "    optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) - g*del\n",
    "    for a in 1:i1\n",
    "        testH = h[i1-a+1,i2+a+1]\n",
    "        if testH <= optH\n",
    "            optA = a\n",
    "            optH = testH\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8ab05cf",
   "metadata": {},
   "source": [
    "Given h, constructs optimal policy using exact PI method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6335a065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the exact PI policy strictly for a homogeneous problem\n",
    "function piPolicyHomog(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "    policy = zeros(Int64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            policy[i1+1,i2+1] = piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34525984",
   "metadata": {},
   "source": [
    "Given h, constructs optimal policy using Q(s,a) = h(s,a) for a!=0 approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "404bfec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the approx PI policy based on instantaneous approximation, strictly for a homogeneous problem\n",
    "function piPolicyHomogApprox(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = false)\n",
    "    policy = zeros(Int64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            policy[i1+1,i2+1] = piActionHomogApprox(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = forceActive)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905b77bf",
   "metadata": {},
   "source": [
    "Constructs a h table from a VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42156f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hFromVFAHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hFromVFAHomog(N, params, features)\n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return hIn\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c11abe",
   "metadata": {},
   "source": [
    "# Exact DP for Homogeneous problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fc32308",
   "metadata": {},
   "source": [
    "Actual DP algorithms "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99f7e6cf",
   "metadata": {},
   "source": [
    "Given a h table, performs PE on PI policy derived from h, and returns g, h, n (number of iterations), and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc148338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given exact h function, strictly for a homogeneous problem \n",
    "function rpiHomog(N::Int64, hIn, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    policy = piPolicyHomog(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = policy[i1+1,i2+1]\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b8046e6",
   "metadata": {},
   "source": [
    "Performs PE on the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e39012f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpeFAHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates the fully active policy, strictly for a homogeneous problem \n",
    "function rpeFAHomog(N::Int64, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = i1\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c051660",
   "metadata": {},
   "source": [
    "Similar to rpiHomog, but uses Q(s,a) = h(s,a) approximation for PI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "964c01ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates an approximate PI policy based on a given exact h function and instananeous actions, strictly for a homogeneous problem \n",
    "function rpiHomogApprox(N::Int64, hIn, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64, g::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    policy = piPolicyHomogApprox(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = forceActive)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = policy[i1+1,i2+1]\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e1c1de",
   "metadata": {},
   "source": [
    "Similar to above, but uses VFA as input h, and uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7146cc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogVFAApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given VFA, using instananeous approximation, strictly for a homogeneous problem \n",
    "function rpiHomogVFAApprox(N::Int64, params, features, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64, g::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    \n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #run standard function\n",
    "    return rpiHomogApprox(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = nMax, delScale = delScale, forceActive = forceActive)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54d455db",
   "metadata": {},
   "source": [
    "Similar to above, WITHOUT Q(s,a) = h(s+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04dc9704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given VFA, strictly for a homogeneous problem \n",
    "function rpiHomogVFA(N::Int64, params, features, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    \n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #run standard function\n",
    "    return rpiHomog(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, forceActive = forceActive)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "690683ed",
   "metadata": {},
   "source": [
    "Performs RVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "939c3f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rviHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs RVIA \n",
    "function rviHomog(N::Int64, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b3849",
   "metadata": {},
   "source": [
    "# Pre-requisite Functions for Exact DP on Inhomogeneous Problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1e53ece",
   "metadata": {},
   "source": [
    "Helper functions for inhomogeneous exact DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14ea586f",
   "metadata": {},
   "source": [
    "Given a state-action pair and h, calculates the expected next value of the value function after one timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d45adbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueExact (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates E(h(s')) from s,a using exact h table\n",
    "function expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    \n",
    "    flows = zeros(Float64, N)\n",
    "    if healthy > 0\n",
    "        #otherwise, find best route, and return\n",
    "        bestCost = maximum(c0) + 1\n",
    "        usedLink = 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 1 && c0[k] < bestCost\n",
    "                bestCost = c0[k]\n",
    "                usedLink = k\n",
    "            end \n",
    "        end\n",
    "        \n",
    "        flows[usedLink] = beta\n",
    "    end\n",
    "    \n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*h[sNext]\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*h[sNext]\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*h[sNext]\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*h[sPrime]\n",
    "end "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f2dad95",
   "metadata": {},
   "source": [
    "Given a state and a h table, calculates the PI action for s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b71c013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExact (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table\n",
    "function piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2cb0d43",
   "metadata": {},
   "source": [
    "Similar to above, but uses the approximation Q(s,a) = h(s+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17b1bbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table, based off instananeous actions\n",
    "function piActionExactInstant(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h) - g*del\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = h[s-a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7eb2f91a",
   "metadata": {},
   "source": [
    "Similar to above, but uses a VFA instead of a h table, WITHOUT Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bab1aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using a VFA\n",
    "function piActionVFA(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4096d38",
   "metadata": {},
   "source": [
    "Similar to above, WITH Q(s,a) = h(s,a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "604bf237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using a VFA and instananeous actions\n",
    "function piActionVFAInstant(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features) - g*del\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = v(s-a,params,features)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884618d7",
   "metadata": {},
   "source": [
    "Constructs PI policy using h table and no approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1dfe8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExact (generic function with 1 method)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy using exact h table\n",
    "function piPolicyExact(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8842b6de",
   "metadata": {},
   "source": [
    "Constructs PI policy using h table and Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfb9dcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy using exact h table, interpretting h with instant actions\n",
    "function piPolicyExactInstant(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactInstant(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d2fa28",
   "metadata": {},
   "source": [
    "Constructs PI policy using VFA and no approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72a89a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy from a VFA\n",
    "function piPolicyVFA(params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionVFA(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d72cd38",
   "metadata": {},
   "source": [
    "Constructs PI policy using VFA and Q(s,a) = h(s+a) approximation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14bba9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy from a VFA, using instant actions to interpret h\n",
    "function piPolicyVFAInstant(params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionVFAInstant(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f8cc379",
   "metadata": {},
   "source": [
    "Constructs h table from VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28d6c8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hFromVFAInhomog (generic function with 1 method)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hFromVFAInhomog(N, params, features)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = v(s, params. features)\n",
    "    end\n",
    "\n",
    "    return h\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7172ee4",
   "metadata": {},
   "source": [
    "# Exact DP for Inhomogeneous Problem (using exact h or VFA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43e2ee1b",
   "metadata": {},
   "source": [
    "DP algorithms for inhomogeneous problem\n",
    "\n",
    "Note that throughout when we talk of a Q(s,a) = h(s+a) approximation, this only refers to action selection and not update rules, and excludes a=0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2225adc2",
   "metadata": {},
   "source": [
    "Given an explicit policy table, performs PE, returns g, h and n (# of iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b118e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpe (generic function with 1 method)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs PE using exact policy table\n",
    "function rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    h = Dict()\n",
    "    w = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        w[s] = 0.0\n",
    "    end\n",
    "    s0  = fill(1, N)\n",
    "    n = 0\n",
    "    \n",
    "    #do until max iterations met or epsilon convergence\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #find updates for every state\n",
    "        for s in stateSpace\n",
    "            a = policy[s]\n",
    "            w[s] = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "        end\n",
    "        \n",
    "        #calculate relative values and delta\n",
    "        delta = 0\n",
    "        for s in stateSpace\n",
    "            update = w[s] - w[s0]\n",
    "            if delta < update - h[s] || delta == 0\n",
    "                delta = update - h[s]\n",
    "            end\n",
    "            \n",
    "            h[s] = update\n",
    "        end\n",
    "        \n",
    "        #stopping condition\n",
    "        if delta < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if printProgress && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    a = zeros(Int64, N)\n",
    "    g = instantCostUnif(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h) - h[s0]\n",
    "    \n",
    "    return g/del, h, n\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1ad5a2b",
   "metadata": {},
   "source": [
    "Given a h table, constructs PI policy and performs PE, returning g, h, n and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c57636f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiExact (generic function with 1 method)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using exact h table\n",
    "function rpiExact(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    policy = piPolicyExact(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    output = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "    return output[1], output[2], output[3], policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec78eb5a",
   "metadata": {},
   "source": [
    "Similar to above, but uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "266027ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using exact h table, using instant actions to interpet h\n",
    "function rpiExactInstant(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    policy = piPolicyExactInstant(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    output = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "    return output[1], output[2], output[3], policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa6660e",
   "metadata": {},
   "source": [
    "Performs PE on the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f773f8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpeFA (generic function with 1 method)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs exact PE on the fully-active policy\n",
    "function rpeFA(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = faAction(s)\n",
    "    end\n",
    "    \n",
    "    return rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c325a927",
   "metadata": {},
   "source": [
    "Performs PE on fully passive policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d304fb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpePassive (generic function with 1 method)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs exact PE on the passive policy\n",
    "function rpePassive(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    return rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52c45280",
   "metadata": {},
   "source": [
    "Given a VFA, constructs PI policy and performs PE, returning g, h, n and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ec6cacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using VFA\n",
    "function rpiVFA(N, params, features, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    hIn = hFromVFAInhomog(N, params, features)\n",
    "    return rpiExact(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68209e10",
   "metadata": {},
   "source": [
    "Similar to above, but uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a19903b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using VFA and instantaneous actions to interpret h\n",
    "function rpiVFAInstant(N, params, features, g, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    hIn = hFromVFAInhomog(N, params, features)\n",
    "    return rpiExactInstant(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b0b6c10",
   "metadata": {},
   "source": [
    "Performs RVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d38fbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rvi (generic function with 1 method)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs RVIA\n",
    "function rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    h = Dict()\n",
    "    w = Dict()\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        w[s] = 0.0\n",
    "        policy[s] = zeros(Int64,N)\n",
    "    end\n",
    "    s0  = fill(1, N)\n",
    "    n = 0\n",
    "    \n",
    "    #do until max iterations met or epsilon convergence\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #find updates for every state\n",
    "        for s in stateSpace\n",
    "            a = piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            w[s] = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "        end\n",
    "        \n",
    "        #calculate relative values and delta\n",
    "        delta = 0\n",
    "        for s in stateSpace\n",
    "            update = w[s] - w[s0]\n",
    "            if delta < update - h[s] || delta == 0\n",
    "                delta = update - h[s]\n",
    "            end\n",
    "            \n",
    "            h[s] = update\n",
    "        end\n",
    "        \n",
    "        #stopping condition\n",
    "        if delta < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if printProgress && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    a = zeros(Int64, N)\n",
    "    g = instantCostUnif(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h) - h[s0]\n",
    "    \n",
    "    return g/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb424a1e",
   "metadata": {},
   "source": [
    "# Evaluation via simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bcbbb37",
   "metadata": {},
   "source": [
    "Various evaluation functions for approximating g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ef631d9",
   "metadata": {},
   "source": [
    "Takes a trained VFA and learns g, using g also for control, starting from state s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8261907a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation (generic function with 1 method)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA via PI using simulation\n",
    "function gEvaluation(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, printState = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if printState\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "320c63e1",
   "metadata": {},
   "source": [
    "Similar to above, but starting from a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9fd18438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFromS (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluationFromS(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    flowResult = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    flows = flowResult[1]\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end \n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ceb6a98",
   "metadata": {},
   "source": [
    "Similar to gEvaluation, but uses a fixed g0 for control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba5ce5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluation_g0(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3cd3eae",
   "metadata": {},
   "source": [
    "Similar to above, but starts from a given state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b85a37ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFromS_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluationFromS_g0(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    flowResult = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    flows = flowResult[1]\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end \n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6636431",
   "metadata": {},
   "source": [
    "Finds the g of the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33be91d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFA (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates the FA policy\n",
    "function gEvaluationFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate FA action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d651dbd0",
   "metadata": {},
   "source": [
    "Similar to gEvaluation, but only uses the BAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8ba1023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA via PI using simulation\n",
    "function gEvaluationBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        tPassive = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tPassive\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        tActive = sojournTime(s, testA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        testV = instantCostCont(s,testA, N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tActive\n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA = testA\n",
    "            optV = testV\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            append!(runningTotals, [runningTotal])\n",
    "            timePassed += time\n",
    "            append!(times,[timePassed])\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d81acf",
   "metadata": {},
   "source": [
    "Similar to gEvaluation_g0, but assumes that flows are passed to the VFA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "862863bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation_g0_flows (generic function with 1 method)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluation_g0_flows(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContFlows(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, flows, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, flows, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, flows, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88307ffe",
   "metadata": {},
   "source": [
    "# APE on Fully Active Policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7749e9f",
   "metadata": {},
   "source": [
    "Performs APE on the Fully Active Policy using each of the four approaches to estimating a VFA (mixes of uniform/smar and simulated-next-state/expectation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "666815fa",
   "metadata": {},
   "source": [
    "Returns the FA action for a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1319dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "faAction (generic function with 1 method)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computes the Fully Active action for a given state s\n",
    "function faAction(s)\n",
    "    N = length(s)\n",
    "    a = zeros(Int64,N)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a[i] = 1\n",
    "        end\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9168d0b",
   "metadata": {},
   "source": [
    "Evaluates the FA policy using a VFA and uniformisation, and update targets c + V(s') - gt, where s' is simulated next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e894ba39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAUnifApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE of FA policy in uniformised setting, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state\n",
    "function apeFAUnifApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + v(sPrime, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e96581e",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation for updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ccde3d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAUnifFull (generic function with 1 method)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE of FA policy in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function apeFAUnifFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + expectedNextValueUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d7bdaa9",
   "metadata": {},
   "source": [
    "Performs SMARPE on FA policy, with update target c + V(s') - gt where s' is the next simulated state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bd7df03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs SMARPE on FA policy, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "function smarpeFAApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + v(sPrime, vParams, features) - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af073bb3",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation in update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8865800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAFull (generic function with 1 method)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarpeFAFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + expectedNextValueCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ceece53",
   "metadata": {},
   "source": [
    "Similar to smarpeFAApprox, but incorporates state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b68eb6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAApproxST (generic function with 1 method)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE on FA policy in continuous time setting, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "#Also incorporates the state trace when actions are taken\n",
    "function smarpeFAApproxST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update state trace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #for passive action, do proper update\n",
    "        if bestA == zeros(Int64,N)\n",
    "            #find value of v^n\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + v(sPrime, vParams, features) - g*t - v(s0, vParams,features)\n",
    "\n",
    "            #update VFA\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            stateTrace = []\n",
    "            \n",
    "            #update g, state, and flows\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #for other action, simply update state and move on\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c19d280d",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation for update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47d8caaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAFullST (generic function with 1 method)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE on FA policy in continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Also incorporates the state trace when actions are taken\n",
    "function smarpeFAFullST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update state trace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #for passive action, do proper update\n",
    "        if bestA == zeros(Int64,N)\n",
    "            #find value of v^n\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + expectedNextValueCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t - v(s0, vParams,features)\n",
    "\n",
    "            #update VFA\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            stateTrace = []\n",
    "            \n",
    "            #update g, state, and flows\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #for other action, simply update state and move on\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f5b89",
   "metadata": {},
   "source": [
    "# SMARPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54992054",
   "metadata": {},
   "source": [
    "## Semi-Markov Approximate Relative Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be11b69",
   "metadata": {},
   "source": [
    "- SMARPE takes some trained VFA as input, and seeks to learn the associated long run cost g and the value function of the policy derived from the given VFA\n",
    "\n",
    "- Standard SMARPE uses the online training value of g for action selection, allowing the policy to vary throughout training\n",
    "\n",
    "- SMARPE_g0 takes a pre-learned value of g0 to be used for action selection, keeping the policy constant throughout. This value of g0 might be taken directly from SMARVI or from some gEval function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffc082",
   "metadata": {},
   "source": [
    "Worth also discussing is the exact behaviour of the gEval functions.\n",
    "\n",
    "- Standard gEval simply evaluates a VFA, and learns g throughout. In turn, this value of g is used for action selection, so the policy may vary throughout evaluation.\n",
    "\n",
    "- gEval_g0 evaluates a VFA-g0 pair, keeping the policy constant throughout. It may be good practice to always follow standard gEval with gEval_g0, due to the lack of policy variability.\n",
    "\n",
    "gEval functions are the part that actually calculate the PI actions based on the VFAs derived from SMARPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a0e5de3",
   "metadata": {},
   "source": [
    "Given a VFA-g pair, evaluates the PI policy derived from the pair via a new VFA with the same architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6082e341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpe (generic function with 1 method)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE in the continuous time setting, approximating E(h(s')) using all possible transitions, and with a fixed g0 for action selection\n",
    "function smarpe(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, paramsIn, paramsOut, features, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [paramsOut]\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, paramsIn, features, g0)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #recalculate optA in terms of new VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, paramsOut, features) - g*t\n",
    "        else\n",
    "            optV = v(s - bestA, paramsOut, features)\n",
    "        end \n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, paramsOut ,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, paramsOut, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        paramsOut = paramsOut + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[paramsOut])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs, [g])\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return paramsOut, paramHist, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d24567",
   "metadata": {},
   "source": [
    "# Tabular SMARVI and gEval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b101ab9",
   "metadata": {},
   "source": [
    "Tabular SMARVI algorithms (non e-greedy, e-greedy, and e-greedt with state trace), associated gEval function, and helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a829f15",
   "metadata": {},
   "source": [
    "Given a state s, its flows, and a h-g pair, return the optimal action and V value for s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d500f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromTable (generic function with 1 method)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "    #find optimal action\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        \n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "               \n",
    "            if h[s-a] <= optV\n",
    "                optV = h[s-a]\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    #Fix choose optimal non-passive action if state is [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optV = h[s-optA]\n",
    "            \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                testV = h[s-a]\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94fd262e",
   "metadata": {},
   "source": [
    "Given a state-action pair and a h table, compute the next expected h value given that a transition has occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd0e5b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContTab (generic function with 1 method)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and tabular h\n",
    "function expectedNextValueContTab(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return h[sPrime]\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*h[sNext]\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*h[sNext]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*h[sNext]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e27d5cd6",
   "metadata": {},
   "source": [
    "Tabular version of SMARVI, with no state trace or e-greedy actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6447836b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab (generic function with 1 method)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Uses tabular representation instead of VFA\n",
    "function smarviTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - h[s0]\n",
    "        else\n",
    "            bestV = optV - h[s0]\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = h[s]\n",
    "        h[s] += (b/(b + numVisits[s]))*(bestV - currentEst)\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2833561",
   "metadata": {},
   "source": [
    "e-greedy version of the above. e can be chosen to depend on the state or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cf860122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab_epsGreedy (generic function with 1 method)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Uses tabular representation instead of VFA and e-greedy action selection\n",
    "function smarviTab_epsGreedy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = h[s]\n",
    "        h[s] += (b/(b + numVisits[s]))*(bestV - currentEst)\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f7777e",
   "metadata": {},
   "source": [
    "Similar to above, but incorporates the state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de3e948e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab_epsGreedyST (generic function with 1 method)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses tabular representation instead of VFA, e-greedy action selection, and state trace \n",
    "function smarviTab_epsGreedyST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        #choose actual action\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5619d4e1",
   "metadata": {},
   "source": [
    "Similar to above (so e-greedy and state trace), but uses a moving average window to approximate g, allowing old estimates to be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e0f86bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTabMA (generic function with 1 method)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tabular SMARVI with moving average online approximation for g, e-greedy action selection, and state trace\n",
    "function smarviTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; window = 2500000, printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    totalCosts = [0.0]\n",
    "    timePassed = 0.0\n",
    "    totalTimes = [0.0]\n",
    "    lenTotals = 1\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        #choose actual action\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            push!(totalCosts, runningTotal)\n",
    "            push!(totalTimes, timePassed)\n",
    "            lenTotals += 1\n",
    "            if lenTotals <= window\n",
    "                g = runningTotal/timePassed\n",
    "            else\n",
    "                g = (runningTotal - totalCosts[lenTotals - window])/(timePassed - totalTimes[lenTotals - window])\n",
    "            end\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0ca583e",
   "metadata": {},
   "source": [
    "Given a state, a h table and g, return the PI action for s. Uses the Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a89e1d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactCont (generic function with 1 method)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table, based off continuous model\n",
    "function piActionExactCont(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optH = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows,h) - g*t\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = h[s-a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optH = h[s - optA]\n",
    "\n",
    "        for i in 2:N\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            testH = h[s - a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5696a2b2",
   "metadata": {},
   "source": [
    "Constructs a PI policy using the above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47ec161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactCont (generic function with 1 method)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piPolicyExactCont(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactCont(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c7cc371",
   "metadata": {},
   "source": [
    "Given a h table and fixed g0, approximates the g of the PI policy derived using the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "161485ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationTab (generic function with 1 method)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, h, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    println(\"State Space Completed\")\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    policy = piPolicyExactCont(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Completed\")\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = policy[s]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6efb11fd",
   "metadata": {},
   "source": [
    "Returns an array of feasible actions for N-dim state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c00dff36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enumerateFeasibleActions (generic function with 1 method)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function enumerateFeasibleActions(s,N)\n",
    "    actionSpace = []\n",
    "    if s == fill(3, N)\n",
    "        for i in 1:N\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            push!(actionSpace, a)\n",
    "        end\n",
    "        return actionSpace\n",
    "    end\n",
    "\n",
    "    push!(actionSpace, zeros(Int64,N))\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            push!(actionSpace, a)\n",
    "        end\n",
    "    end\n",
    "    return actionSpace\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884ee22f",
   "metadata": {},
   "source": [
    "# SMART Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc90c99f",
   "metadata": {},
   "source": [
    "Tabular SMART algorithm, associated gEvaluation function, and helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6bef550",
   "metadata": {},
   "source": [
    "Given a state and a q-table, return optimal action and associated Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b86ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actionFromQTab (generic function with 1 method)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function actionFromQTab(s, N, q)\n",
    "    feasibleActions = enumerateFeasibleActions(s,N)\n",
    "\n",
    "    #formulate action\n",
    "    optA = zeros(Int64, N)\n",
    "    if s == fill(3,N)\n",
    "        optA[1] = 1\n",
    "    end\n",
    "    optQ = q[s,optA]\n",
    "    for a in feasibleActions\n",
    "        testQ = q[s,a]\n",
    "        if testQ < optQ\n",
    "            optQ = testQ\n",
    "            optA = a\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optQ\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93d0c99a",
   "metadata": {},
   "source": [
    "Construct policy using above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "233657b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactContQ (generic function with 1 method)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piPolicyExactContQ(q, N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = actionFromQTab(s, N, q)[1]\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bcb0dda",
   "metadata": {},
   "source": [
    "Performs SMART, using a state-action trace and e-greedy action selection, where e can be chosen to depend on the state or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f7db3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartTab (generic function with 1 method)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smartTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    optAandQ = actionFromQTab(s, N, q)\n",
    "    optA = optAandQ[1]\n",
    "    optQ = optAandQ[2]\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        optFlag = true\n",
    "        \n",
    "        #choose e-greedy action\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            optQ = q[s,optA]\n",
    "            optFlag = false\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g if optimal action taken\n",
    "            if optFlag\n",
    "                runningTotal += c\n",
    "                timePassed += time\n",
    "                g = runningTotal/timePassed\n",
    "            end\n",
    "\n",
    "            #find next optimal action and q value\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb19c603",
   "metadata": {},
   "source": [
    "Taking a Q table as input, formulates the associated policy and simulates it to approximate g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0943848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationTabQ (generic function with 1 method)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationTabQ(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, q; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    println(\"State Space Completed\")\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    policy = piPolicyExactContQ(q, N)\n",
    "    println(\"Policy Completed\")\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = policy[s]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15b7f9ed",
   "metadata": {},
   "source": [
    "On-Policy equivalent of SMART, using next chosen action instead of next optimal action for the update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "433d3d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartOnPolicyTab (generic function with 1 method)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smartOnPolicyTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #choose only action for s = s0\n",
    "    optA = zeros(Int64, N)\n",
    "    optQ = q[s,optA]\n",
    "    optFlag = true\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "\n",
    "            #find next e-greedy action and q value\n",
    "            #find optimal action and q-value\n",
    "            optFlag = true\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            #choose epsilon\n",
    "            epsilon = c/(c + n)\n",
    "            \n",
    "            if stateDepEpsilon\n",
    "                epsilon = c/(c + numVisits[sPrime])\n",
    "            end\n",
    "\n",
    "            #select random action with probability epsilon\n",
    "            if rand(Uniform(0,1)) < epsilon\n",
    "                nextOptA = randomAction(sPrime, N)\n",
    "                nextOptQ = q[sPrime,nextOptA]\n",
    "                optFlag = false\n",
    "            end\n",
    "            \n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            optFlag = true\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            #choose epsilon\n",
    "            epsilon = c/(c + n)\n",
    "            \n",
    "            if stateDepEpsilon\n",
    "                epsilon = c/(c + numVisits[sPrime])\n",
    "            end\n",
    "\n",
    "            #select random action with probability epsilon\n",
    "            if rand(Uniform(0,1)) < epsilon\n",
    "                nextOptA = randomAction(sPrime, N)\n",
    "                nextOptQ = q[sPrime,nextOptA]\n",
    "                optFlag = false\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70994d31",
   "metadata": {},
   "source": [
    "Version of SMART using moving average window to approximate g, discarding older data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d8fa137b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartTabMA (generic function with 1 method)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SMART with an MA approximation for g\n",
    "function smartTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; window = 1000000, printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    totalCosts = [0.0]\n",
    "    lenTotalCosts = 1\n",
    "    totalTimes = [0.0]\n",
    "    lenTotal = 1\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    optAandQ = actionFromQTab(s, N, q)\n",
    "    optA = optAandQ[1]\n",
    "    optQ = optAandQ[2]\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        optFlag = true\n",
    "        \n",
    "        #choose e-greedy action\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            optQ = q[s,optA]\n",
    "            optFlag = false\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g if optimal action taken\n",
    "            if optFlag\n",
    "                runningTotal += c\n",
    "                timePassed += time\n",
    "                push!(totalCosts, runningTotal)               \n",
    "                push!(totalTimes, timePassed)\n",
    "                lenTotal += 1\n",
    "                if lenTotal <= window\n",
    "                    g = runningTotal/timePassed\n",
    "                else\n",
    "                    g = (runningTotal - totalCosts[lenTotal - window])/(timePassed - totalTimes[lenTotal - window])\n",
    "                end\n",
    "            end\n",
    "\n",
    "            #find next optimal action and q value\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "199fc408",
   "metadata": {},
   "source": [
    "# Tabular SMARPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29ea47f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabST (generic function with 1 method)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SMARPE with tabular representation instead of VFA, and state trace \n",
    "function smarpeTabST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, hIn, g0, nMax, b; copyH = false, printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    policy = piPolicyExactCont(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Constructed\")\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        if copyH\n",
    "            h[s] = hIn[s]\n",
    "        else\n",
    "            h[s] = 0.0\n",
    "        end\n",
    "        \n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optA = policy[s]\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9483ef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabStochST (generic function with 1 method)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses tabular representation instead of VFA, e-greedy action selection, and state trace \n",
    "function smarpeTabStochST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #choose random action\n",
    "        optA = randomAction(s,N)\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2da93d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabST_epsSoft_onPolicy (generic function with 1 method)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tabular SMARPE with state trace using e-soft policy\n",
    "function smarpeTabST_epsSoft_onPolicy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, hIn, g0, nMax, b, c; copyH = false, printProgress = false, modCounter = 100000, stateDepEpsilon = true)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    policy = piPolicyExactCont(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Constructed\")\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        if copyH\n",
    "            h[s] = hIn[s]\n",
    "        else\n",
    "            h[s] = 0.0\n",
    "        end\n",
    "        \n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate action\n",
    "        optA = policy[s]\n",
    "        optV = 0.0\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        #e-greedy action\n",
    "        bestA = optA\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            bestA = randomAction(s,N)\n",
    "            if bestA == zeros(Int64, N)\n",
    "                t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            end\n",
    "        end\n",
    "\n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe61cfb6",
   "metadata": {},
   "source": [
    "# New Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2411428a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subStates (generic function with 1 method)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns substates of edges in an array, and state of destination node\n",
    "function subStates(s, N, flows)\n",
    "    sis = []\n",
    "    sn = 0\n",
    "    for i in 1:N\n",
    "        if s[i] == 2 || s[i] == 3\n",
    "            push!(sis, s[i])\n",
    "        elseif flows[i] == 0\n",
    "            push!(sis, 0)\n",
    "        else\n",
    "            push!(sis, 1)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if flows == fill(0.0, N)\n",
    "        sn = 1\n",
    "    end\n",
    "    \n",
    "    return sis, sn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b40410e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 4 methods)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates v from seperate ve and vn tables\n",
    "function v(s::Vector{Int64}, N::Int64, flows::Vector{Float64}, ve::Dict, vn::Dict)\n",
    "    substates = subStates(s, N, flows)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    v = 0.0\n",
    "    for i in 1:N\n",
    "        si = sis[i]\n",
    "        v += ve[i,si]\n",
    "    end\n",
    "\n",
    "    v += vn[sn]\n",
    "\n",
    "    return v\n",
    "end\n",
    "\n",
    "function v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    flowsAndCost = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    return v(s, N, flowsAndCost[1], ve, vn)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bdcbb3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and VFA from ve and vn tables\n",
    "function expectedNextValueContNewVFA(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e63f3219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "    #find optimal action\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContNewVFA(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn) - g*t\n",
    "    zeroV = optV\n",
    "\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            \n",
    "            testV = v(s-a, N, flows, ve, vn)\n",
    "            if testV <= optV\n",
    "                optV = testV\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    #Fix choose optimal non-passive action if state is [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optV = v(s-optA, N, flows, ve, vn)\n",
    "            \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "\n",
    "                testV = v(s-a, N, flows, ve, vn)\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV, zeroV\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "66a7c3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateVFA (generic function with 3 methods)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateVFA(s, substates, target, ve, vn, numVisitsE, numVisitsN, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += (b/(b + numVisitsE[i, si]))*(target - currentEst)\n",
    "    end\n",
    "\n",
    "    vn[sn] += (b/(b + numVisitsN[sn]))*(target - currentEst)\n",
    "\n",
    "    return ve, vn\n",
    "end\n",
    "\n",
    "function updateVFA(s, substates, target, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, stepsize)\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += stepsize*(target - currentEst)\n",
    "    end\n",
    "\n",
    "    vn[sn] += stepsize*(target - currentEst)\n",
    "\n",
    "    return ve, vn\n",
    "end\n",
    "\n",
    "function updateVFA(s, substates, target, ve, vn, n, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += (b/(b + n))*(target - currentEst)\n",
    "    end\n",
    "\n",
    "    vn[sn] += (b/(b + n))*(target - currentEst)\n",
    "\n",
    "    return ve, vn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "379e3f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviNewVFA_ST (generic function with 1 method)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses new VFA architecture, e-greedy action selection, and state trace \n",
    "#stepsizeType options: \n",
    "# - varyByNumVisits: uses stepsize b/(b + numVisits)\n",
    "# - varyByIteration: uses stepsize b/(b + n) where n is the iteration modCounter\n",
    "# - constant: uses stepsize b\n",
    "function smarviNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; stepsizeType = \"varyByNumVisits\", printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    \n",
    "    #initialise ve and vn tables\n",
    "    ve = Dict()\n",
    "    numVisitsE = Dict()\n",
    "    for i in 1:N\n",
    "        for si in 0:3\n",
    "            ve[i,si] = 0.0\n",
    "            numVisitsE[i,si] = 0\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    vn = Dict()\n",
    "    numVisitsN = Dict()\n",
    "    for i in 0:1\n",
    "        vn[i] = 0.0\n",
    "        numVisitsN[i] = 0\n",
    "    end\n",
    "\n",
    "    vs0Hist = [0.0]\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    flows0 = copy(flows)\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        substates = subStates(s, N, flows)\n",
    "        sis = substates[1]\n",
    "        sn = substates[2]\n",
    "        for i in 1:N\n",
    "            numVisitsE[i,sis[i]] += 1\n",
    "        end\n",
    "\n",
    "        numVisitsN[sn] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action and v value\n",
    "        optAandV = smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        zeroV = optAandV[3]\n",
    "\n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "\n",
    "        #if random action chosen, choose action action and v value\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N) \n",
    "                optV = zeroV\n",
    "            else \n",
    "                optV = v(s - optA, N, flows, ve, vn)\n",
    "            end \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV \n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                if stepsizeType == \"varyByNumVisits\"\n",
    "                    ve,vn = updateVFA(st, substates, bestV, ve, vn, numVisitsE, numVisitsN, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "                elseif stepsizeType == \"constant\"\n",
    "                    ve,vn = updateVFA(st, substates, bestV, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "                elseif stepsizeType == \"varyByIteration\"\n",
    "                    ve,vn = updateVFA(st, substates, bestV, ve, vn, n, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "                else\n",
    "                    println(\"Invalid stepsize rule\")\n",
    "                    return 0\n",
    "                end\n",
    "\n",
    "                push!(vs0Hist, v(s0, N, flows0, ve, vn))\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return ve, vn, g, gs, vs0Hist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fba49dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationNewVFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, ve, vn, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        substates = subStates(s, N, flows)\n",
    "        sis = substates[1]\n",
    "        sn = substates[2]\n",
    "        for i in 1:N\n",
    "            numVisitsE[i,sis[i]] += 1\n",
    "        end\n",
    "\n",
    "        numVisitsN[sn] += 1\n",
    "\n",
    "        #formulate optimal action and v value\n",
    "        optAandV = smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "        optA = optAandV[1]\n",
    "\n",
    "        #update state, flows and g\n",
    "        if optA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b30394b",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9bce9bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 100.0\n",
       " 200.0\n",
       " 300.0\n",
       " 400.0\n",
       " 500.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "function tau(x)\n",
    "    return x\n",
    "end\n",
    "\n",
    "alpha_d = [0.01*i for i in 1:N]\n",
    "alpha_r = [0.001*i for i in 1:N] \n",
    "beta=10.0\n",
    "c0=[1.0*i for i in 1:N] \n",
    "c1=100.0\n",
    "r=[100.0*i for i in 1:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f00d0796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "3500000\n",
      "4000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.868438101910844"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nMax = 4000000\n",
    "resultNewVFA = smarviNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, 1.0, 10.0; stepsizeType = \"varyByNumVisits\", printProgress = true, modCounter = 500000)\n",
    "resultNewVFA[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "443cd336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x0000000067D7E430>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = floor(Int64, nMax/10)\n",
    "PyPlot.plot(resultNewVFA[4][start:nMax])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b89a343",
   "metadata": {},
   "source": [
    "Not converged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e237426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-665.7295111678026\n",
      "1393.0178297399875\n"
     ]
    }
   ],
   "source": [
    "ve = resultNewVFA[1]\n",
    "vn = resultNewVFA[2]\n",
    "println(v([1,1,1,1,1],N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn))\n",
    "println(v([3,3,3,3,3],N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3a80d675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 2 entries:\n",
       "  0 => 7.34776\n",
       "  1 => 534.439"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "69284c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x000000008EFC7A30>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs0Hist = resultNewVFA[5]\n",
    "PyPlot.plot(vs0Hist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a3e1fa1",
   "metadata": {},
   "source": [
    "## Constant Stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "52017ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "3500000\n",
      "4000000\n",
      "Complete\n",
      "26.570057230682146\n"
     ]
    }
   ],
   "source": [
    "nMax = 4000000\n",
    "resultNewVFA = smarviNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, 0.01, 10.0; stepsizeType = \"constant\", printProgress = true, modCounter = 500000)\n",
    "println(\"Complete\")\n",
    "println(resultNewVFA[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9197b38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x000000006605E7F0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPlot.plot(resultNewVFA[4][1:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "712a3b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x000000008F13FD30>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs0Hist = resultNewVFA[5]\n",
    "PyPlot.plot(vs0Hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fc6c2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-303.00214413340655\n",
      "909.8320858374173\n"
     ]
    }
   ],
   "source": [
    "ve = resultNewVFA[1]\n",
    "vn = resultNewVFA[2]\n",
    "println(v([1,1,1,1,1],N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn))\n",
    "println(v([3,3,3,3,3],N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8219338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 2 entries:\n",
       "  0 => 45.5698\n",
       "  1 => 416.651"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330a4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
