{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c967fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using Random\n",
    "using Plots\n",
    "using PyPlot\n",
    "using StatsBase\n",
    "using StatsPlots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b1ef523",
   "metadata": {},
   "source": [
    "# Problem-Suite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebbc7791",
   "metadata": {},
   "source": [
    "Problem-Suite is a large structured notebook containing all of the functions created so far for this project.\n",
    "\n",
    "Sections:\n",
    "\n",
    "-[Miscellaneous Functions](#Miscellaneous-Functions)\n",
    "\n",
    "-[Pre-requisite functions for uniformised AVI](#Pre-requisite-functions-for-uniformised-AVI)\n",
    "\n",
    "-[Uniformised AVI functions](#Uniformised-AVI-functions)\n",
    "\n",
    "-[Pre-requisite functions for SMARVI](#Pre-requisite-functions-for-SMARVI)\n",
    "\n",
    "-[SMARVI Functions](#SMARVI-Functions)\n",
    "\n",
    "-[Pre-requisite Functions for Exact DP on Homogeneous Problems](#Pre-requisite-Functions-for-Exact-DP-on-Homogeneous-Problems)\n",
    "\n",
    "-[Exact DP on Homogeneous Problems (RVIA and PE/PI)](#Exact-DP-for-Homogeneous-problem)\n",
    "\n",
    "-[Pre-requisite Functions for Exact DP on Inhomogeneous Problems](#Pre-requisite-Functions-for-Exact-DP-on-Inhomogeneous-Problems)\n",
    "\n",
    "-[Exact DP on Inhomogeneous Problems (RVIA and PE/PI)](#Exact-DP-for-Inhomogeneous-Problem-(using-exact-h-or-VFA))\n",
    "\n",
    "-[Evaluation via simulation](#Evaluation-via-simulation)\n",
    "\n",
    "-[APE on Fully Active Policy](#APE-on-Fully-Active-Policy)\n",
    "\n",
    "-[SMARPE](#SMARPE)\n",
    "\n",
    "-[Tabular SMARVI and gEval](#tabular-smarvi-and-geval)\n",
    "\n",
    "-[SMART Functions](#SMART-Functions)\n",
    "\n",
    "-[New Functions](#new-functions)\n",
    "\n",
    "-[Tests](#Tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12fe12",
   "metadata": {},
   "source": [
    "# Miscellaneous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2556d9f",
   "metadata": {},
   "source": [
    "-Functions for enumerating state and action spaces\n",
    "\n",
    "-Functions for calculating flows given a state or state-action pair\n",
    "\n",
    "-Function for evaluating a VFA at a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a0da98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arrayToString (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#produce an array of array representations of all possible states\n",
    "function enumerateStates(N::Int64)\n",
    "    if N==1\n",
    "        return [[1],[2],[3]]\n",
    "    end\n",
    "    \n",
    "    output = []\n",
    "    lower = enumerateStates(N-1)\n",
    "    for s in lower\n",
    "        new1 = append!([1],s)\n",
    "        new2 = append!([2],s)\n",
    "        new3 = append!([3],s)\n",
    "        append!(output,[new1])\n",
    "        append!(output,[new2])\n",
    "        append!(output,[new3])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "#produce an array of array representations of all possible actions\n",
    "function enumerateActions(N::Int64)\n",
    "    if N==1\n",
    "        return [[0],[1]]\n",
    "    end\n",
    "    \n",
    "    output = []\n",
    "    lower = enumerateActions(N-1)\n",
    "    for a in lower\n",
    "        new1 = append!([0],a)\n",
    "        new2 = append!([1],a)\n",
    "        append!(output,[new1])\n",
    "        append!(output,[new2])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end    \n",
    "\n",
    "#produce array of array representations of all restricted, or single-repair, actions\n",
    "function enumerateRestrictedActions(N::Int64)\n",
    "    if N==1\n",
    "        return [[0],[1]]\n",
    "    end\n",
    "    \n",
    "    output = [zeros(Int64,N)]\n",
    "    for i in 1:N\n",
    "        temp = zeros(N)\n",
    "        temp[i] = 1\n",
    "        append!(output,[temp])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "#convert all array elements to string, then concatanate all elements (DEPRECATED AS DICTS CAN TAKE ARRAYS AS KEYS)\n",
    "function arrayToString(x)\n",
    "    return join(string.(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ba543e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateFlows (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function for calculating the flows given a state\n",
    "function calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    #update flows\n",
    "    flows = zeros(N)\n",
    "    healthy = sum(i == 1 for i in s)\n",
    "    \n",
    "    #if no links are healthy, return \n",
    "    if healthy == 0\n",
    "        return flows, c1\n",
    "    end\n",
    "    \n",
    "    #otherwise, find best route, and return\n",
    "    bestCost = maximum(c0) + 1\n",
    "    usedLink = 0\n",
    "    for k in 1:N\n",
    "        if s[k] == 1 && c0[k] < bestCost\n",
    "            bestCost = c0[k]\n",
    "            usedLink = k\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[usedLink] = beta\n",
    "    \n",
    "    return flows, bestCost\n",
    "end\n",
    "\n",
    "#function for calculating the flows given a state-action pair\n",
    "function calculateFlows(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    sPrime = s - a\n",
    "    return calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "127d3f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate a VFA at a given state\n",
    "function v(s::Vector{Int64}, params::Vector{Float64}, features::Vector{Function})\n",
    "    numFeatures = length(features)\n",
    "    return params[1] + sum(params[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "960bf4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version of v that takes flows for the features\n",
    "function v(s::Vector{Int64}, flows::Vector{Float64}, params::Vector{Float64}, features::Vector{Function})\n",
    "    N = length(params)\n",
    "    return params[1] + sum(params[i]*features[i-1](s, flows) for i in 2:N)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05d0fc",
   "metadata": {},
   "source": [
    "# Pre-requisite functions for uniformised AVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c56072f",
   "metadata": {},
   "source": [
    "This section contains functions used within the AVI algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "476755cd",
   "metadata": {},
   "source": [
    "Given a state-action pair, return the next random pre-decision state, the instant cost, and the updated flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d157b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateStateAndFlowsUnif (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateStateAndFlowsUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #observe exogenous information\n",
    "    w = rand(Uniform(0, 1))\n",
    "    \n",
    "    #interpret exog info: is it a demand deg, rare deg, or completed repair \n",
    "    found = false\n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        if runningTotal <= w <= runningTotal + flows[k]*alpha_d[k]*del\n",
    "            found = true\n",
    "            sPrime[k] = 3\n",
    "            #println(\"Demand Deg at \"*string.(k))\n",
    "            break\n",
    "        end\n",
    "        runningTotal = runningTotal + flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    if found == false\n",
    "        for k in 1:N\n",
    "            if runningTotal <= w <= runningTotal + alpha_r[k]*del\n",
    "                found = true\n",
    "                sPrime[k] = 3\n",
    "                #println(\"Rare Deg at \"*string.(k))\n",
    "                break\n",
    "            end\n",
    "            runningTotal = runningTotal + alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if found == false && repair > 0\n",
    "        if runningTotal <= w <= runningTotal + tau(repair)*del\n",
    "            found = true\n",
    "            #find all repairing links\n",
    "            repairing = []\n",
    "            for k in 1:N\n",
    "                if sPrime[k] == 2\n",
    "                    append!(repairing,[k])\n",
    "                end\n",
    "            end\n",
    "            repaired = sample(repairing)\n",
    "            sPrime[repaired] = 1\n",
    "            #println(\"Repair completed at \"*string.(repaired))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if found == false\n",
    "        #println(\"No Event\")\n",
    "    end\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    \n",
    "    return sPrime, (beta*bestCost + sum(r[k]*(sPrime[k]==2) for k in 1:N))*del, newFlows\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "836f070e",
   "metadata": {},
   "source": [
    "Given a state action pair, return the instant cost over the delta timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "685eb320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostUnif (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instant cost over the timestep\n",
    "function instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(sPrime[i] == 1 for i in 1:N)\n",
    "    repair = sum(sPrime[i] == 2 for i in 1:N)\n",
    "    damaged = sum(sPrime[i] == 3 for i in 1:N)\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    \n",
    "    return (beta*bestCost + sum(r[k]*(sPrime[k]==2) for k in 1:N))*del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9603d5cd",
   "metadata": {},
   "source": [
    "Given a state-action pair and a VFA, calculate the expected value of the value function after one timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0a307f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueUnif (generic function with 2 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h. Also used in Exact PE/PI when using a VFA\n",
    "#One version takes flows as an argument, the other calculates the flows\n",
    "function expectedNextValueUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*v(sPrime, vParams, features)\n",
    "end  \n",
    "\n",
    "function expectedNextValueUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, vParams, features)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    flows = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*v(sPrime, vParams, features)\n",
    "end  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3920172",
   "metadata": {},
   "source": [
    "# Uniformised AVI functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80cbe1e5",
   "metadata": {},
   "source": [
    "Algorithms that perform RAVI on the uniformised version of the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5310d49c",
   "metadata": {},
   "source": [
    "Given some parallel link problem and VFA architecture, perform RAVI, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f83714d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in uniformised setting, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state\n",
    "function aviApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - g\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + v(sPrime, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "718d21d7",
   "metadata": {},
   "source": [
    "Given some parallel link problem and VFA architecture, perform RAVI, using a full expectation for update targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fb36c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviFull (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function aviFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - g\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d76fa41",
   "metadata": {},
   "source": [
    "Similar to above, but only uses the Binary Action Space (BAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ebdac30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviUnifBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI with BAS in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function aviUnifBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        vs0 = v(s0, vParams, features)\n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - vs0\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        testV = instantCostUnif(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - vs0\n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA = testA\n",
    "            optV = testV\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f587fd5b",
   "metadata": {},
   "source": [
    "# Pre-requisite functions for SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2008a2c4",
   "metadata": {},
   "source": [
    "Helper functions for the SMARVI algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1f36cda",
   "metadata": {},
   "source": [
    "Given a state-action pair and pre-calculated flows, return the expected sojourn time for the state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1506350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sojournTime (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the expected sojourn time of a state-action pair\n",
    "function sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    s = s - a\n",
    "    if s == fill(3,N)\n",
    "        return 1/(beta*sum(alpha_d) + sum(alpha_r) + tau(N))\n",
    "    end\n",
    "    \n",
    "    numRep = sum(i == 2 for i in s)\n",
    "    cumulativeRate = 0.0\n",
    "    for i in 1:N\n",
    "        if s[i] == 1\n",
    "            cumulativeRate += flows[i]*alpha_d[i] + alpha_r[i]\n",
    "        elseif s[i] == 2\n",
    "            cumulativeRate += alpha_r[i] + tau(numRep)/numRep\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return 1/cumulativeRate\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb2c6d2b",
   "metadata": {},
   "source": [
    "Given a state-action pair and flows, calculate the expected cost accumulated until a transition occurs, or calculate the simulated cost accumulated over a simulated time del."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66c5307b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostCont (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the expected cost accumulated until a transition \n",
    "function instantCostCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows; del = 0)\n",
    "    if del == 0\n",
    "        del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    end\n",
    "    \n",
    "    return instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ecc13f2",
   "metadata": {},
   "source": [
    "Given a state-action pair, return the next random pre-decision state, the cost accumulated over the sojourn time, and the updated flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75587424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateStateAndFlowsCont (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given a state-action pair, return the next random pre-decision state, the cost accumulated over the sojourn time, and the updated flows\n",
    "function updateStateAndFlowsCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    actualTime = rand(Exponential(del))\n",
    "    result = updateStateAndFlowsUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "    return result[1], instantCostCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows; del = actualTime), result[3], actualTime\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae4efc43",
   "metadata": {},
   "source": [
    "Given a state-action pair, precalculated flows, and a VFA, return the expected value of the VFA after a transition has occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60cda4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueCont (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h\n",
    "function expectedNextValueCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime,vParams,features)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc5b77a7",
   "metadata": {},
   "source": [
    "Similar to above, but assumes the features of the VFA take precalcuated flows as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "488f16a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContFlows (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h\n",
    "#Assumes the VFA is constructed with features taking arguments (s, flows), so flows are precalculated\n",
    "function expectedNextValueContFlows(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime, flows, vParams,features)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, flowsNext, vParams, features)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, flowsNext, vParams, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, flowsNext, vParams, features)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a2e614d",
   "metadata": {},
   "source": [
    "Given a state, flows, and a VFA-g pair, return the optimal action and associated V value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97b01347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "    #formulate optimal action and calculate optV\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "    \n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            testV = v(s-a, vParams, features)\n",
    "            if testV <= optV\n",
    "                optV = testV\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #If wanted, force a repair if optA is passive for [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA = zeros(Int64,N)\n",
    "        optA[1] = 1\n",
    "        optV = v(s-optA, vParams, features)\n",
    "        \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                testV = v(s-a, vParams, features)\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f824e",
   "metadata": {},
   "source": [
    "# SMARVI Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dbccab0",
   "metadata": {},
   "source": [
    "Variety of functions which perform the SMARVI algorithm, with different additional features.\n",
    "For clarity, the \"state-trace\" is a method which collects a sequence of states connected by instantaneous actions together, and ensures they all have the same update target. For example, if in state s we take action a!=0, and in the resulting state s+a we take action 0, both s and s+a will have update target (c + E(V(s')) - gt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39c05112",
   "metadata": {},
   "source": [
    "Given a problem and a VFA architecture, perform SMARVI, with no e-greedy action selection or state trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea289cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs SMARVI\n",
    "function smarvi(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action and calculate optV\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = optV - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5c6e48e",
   "metadata": {},
   "source": [
    "Perform SMARVI with a state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2005234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviST (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in continuous time setting, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "function smarviST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    \n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    actionFlag = false\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update stateTrace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate optimal action and calculate optV\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #if optimal action is passive, update VFA for all states in the stateTrace, and simulate the next state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            \n",
    "            #find simulated next state\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "        \n",
    "            bestV = optV - v(s0, vParams, features)\n",
    "            \n",
    "            #update VFA\n",
    "            traceLength = length(stateTrace)\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            #reset stateTrace\n",
    "            stateTrace = []\n",
    "            \n",
    "            #update flows and average\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #if some action is optimal, simply update the state\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58458323",
   "metadata": {},
   "source": [
    "Performs SMARVI with a given fixed value of g0 for action selection. This prevents bad initial estimates of g from severely impacting the algorithm, but restricts the policy space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1840b7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions, and controlling action selection using some fixed g0\n",
    "function smarvi_g0(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g0)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV + g0*t - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25861988",
   "metadata": {},
   "source": [
    "Similar to regular SMARVI, but only using the BAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "318f0267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI with BAS in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "function smarviBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        tPassive = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tPassive\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        tActive = sojournTime(s, testA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        testV = instantCostCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tActive\n",
    "        \n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        #Ignore passive action for broken network\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d439d00",
   "metadata": {},
   "source": [
    "Similar to regular SMARVI, but where flows are assumes to be passed to the VFA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0daaef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviFlows (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version of smarvi where flows are passed to features\n",
    "function smarviFlows(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    flows0 = copy(flows)\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContFlows(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                \n",
    "                if vParams[1] + sum(vParams[i+1]*features[i](s-a, flows) for i in 1:numFeatures) <= optV\n",
    "                    optV = vParams[1] + sum(vParams[i+1]*features[i](s-a, flows) for i in 1:numFeatures)\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, flows, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    testV = v(s-a, flows, vParams, features)\n",
    "                    if testV <= optV\n",
    "                        optV = testV\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - v(s0, flows0, vParams,features)\n",
    "        else\n",
    "            bestV = optV - v(s0, flows0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s, flows) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s, flows) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8365b78",
   "metadata": {},
   "source": [
    "## e-greedy SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f3cba33",
   "metadata": {},
   "source": [
    "Helper functions and main algorithm for e-greedy SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fd55c88",
   "metadata": {},
   "source": [
    "Samples a random feasible action for a state s of length N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2fd2e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randomActionAllDamaged (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate random action\n",
    "function randomAction(s,N)\n",
    "    #deal with all-damaged case\n",
    "    if s == fill(3,N)\n",
    "        return randomActionAllDamaged(N)\n",
    "    end\n",
    "\n",
    "    damaged = [0]\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            append!(damaged, [i])\n",
    "        end\n",
    "    end\n",
    "            \n",
    "    choice = sample(damaged)\n",
    "    optA = zeros(Int64, N)\n",
    "    if choice == 0\n",
    "        return optA\n",
    "    else\n",
    "        optA[choice] = 1\n",
    "        return optA\n",
    "    end\n",
    "end\n",
    "\n",
    "#calculate random action for [3,3...,3] state\n",
    "function randomActionAllDamaged(N)\n",
    "    choice = sample(1:N)\n",
    "    a = zeros(Int64, N)\n",
    "    a[choice] = 1\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48f72466",
   "metadata": {},
   "source": [
    "Performs an e-greedy version of SMARVI, with e_n = b/b+n for some given b. Allows SMARVI to perform some exploratory actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a38d016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi_epsGreedy (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#actions are choosen via e-greedy action selection, where a random action is chosen with probability b/b+n\n",
    "function smarvi_epsGreedy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; b = 1.0, printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate e-greedy action\n",
    "        if rand(Uniform(0,1)) <= b/(b + n) \n",
    "            optA = randomAction(s,N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "            else\n",
    "                optV = v(s-optA, vParams, features)\n",
    "            end\n",
    "        else                \n",
    "            optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "            optA = optAandV[1]\n",
    "            optV = optAandV[2]\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eaa235",
   "metadata": {},
   "source": [
    "# Pre-requisite Functions for Exact DP on Homogeneous Problems "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7b7c104",
   "metadata": {},
   "source": [
    "Helper functions for Homogeneous Exact DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1801225",
   "metadata": {},
   "source": [
    "Calculates instant cost for homogeneous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22678b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instant cost function strictly for homogeneous problem\n",
    "function instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    i1Prime = i1 - a\n",
    "    i2Prime = i2 + a\n",
    "    \n",
    "    #if no links are healthy, return \n",
    "    if N - i1 - i2 == 0\n",
    "        return (beta*c1 + r*i2Prime)*del\n",
    "    end\n",
    "    \n",
    "    \n",
    "    return (beta*c0 + r*i2Prime)*del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94e504b7",
   "metadata": {},
   "source": [
    "Given state (i_1, i_2) and action a, calculates the expected next value function after one timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a17eacf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates E(h(s')) from s,a strictly for a homogeneous problem\n",
    "function expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    i1Prime = i1 - a\n",
    "    i2Prime = i2 + a\n",
    "    thisH = h[i1+1,i2+1]\n",
    "    \n",
    "    #if all are damaged\n",
    "    if i1Prime == N\n",
    "        return thisH\n",
    "    end\n",
    "    \n",
    "    #if none are healthy\n",
    "    if N - i1 - i2 == 0\n",
    "        return thisH + tau(i2Prime)*del*(h[i1Prime+1,i2Prime-1+1] - thisH) + i2Prime*del*alpha_r*(h[i1Prime+1+1, i2Prime-1+1] - thisH)\n",
    "    end\n",
    "    \n",
    "    #if none are repairing\n",
    "    if i2Prime == 0\n",
    "        return thisH + (beta*alpha_d + (N - i1 - i2)*alpha_r)*del*(h[i1Prime+1+1,i2Prime+1] - thisH)\n",
    "    end\n",
    "    \n",
    "    return thisH + (beta*alpha_d + (N - i1 - i2)*alpha_r)*del*(h[i1Prime+1+1,i2Prime+1] - thisH) + i2Prime*alpha_r*del*(h[i1Prime+1+1,i2Prime-1+1] - thisH) + tau(i2Prime)*del*(h[i1Prime+1,i2Prime-1+1] - thisH)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68c7af12",
   "metadata": {},
   "source": [
    "Given state (i_1,i_2) and value function h, calculates and returns the best action for the state using full expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beeb2b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the exact PI action strictly for a homogeneous problem\n",
    "function piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "    if i1 == 0\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    if i1 == N && forceActive\n",
    "        optA = 1\n",
    "        optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        for a in 2:i1\n",
    "            testH = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            if testH <= optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "        return optA\n",
    "    end\n",
    "    \n",
    "    optA = 0\n",
    "    optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    for a in 1:i1\n",
    "        testH = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        if testH <= optH\n",
    "            optA = a\n",
    "            optH = testH\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71928099",
   "metadata": {},
   "source": [
    "Similar to above, but uses the approximation Q(s,a) = h(s+a) for a!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b58a1177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the approx PI action based on instantaneous approximation, strictly for a homogeneous problem\n",
    "function piActionHomogApprox(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = false)\n",
    "    #deal with \"nothing damaged\" edge case\n",
    "    if i1 == 0\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    #deal with \"everything damaged\" edge case\n",
    "    if i1 == N && forceActive\n",
    "        optA = 1\n",
    "        optH = h[i1-optA+1,i2+optA+1]\n",
    "        for a in 2:i1\n",
    "            testH = h[i1-a+1,i2+a+1]\n",
    "            if testH <= optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "        return optA\n",
    "    end\n",
    "    \n",
    "    optA = 0\n",
    "    optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) - g*del\n",
    "    for a in 1:i1\n",
    "        testH = h[i1-a+1,i2+a+1]\n",
    "        if testH <= optH\n",
    "            optA = a\n",
    "            optH = testH\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8ab05cf",
   "metadata": {},
   "source": [
    "Given h, constructs optimal policy using exact PI method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6335a065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the exact PI policy strictly for a homogeneous problem\n",
    "function piPolicyHomog(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "    policy = zeros(Int64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            policy[i1+1,i2+1] = piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34525984",
   "metadata": {},
   "source": [
    "Given h, constructs optimal policy using Q(s,a) = h(s,a) for a!=0 approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "404bfec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the approx PI policy based on instantaneous approximation, strictly for a homogeneous problem\n",
    "function piPolicyHomogApprox(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = false)\n",
    "    policy = zeros(Int64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            policy[i1+1,i2+1] = piActionHomogApprox(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = forceActive)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905b77bf",
   "metadata": {},
   "source": [
    "Constructs a h table from a VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42156f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hFromVFAHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hFromVFAHomog(N, params, features)\n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return hIn\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c11abe",
   "metadata": {},
   "source": [
    "# Exact DP for Homogeneous problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fc32308",
   "metadata": {},
   "source": [
    "Actual DP algorithms "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99f7e6cf",
   "metadata": {},
   "source": [
    "Given a h table, performs PE on PI policy derived from h, and returns g, h, n (number of iterations), and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc148338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given exact h function, strictly for a homogeneous problem \n",
    "function rpiHomog(N::Int64, hIn, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    policy = piPolicyHomog(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = policy[i1+1,i2+1]\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b8046e6",
   "metadata": {},
   "source": [
    "Performs PE on the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e39012f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpeFAHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates the fully active policy, strictly for a homogeneous problem \n",
    "function rpeFAHomog(N::Int64, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = i1\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c051660",
   "metadata": {},
   "source": [
    "Similar to rpiHomog, but uses Q(s,a) = h(s,a) approximation for PI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "964c01ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates an approximate PI policy based on a given exact h function and instananeous actions, strictly for a homogeneous problem \n",
    "function rpiHomogApprox(N::Int64, hIn, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64, g::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    policy = piPolicyHomogApprox(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = forceActive)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = policy[i1+1,i2+1]\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e1c1de",
   "metadata": {},
   "source": [
    "Similar to above, but uses VFA as input h, and uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7146cc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogVFAApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given VFA, using instananeous approximation, strictly for a homogeneous problem \n",
    "function rpiHomogVFAApprox(N::Int64, params, features, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64, g::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    \n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #run standard function\n",
    "    return rpiHomogApprox(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = nMax, delScale = delScale, forceActive = forceActive)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54d455db",
   "metadata": {},
   "source": [
    "Similar to above, WITHOUT Q(s,a) = h(s+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04dc9704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given VFA, strictly for a homogeneous problem \n",
    "function rpiHomogVFA(N::Int64, params, features, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    \n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #run standard function\n",
    "    return rpiHomog(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, forceActive = forceActive)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "690683ed",
   "metadata": {},
   "source": [
    "Performs RVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "939c3f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rviHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs RVIA \n",
    "function rviHomog(N::Int64, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b3849",
   "metadata": {},
   "source": [
    "# Pre-requisite Functions for Exact DP on Inhomogeneous Problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1e53ece",
   "metadata": {},
   "source": [
    "Helper functions for inhomogeneous exact DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14ea586f",
   "metadata": {},
   "source": [
    "Given a state-action pair and h, calculates the expected next value of the value function after one timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d45adbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueExact (generic function with 1 method)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates E(h(s')) from s,a using exact h table\n",
    "function expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    \n",
    "    flows = zeros(Float64, N)\n",
    "    if healthy > 0\n",
    "        #otherwise, find best route, and return\n",
    "        bestCost = maximum(c0) + 1\n",
    "        usedLink = 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 1 && c0[k] < bestCost\n",
    "                bestCost = c0[k]\n",
    "                usedLink = k\n",
    "            end \n",
    "        end\n",
    "        \n",
    "        flows[usedLink] = beta\n",
    "    end\n",
    "    \n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*h[sNext]\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*h[sNext]\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*h[sNext]\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*h[sPrime]\n",
    "end "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f2dad95",
   "metadata": {},
   "source": [
    "Given a state and a h table, calculates the PI action for s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b71c013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExact (generic function with 1 method)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table\n",
    "function piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2cb0d43",
   "metadata": {},
   "source": [
    "Similar to above, but uses the approximation Q(s,a) = h(s+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17b1bbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table, based off instananeous actions\n",
    "function piActionExactInstant(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h) - g*del\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = h[s-a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7eb2f91a",
   "metadata": {},
   "source": [
    "Similar to above, but uses a VFA instead of a h table, WITHOUT Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bab1aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using a VFA\n",
    "function piActionVFA(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4096d38",
   "metadata": {},
   "source": [
    "Similar to above, WITH Q(s,a) = h(s,a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "604bf237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using a VFA and instananeous actions\n",
    "function piActionVFAInstant(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features) - g*del\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = v(s-a,params,features)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884618d7",
   "metadata": {},
   "source": [
    "Constructs PI policy using h table and no approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1dfe8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExact (generic function with 1 method)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy using exact h table\n",
    "function piPolicyExact(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8842b6de",
   "metadata": {},
   "source": [
    "Constructs PI policy using h table and Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfb9dcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy using exact h table, interpretting h with instant actions\n",
    "function piPolicyExactInstant(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactInstant(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d2fa28",
   "metadata": {},
   "source": [
    "Constructs PI policy using VFA and no approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72a89a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy from a VFA\n",
    "function piPolicyVFA(params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionVFA(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d72cd38",
   "metadata": {},
   "source": [
    "Constructs PI policy using VFA and Q(s,a) = h(s+a) approximation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14bba9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy from a VFA, using instant actions to interpret h\n",
    "function piPolicyVFAInstant(params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionVFAInstant(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f8cc379",
   "metadata": {},
   "source": [
    "Constructs h table from VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28d6c8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hFromVFAInhomog (generic function with 1 method)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hFromVFAInhomog(N, params, features)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = v(s, params. features)\n",
    "    end\n",
    "\n",
    "    return h\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7172ee4",
   "metadata": {},
   "source": [
    "# Exact DP for Inhomogeneous Problem (using exact h or VFA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43e2ee1b",
   "metadata": {},
   "source": [
    "DP algorithms for inhomogeneous problem\n",
    "\n",
    "Note that throughout when we talk of a Q(s,a) = h(s+a) approximation, this only refers to action selection and not update rules, and excludes a=0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2225adc2",
   "metadata": {},
   "source": [
    "Given an explicit policy table, performs PE, returns g, h and n (# of iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b118e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpe (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs PE using exact policy table\n",
    "function rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    h = Dict()\n",
    "    w = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        w[s] = 0.0\n",
    "    end\n",
    "    s0  = fill(1, N)\n",
    "    n = 0\n",
    "    \n",
    "    #do until max iterations met or epsilon convergence\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #find updates for every state\n",
    "        for s in stateSpace\n",
    "            a = policy[s]\n",
    "            w[s] = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "        end\n",
    "        \n",
    "        #calculate relative values and delta\n",
    "        delta = 0\n",
    "        for s in stateSpace\n",
    "            update = w[s] - w[s0]\n",
    "            if delta < update - h[s] || delta == 0\n",
    "                delta = update - h[s]\n",
    "            end\n",
    "            \n",
    "            h[s] = update\n",
    "        end\n",
    "        \n",
    "        #stopping condition\n",
    "        if delta < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if printProgress && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    a = zeros(Int64, N)\n",
    "    g = instantCostUnif(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h) - h[s0]\n",
    "    \n",
    "    return g/del, h, n\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1ad5a2b",
   "metadata": {},
   "source": [
    "Given a h table, constructs PI policy and performs PE, returning g, h, n and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c57636f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiExact (generic function with 1 method)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using exact h table\n",
    "function rpiExact(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    policy = piPolicyExact(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    output = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "    return output[1], output[2], output[3], policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec78eb5a",
   "metadata": {},
   "source": [
    "Similar to above, but uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "266027ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using exact h table, using instant actions to interpet h\n",
    "function rpiExactInstant(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    policy = piPolicyExactInstant(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    output = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "    return output[1], output[2], output[3], policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa6660e",
   "metadata": {},
   "source": [
    "Performs PE on the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f773f8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpeFA (generic function with 1 method)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs exact PE on the fully-active policy\n",
    "function rpeFA(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = faAction(s)\n",
    "    end\n",
    "    \n",
    "    return rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c325a927",
   "metadata": {},
   "source": [
    "Performs PE on fully passive policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d304fb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpePassive (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs exact PE on the passive policy\n",
    "function rpePassive(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    return rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52c45280",
   "metadata": {},
   "source": [
    "Given a VFA, constructs PI policy and performs PE, returning g, h, n and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ec6cacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using VFA\n",
    "function rpiVFA(N, params, features, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    hIn = hFromVFAInhomog(N, params, features)\n",
    "    return rpiExact(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68209e10",
   "metadata": {},
   "source": [
    "Similar to above, but uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a19903b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using VFA and instantaneous actions to interpret h\n",
    "function rpiVFAInstant(N, params, features, g, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    hIn = hFromVFAInhomog(N, params, features)\n",
    "    return rpiExactInstant(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b0b6c10",
   "metadata": {},
   "source": [
    "Performs RVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d38fbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rvi (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs RVIA\n",
    "function rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    h = Dict()\n",
    "    w = Dict()\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        w[s] = 0.0\n",
    "        policy[s] = zeros(Int64,N)\n",
    "    end\n",
    "    s0  = fill(1, N)\n",
    "    n = 0\n",
    "    \n",
    "    #do until max iterations met or epsilon convergence\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #find updates for every state\n",
    "        for s in stateSpace\n",
    "            a = piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            w[s] = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "        end\n",
    "        \n",
    "        #calculate relative values and delta\n",
    "        delta = 0\n",
    "        for s in stateSpace\n",
    "            update = w[s] - w[s0]\n",
    "            if delta < update - h[s] || delta == 0\n",
    "                delta = update - h[s]\n",
    "            end\n",
    "            \n",
    "            h[s] = update\n",
    "        end\n",
    "        \n",
    "        #stopping condition\n",
    "        if delta < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if printProgress && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    a = zeros(Int64, N)\n",
    "    g = instantCostUnif(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h) - h[s0]\n",
    "    \n",
    "    return g/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb424a1e",
   "metadata": {},
   "source": [
    "# Evaluation via simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bcbbb37",
   "metadata": {},
   "source": [
    "Various evaluation functions for approximating g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ef631d9",
   "metadata": {},
   "source": [
    "Takes a trained VFA and learns g, using g also for control, starting from state s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8261907a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA via PI using simulation\n",
    "function gEvaluation(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, printState = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if printState\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "320c63e1",
   "metadata": {},
   "source": [
    "Similar to above, but starting from a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9fd18438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFromS (generic function with 1 method)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluationFromS(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    flowResult = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    flows = flowResult[1]\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end \n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ceb6a98",
   "metadata": {},
   "source": [
    "Similar to gEvaluation, but uses a fixed g0 for control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba5ce5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluation_g0(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3cd3eae",
   "metadata": {},
   "source": [
    "Similar to above, but starts from a given state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b85a37ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFromS_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluationFromS_g0(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    flowResult = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    flows = flowResult[1]\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end \n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6636431",
   "metadata": {},
   "source": [
    "Finds the g of the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33be91d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFA (generic function with 1 method)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates the FA policy\n",
    "function gEvaluationFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate FA action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d651dbd0",
   "metadata": {},
   "source": [
    "Similar to gEvaluation, but only uses the BAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8ba1023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA via PI using simulation\n",
    "function gEvaluationBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        tPassive = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tPassive\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        tActive = sojournTime(s, testA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        testV = instantCostCont(s,testA, N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tActive\n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA = testA\n",
    "            optV = testV\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            append!(runningTotals, [runningTotal])\n",
    "            timePassed += time\n",
    "            append!(times,[timePassed])\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d81acf",
   "metadata": {},
   "source": [
    "Similar to gEvaluation_g0, but assumes that flows are passed to the VFA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "862863bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation_g0_flows (generic function with 1 method)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluation_g0_flows(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContFlows(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, flows, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, flows, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, flows, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88307ffe",
   "metadata": {},
   "source": [
    "# APE on Fully Active Policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7749e9f",
   "metadata": {},
   "source": [
    "Performs APE on the Fully Active Policy using each of the four approaches to estimating a VFA (mixes of uniform/smar and simulated-next-state/expectation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "666815fa",
   "metadata": {},
   "source": [
    "Returns the FA action for a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1319dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "faAction (generic function with 1 method)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computes the Fully Active action for a given state s\n",
    "function faAction(s)\n",
    "    N = length(s)\n",
    "    a = zeros(Int64,N)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a[i] = 1\n",
    "        end\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9168d0b",
   "metadata": {},
   "source": [
    "Evaluates the FA policy using a VFA and uniformisation, and update targets c + V(s') - gt, where s' is simulated next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e894ba39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAUnifApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE of FA policy in uniformised setting, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state\n",
    "function apeFAUnifApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + v(sPrime, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e96581e",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation for updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ccde3d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAUnifFull (generic function with 1 method)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE of FA policy in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function apeFAUnifFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + expectedNextValueUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d7bdaa9",
   "metadata": {},
   "source": [
    "Performs SMARPE on FA policy, with update target c + V(s') - gt where s' is the next simulated state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd7df03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs SMARPE on FA policy, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "function smarpeFAApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + v(sPrime, vParams, features) - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af073bb3",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation in update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d8865800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAFull (generic function with 1 method)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarpeFAFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + expectedNextValueCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ceece53",
   "metadata": {},
   "source": [
    "Similar to smarpeFAApprox, but incorporates state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b68eb6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAApproxST (generic function with 1 method)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE on FA policy in continuous time setting, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "#Also incorporates the state trace when actions are taken\n",
    "function smarpeFAApproxST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update state trace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #for passive action, do proper update\n",
    "        if bestA == zeros(Int64,N)\n",
    "            #find value of v^n\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + v(sPrime, vParams, features) - g*t - v(s0, vParams,features)\n",
    "\n",
    "            #update VFA\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            stateTrace = []\n",
    "            \n",
    "            #update g, state, and flows\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #for other action, simply update state and move on\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c19d280d",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation for update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "47d8caaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAFullST (generic function with 1 method)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE on FA policy in continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Also incorporates the state trace when actions are taken\n",
    "function smarpeFAFullST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update state trace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #for passive action, do proper update\n",
    "        if bestA == zeros(Int64,N)\n",
    "            #find value of v^n\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + expectedNextValueCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t - v(s0, vParams,features)\n",
    "\n",
    "            #update VFA\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            stateTrace = []\n",
    "            \n",
    "            #update g, state, and flows\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #for other action, simply update state and move on\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f5b89",
   "metadata": {},
   "source": [
    "# SMARPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54992054",
   "metadata": {},
   "source": [
    "## Semi-Markov Approximate Relative Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be11b69",
   "metadata": {},
   "source": [
    "- SMARPE takes some trained VFA as input, and seeks to learn the associated long run cost g and the value function of the policy derived from the given VFA\n",
    "\n",
    "- Standard SMARPE uses the online training value of g for action selection, allowing the policy to vary throughout training\n",
    "\n",
    "- SMARPE_g0 takes a pre-learned value of g0 to be used for action selection, keeping the policy constant throughout. This value of g0 might be taken directly from SMARVI or from some gEval function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffc082",
   "metadata": {},
   "source": [
    "Worth also discussing is the exact behaviour of the gEval functions.\n",
    "\n",
    "- Standard gEval simply evaluates a VFA, and learns g throughout. In turn, this value of g is used for action selection, so the policy may vary throughout evaluation.\n",
    "\n",
    "- gEval_g0 evaluates a VFA-g0 pair, keeping the policy constant throughout. It may be good practice to always follow standard gEval with gEval_g0, due to the lack of policy variability.\n",
    "\n",
    "gEval functions are the part that actually calculate the PI actions based on the VFAs derived from SMARPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a0e5de3",
   "metadata": {},
   "source": [
    "Given a VFA-g pair, evaluates the PI policy derived from the pair via a new VFA with the same architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6082e341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpe (generic function with 1 method)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE in the continuous time setting, approximating E(h(s')) using all possible transitions, and with a fixed g0 for action selection\n",
    "function smarpe(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, paramsIn, paramsOut, features, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [paramsOut]\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, paramsIn, features, g0)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #recalculate optA in terms of new VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, paramsOut, features) - g*t\n",
    "        else\n",
    "            optV = v(s - bestA, paramsOut, features)\n",
    "        end \n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, paramsOut ,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, paramsOut, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        paramsOut = paramsOut + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[paramsOut])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs, [g])\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return paramsOut, paramHist, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d24567",
   "metadata": {},
   "source": [
    "# Tabular SMARVI and gEval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b101ab9",
   "metadata": {},
   "source": [
    "Tabular SMARVI algorithms (non e-greedy, e-greedy, and e-greedt with state trace), associated gEval function, and helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a829f15",
   "metadata": {},
   "source": [
    "Given a state s, its flows, and a h-g pair, return the optimal action and V value for s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d500f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromTable (generic function with 1 method)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "    #find optimal action\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        \n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "               \n",
    "            if h[s-a] <= optV\n",
    "                optV = h[s-a]\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    #Fix choose optimal non-passive action if state is [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optV = h[s-optA]\n",
    "            \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                testV = h[s-a]\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94fd262e",
   "metadata": {},
   "source": [
    "Given a state-action pair and a h table, compute the next expected h value given that a transition has occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bd0e5b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContTab (generic function with 1 method)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and tabular h\n",
    "function expectedNextValueContTab(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return h[sPrime]\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*h[sNext]\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*h[sNext]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*h[sNext]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e27d5cd6",
   "metadata": {},
   "source": [
    "Tabular version of SMARVI, with no state trace or e-greedy actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6447836b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab (generic function with 1 method)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Uses tabular representation instead of VFA\n",
    "function smarviTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - h[s0]\n",
    "        else\n",
    "            bestV = optV - h[s0]\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = h[s]\n",
    "        h[s] += (b/(b + numVisits[s]))*(bestV - currentEst)\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2833561",
   "metadata": {},
   "source": [
    "e-greedy version of the above. e can be chosen to depend on the state or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf860122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab_epsGreedy (generic function with 1 method)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Uses tabular representation instead of VFA and e-greedy action selection\n",
    "function smarviTab_epsGreedy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = h[s]\n",
    "        h[s] += (b/(b + numVisits[s]))*(bestV - currentEst)\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f7777e",
   "metadata": {},
   "source": [
    "Similar to above, but incorporates the state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de3e948e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab_epsGreedyST (generic function with 1 method)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses tabular representation instead of VFA, e-greedy action selection, and state trace \n",
    "function smarviTab_epsGreedyST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        #choose actual action\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5619d4e1",
   "metadata": {},
   "source": [
    "Similar to above (so e-greedy and state trace), but uses a moving average window to approximate g, allowing old estimates to be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e0f86bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTabMA (generic function with 1 method)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tabular SMARVI with moving average online approximation for g, e-greedy action selection, and state trace\n",
    "function smarviTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; window = 2500000, printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    totalCosts = [0.0]\n",
    "    timePassed = 0.0\n",
    "    totalTimes = [0.0]\n",
    "    lenTotals = 1\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        #choose actual action\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            push!(totalCosts, runningTotal)\n",
    "            push!(totalTimes, timePassed)\n",
    "            lenTotals += 1\n",
    "            if lenTotals <= window\n",
    "                g = runningTotal/timePassed\n",
    "            else\n",
    "                g = (runningTotal - totalCosts[lenTotals - window])/(timePassed - totalTimes[lenTotals - window])\n",
    "            end\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0ca583e",
   "metadata": {},
   "source": [
    "Given a state, a h table and g, return the PI action for s. Uses the Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a89e1d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactCont (generic function with 1 method)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table, based off continuous model\n",
    "function piActionExactCont(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optH = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows,h) - g*t\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = h[s-a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optH = h[s - optA]\n",
    "\n",
    "        for i in 2:N\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            testH = h[s - a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5696a2b2",
   "metadata": {},
   "source": [
    "Constructs a PI policy using the above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47ec161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactCont (generic function with 1 method)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piPolicyExactCont(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactCont(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c7cc371",
   "metadata": {},
   "source": [
    "Given a h table and fixed g0, approximates the g of the PI policy derived using the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "161485ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationTab (generic function with 1 method)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, h, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    println(\"State Space Completed\")\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    policy = piPolicyExactCont(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Completed\")\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = policy[s]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6efb11fd",
   "metadata": {},
   "source": [
    "Returns an array of feasible actions for N-dim state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c00dff36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enumerateFeasibleActions (generic function with 1 method)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function enumerateFeasibleActions(s,N)\n",
    "    actionSpace = []\n",
    "    if s == fill(3, N)\n",
    "        for i in 1:N\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            push!(actionSpace, a)\n",
    "        end\n",
    "        return actionSpace\n",
    "    end\n",
    "\n",
    "    push!(actionSpace, zeros(Int64,N))\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            push!(actionSpace, a)\n",
    "        end\n",
    "    end\n",
    "    return actionSpace\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884ee22f",
   "metadata": {},
   "source": [
    "# SMART Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc90c99f",
   "metadata": {},
   "source": [
    "Tabular SMART algorithm, associated gEvaluation function, and helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6bef550",
   "metadata": {},
   "source": [
    "Given a state and a q-table, return optimal action and associated Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9b86ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actionFromQTab (generic function with 1 method)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function actionFromQTab(s, N, q)\n",
    "    feasibleActions = enumerateFeasibleActions(s,N)\n",
    "\n",
    "    #formulate action\n",
    "    optA = zeros(Int64, N)\n",
    "    if s == fill(3,N)\n",
    "        optA[1] = 1\n",
    "    end\n",
    "    optQ = q[s,optA]\n",
    "    for a in feasibleActions\n",
    "        testQ = q[s,a]\n",
    "        if testQ < optQ\n",
    "            optQ = testQ\n",
    "            optA = a\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optQ\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93d0c99a",
   "metadata": {},
   "source": [
    "Construct policy using above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "233657b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactContQ (generic function with 1 method)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piPolicyExactContQ(q, N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = actionFromQTab(s, N, q)[1]\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bcb0dda",
   "metadata": {},
   "source": [
    "Performs SMART, using a state-action trace and e-greedy action selection, where e can be chosen to depend on the state or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f7db3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartTab (generic function with 1 method)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smartTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    optAandQ = actionFromQTab(s, N, q)\n",
    "    optA = optAandQ[1]\n",
    "    optQ = optAandQ[2]\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        optFlag = true\n",
    "        \n",
    "        #choose e-greedy action\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            optQ = q[s,optA]\n",
    "            optFlag = false\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g if optimal action taken\n",
    "            if optFlag\n",
    "                runningTotal += c\n",
    "                timePassed += time\n",
    "                g = runningTotal/timePassed\n",
    "            end\n",
    "\n",
    "            #find next optimal action and q value\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb19c603",
   "metadata": {},
   "source": [
    "Taking a Q table as input, formulates the associated policy and simulates it to approximate g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0943848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationTabQ (generic function with 1 method)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationTabQ(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, q; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    println(\"State Space Completed\")\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    policy = piPolicyExactContQ(q, N)\n",
    "    println(\"Policy Completed\")\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = policy[s]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15b7f9ed",
   "metadata": {},
   "source": [
    "On-Policy equivalent of SMART, using next chosen action instead of next optimal action for the update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "433d3d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartOnPolicyTab (generic function with 1 method)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smartOnPolicyTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #choose only action for s = s0\n",
    "    optA = zeros(Int64, N)\n",
    "    optQ = q[s,optA]\n",
    "    optFlag = true\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "\n",
    "            #find next e-greedy action and q value\n",
    "            #find optimal action and q-value\n",
    "            optFlag = true\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            #choose epsilon\n",
    "            epsilon = c/(c + n)\n",
    "            \n",
    "            if stateDepEpsilon\n",
    "                epsilon = c/(c + numVisits[sPrime])\n",
    "            end\n",
    "\n",
    "            #select random action with probability epsilon\n",
    "            if rand(Uniform(0,1)) < epsilon\n",
    "                nextOptA = randomAction(sPrime, N)\n",
    "                nextOptQ = q[sPrime,nextOptA]\n",
    "                optFlag = false\n",
    "            end\n",
    "            \n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            optFlag = true\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            #choose epsilon\n",
    "            epsilon = c/(c + n)\n",
    "            \n",
    "            if stateDepEpsilon\n",
    "                epsilon = c/(c + numVisits[sPrime])\n",
    "            end\n",
    "\n",
    "            #select random action with probability epsilon\n",
    "            if rand(Uniform(0,1)) < epsilon\n",
    "                nextOptA = randomAction(sPrime, N)\n",
    "                nextOptQ = q[sPrime,nextOptA]\n",
    "                optFlag = false\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70994d31",
   "metadata": {},
   "source": [
    "Version of SMART using moving average window to approximate g, discarding older data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d8fa137b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartTabMA (generic function with 1 method)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SMART with an MA approximation for g\n",
    "function smartTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; window = 1000000, printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    totalCosts = [0.0]\n",
    "    lenTotalCosts = 1\n",
    "    totalTimes = [0.0]\n",
    "    lenTotal = 1\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    optAandQ = actionFromQTab(s, N, q)\n",
    "    optA = optAandQ[1]\n",
    "    optQ = optAandQ[2]\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        optFlag = true\n",
    "        \n",
    "        #choose e-greedy action\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            optQ = q[s,optA]\n",
    "            optFlag = false\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g if optimal action taken\n",
    "            if optFlag\n",
    "                runningTotal += c\n",
    "                timePassed += time\n",
    "                push!(totalCosts, runningTotal)               \n",
    "                push!(totalTimes, timePassed)\n",
    "                lenTotal += 1\n",
    "                if lenTotal <= window\n",
    "                    g = runningTotal/timePassed\n",
    "                else\n",
    "                    g = (runningTotal - totalCosts[lenTotal - window])/(timePassed - totalTimes[lenTotal - window])\n",
    "                end\n",
    "            end\n",
    "\n",
    "            #find next optimal action and q value\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "199fc408",
   "metadata": {},
   "source": [
    "# Tabular SMARPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "29ea47f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabST (generic function with 1 method)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SMARPE with tabular representation instead of VFA, and state trace \n",
    "function smarpeTabST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, hIn, g0, nMax, b; copyH = false, printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    policy = piPolicyExactCont(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Constructed\")\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        if copyH\n",
    "            h[s] = hIn[s]\n",
    "        else\n",
    "            h[s] = 0.0\n",
    "        end\n",
    "        \n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optA = policy[s]\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9483ef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabStochST (generic function with 1 method)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses tabular representation instead of VFA, e-greedy action selection, and state trace \n",
    "function smarpeTabStochST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #choose random action\n",
    "        optA = randomAction(s,N)\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2da93d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabST_epsSoft_onPolicy (generic function with 1 method)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tabular SMARPE with state trace using e-soft policy\n",
    "function smarpeTabST_epsSoft_onPolicy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, hIn, g0, nMax, b, c; copyH = false, printProgress = false, modCounter = 100000, stateDepEpsilon = true)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    policy = piPolicyExactCont(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Constructed\")\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        if copyH\n",
    "            h[s] = hIn[s]\n",
    "        else\n",
    "            h[s] = 0.0\n",
    "        end\n",
    "        \n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate action\n",
    "        optA = policy[s]\n",
    "        optV = 0.0\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        #e-greedy action\n",
    "        bestA = optA\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            bestA = randomAction(s,N)\n",
    "            if bestA == zeros(Int64, N)\n",
    "                t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            end\n",
    "        end\n",
    "\n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe61cfb6",
   "metadata": {},
   "source": [
    "# New Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2411428a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subStates (generic function with 1 method)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns substates of edges in an array, and state of destination node\n",
    "function subStates(s, N, flows)\n",
    "    sis = []\n",
    "    sn = 0\n",
    "    for i in 1:N\n",
    "        if s[i] == 2 || s[i] == 3\n",
    "            push!(sis, s[i])\n",
    "        elseif flows[i] == 0\n",
    "            push!(sis, 0)\n",
    "        else\n",
    "            push!(sis, 1)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if flows == fill(0.0, N)\n",
    "        sn = 1\n",
    "    end\n",
    "    \n",
    "    return sis, sn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b40410e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 4 methods)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates v from seperate ve and vn tables\n",
    "function v(s::Vector{Int64}, N::Int64, flows::Vector{Float64}, ve::Dict, vn::Dict)\n",
    "    substates = subStates(s, N, flows)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    v = 0.0\n",
    "    for i in 1:N\n",
    "        si = sis[i]\n",
    "        v += ve[i,si]\n",
    "    end\n",
    "\n",
    "    v += vn[sn]\n",
    "\n",
    "    return v\n",
    "end\n",
    "\n",
    "function v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    flowsAndCost = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    return v(s, N, flowsAndCost[1], ve, vn)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bdcbb3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and VFA from ve and vn tables\n",
    "function expectedNextValueContNewVFA(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e63f3219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "    #find optimal action\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContNewVFA(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn) - g*t\n",
    "    zeroV = optV\n",
    "\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            \n",
    "            testV = v(s-a, N, flows, ve, vn)\n",
    "            if testV <= optV\n",
    "                optV = testV\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    #Fix choose optimal non-passive action if state is [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optV = v(s-optA, N, flows, ve, vn)\n",
    "            \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "\n",
    "                testV = v(s-a, N, flows, ve, vn)\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV, zeroV\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "66a7c3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateVFA (generic function with 3 methods)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateVFA(s, substates, target, ve, vn, numVisitsE, numVisitsN, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += (b/(b + numVisitsE[i, si]))*(target - currentEst)\n",
    "    end\n",
    "\n",
    "    vn[sn] += (b/(b + numVisitsN[sn]))*(target - currentEst)\n",
    "\n",
    "    return ve, vn\n",
    "end\n",
    "\n",
    "function updateVFA(s, substates, target, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, stepsize)\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += stepsize*(target - currentEst)\n",
    "    end\n",
    "\n",
    "    vn[sn] += stepsize*(target - currentEst)\n",
    "\n",
    "    return ve, vn\n",
    "end\n",
    "\n",
    "function updateVFA(s, substates, target, ve, vn, n, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += (b/(b + n))*(target - currentEst)\n",
    "    end\n",
    "\n",
    "    vn[sn] += (b/(b + n))*(target - currentEst)\n",
    "\n",
    "    return ve, vn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "379e3f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviNewVFA_ST (generic function with 1 method)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses new VFA architecture, e-greedy action selection, and state trace \n",
    "#stepsizeType options: \n",
    "# - varyByNumVisits: uses stepsize b/(b + numVisits)\n",
    "# - varyByIteration: uses stepsize b/(b + n) where n is the iteration modCounter\n",
    "# - constant: uses stepsize b\n",
    "function smarviNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; stepsizeType = \"varyByNumVisits\", printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    \n",
    "    #initialise ve and vn tables\n",
    "    ve = Dict()\n",
    "    numVisitsE = Dict()\n",
    "    for i in 1:N\n",
    "        for si in 0:3\n",
    "            ve[i,si] = 0.0\n",
    "            numVisitsE[i,si] = 0\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    vn = Dict()\n",
    "    numVisitsN = Dict()\n",
    "    for i in 0:1\n",
    "        vn[i] = 0.0\n",
    "        numVisitsN[i] = 0\n",
    "    end\n",
    "\n",
    "    vs0Hist = [0.0]\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    flows0 = copy(flows)\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        substates = subStates(s, N, flows)\n",
    "        sis = substates[1]\n",
    "        sn = substates[2]\n",
    "        for i in 1:N\n",
    "            numVisitsE[i,sis[i]] += 1\n",
    "        end\n",
    "\n",
    "        numVisitsN[sn] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action and v value\n",
    "        optAandV = smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        zeroV = optAandV[3]\n",
    "\n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "\n",
    "        #if random action chosen, choose action action and v value\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N) \n",
    "                optV = zeroV\n",
    "            else \n",
    "                optV = v(s - optA, N, flows, ve, vn)\n",
    "            end \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV \n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                if stepsizeType == \"varyByNumVisits\"\n",
    "                    ve,vn = updateVFA(st, substates, bestV, ve, vn, numVisitsE, numVisitsN, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "                elseif stepsizeType == \"constant\"\n",
    "                    ve,vn = updateVFA(st, substates, bestV, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "                elseif stepsizeType == \"varyByIteration\"\n",
    "                    ve,vn = updateVFA(st, substates, bestV, ve, vn, n, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "                else\n",
    "                    println(\"Invalid stepsize rule\")\n",
    "                    return 0\n",
    "                end\n",
    "\n",
    "                push!(vs0Hist, v(s0, N, flows0, ve, vn))\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return ve, vn, g, gs, vs0Hist\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b30394b",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bce9bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 100.0\n",
       " 200.0\n",
       " 300.0\n",
       " 400.0\n",
       " 500.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "function tau(x)\n",
    "    return x\n",
    "end\n",
    "\n",
    "alpha_d = [0.01*i for i in 1:N]\n",
    "alpha_r = [0.001*i for i in 1:N] \n",
    "beta=10.0\n",
    "c0=[1.0*i for i in 1:N] \n",
    "c1=100.0\n",
    "r=[100.0*i for i in 1:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f00d0796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.87907866081415"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nMax = 2000000\n",
    "resultNewVFA = smarviNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, 1.0, 10.0; stepsizeType = \"varyByNumVisits\", printProgress = true, modCounter = 100000)\n",
    "resultNewVFA[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "443cd336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x000000006EC99D90>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPlot.plot(resultNewVFA[4][200000:nMax])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b89a343",
   "metadata": {},
   "source": [
    "Not converged and learning very slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e237426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599.1453974195149\n",
      "2516.037478090161\n"
     ]
    }
   ],
   "source": [
    "ve = resultNewVFA[1]\n",
    "vn = resultNewVFA[2]\n",
    "println(v([1,1,1,1,1],N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn))\n",
    "println(v([3,3,3,3,3],N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3a80d675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 2 entries:\n",
       "  0 => 143.627\n",
       "  1 => 792.357"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "69284c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x0000000087E67FA0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs0Hist = resultNewVFA[5]\n",
    "PyPlot.plot(vs0Hist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a3e1fa1",
   "metadata": {},
   "source": [
    "## Constant Stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52017ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nMax = 2000000\n",
    "resultNewVFA = smarviNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, 0.01, 1.0; stepsizeType = \"constant\", printProgress = true, modCounter = 100000)\n",
    "println(\"Complete\")\n",
    "println(resultNewVFA[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(resultNewVFA[4][200000:nMax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "712a3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vnHist = resultNewVFA[6]\n",
    "vn0 = []\n",
    "vn1 = []\n",
    "for vn in vnHist\n",
    "    push!(vn0, vn[0])\n",
    "    push!(vn1, vn[1])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(vn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(vn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fc6c2339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523.0231240970654"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ve = resultNewVFA[1]\n",
    "vn = resultNewVFA[2]\n",
    "v([1,1,1,1],N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d8219338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 2 entries:\n",
       "  0 => 55.437\n",
       "  1 => 281.849"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
