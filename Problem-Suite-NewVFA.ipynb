{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c967fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using Random\n",
    "using Plots\n",
    "using PyPlot\n",
    "using StatsBase\n",
    "using StatsPlots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b1ef523",
   "metadata": {},
   "source": [
    "# Problem-Suite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebbc7791",
   "metadata": {},
   "source": [
    "Problem-Suite is a large structured notebook containing all of the functions created so far for this project.\n",
    "\n",
    "Sections:\n",
    "\n",
    "-[Miscellaneous Functions](#Miscellaneous-Functions)\n",
    "\n",
    "-[Pre-requisite functions for uniformised AVI](#Pre-requisite-functions-for-uniformised-AVI)\n",
    "\n",
    "-[Uniformised AVI functions](#Uniformised-AVI-functions)\n",
    "\n",
    "-[Pre-requisite functions for SMARVI](#Pre-requisite-functions-for-SMARVI)\n",
    "\n",
    "-[SMARVI Functions](#SMARVI-Functions)\n",
    "\n",
    "-[Pre-requisite Functions for Exact DP on Homogeneous Problems](#Pre-requisite-Functions-for-Exact-DP-on-Homogeneous-Problems)\n",
    "\n",
    "-[Exact DP on Homogeneous Problems (RVIA and PE/PI)](#Exact-DP-for-Homogeneous-problem)\n",
    "\n",
    "-[Pre-requisite Functions for Exact DP on Inhomogeneous Problems](#Pre-requisite-Functions-for-Exact-DP-on-Inhomogeneous-Problems)\n",
    "\n",
    "-[Exact DP on Inhomogeneous Problems (RVIA and PE/PI)](#Exact-DP-for-Inhomogeneous-Problem-(using-exact-h-or-VFA))\n",
    "\n",
    "-[Evaluation via simulation](#Evaluation-via-simulation)\n",
    "\n",
    "-[APE on Fully Active Policy](#APE-on-Fully-Active-Policy)\n",
    "\n",
    "-[SMARPE](#SMARPE)\n",
    "\n",
    "-[Tabular SMARVI and gEval](#tabular-smarvi-and-geval)\n",
    "\n",
    "-[SMART Functions](#SMART-Functions)\n",
    "\n",
    "-[New Functions](#new-functions)\n",
    "\n",
    "-[Tests](#Tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12fe12",
   "metadata": {},
   "source": [
    "# Miscellaneous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2556d9f",
   "metadata": {},
   "source": [
    "-Functions for enumerating state and action spaces\n",
    "\n",
    "-Functions for calculating flows given a state or state-action pair\n",
    "\n",
    "-Function for evaluating a VFA at a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26a0da98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arrayToString (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#produce an array of array representations of all possible states\n",
    "function enumerateStates(N::Int64)\n",
    "    if N==1\n",
    "        return [[1],[2],[3]]\n",
    "    end\n",
    "    \n",
    "    output = []\n",
    "    lower = enumerateStates(N-1)\n",
    "    for s in lower\n",
    "        new1 = append!([1],s)\n",
    "        new2 = append!([2],s)\n",
    "        new3 = append!([3],s)\n",
    "        append!(output,[new1])\n",
    "        append!(output,[new2])\n",
    "        append!(output,[new3])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "#produce an array of array representations of all possible actions\n",
    "function enumerateActions(N::Int64)\n",
    "    if N==1\n",
    "        return [[0],[1]]\n",
    "    end\n",
    "    \n",
    "    output = []\n",
    "    lower = enumerateActions(N-1)\n",
    "    for a in lower\n",
    "        new1 = append!([0],a)\n",
    "        new2 = append!([1],a)\n",
    "        append!(output,[new1])\n",
    "        append!(output,[new2])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end    \n",
    "\n",
    "#produce array of array representations of all restricted, or single-repair, actions\n",
    "function enumerateRestrictedActions(N::Int64)\n",
    "    if N==1\n",
    "        return [[0],[1]]\n",
    "    end\n",
    "    \n",
    "    output = [zeros(Int64,N)]\n",
    "    for i in 1:N\n",
    "        temp = zeros(N)\n",
    "        temp[i] = 1\n",
    "        append!(output,[temp])\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "#convert all array elements to string, then concatanate all elements (DEPRECATED AS DICTS CAN TAKE ARRAYS AS KEYS)\n",
    "function arrayToString(x)\n",
    "    return join(string.(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ba543e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateFlows (generic function with 2 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function for calculating the flows given a state\n",
    "function calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    #update flows\n",
    "    flows = zeros(N)\n",
    "    healthy = sum(i == 1 for i in s)\n",
    "    \n",
    "    #if no links are healthy, return \n",
    "    if healthy == 0\n",
    "        return flows, c1\n",
    "    end\n",
    "    \n",
    "    #otherwise, find best route, and return\n",
    "    bestCost = maximum(c0) + 1\n",
    "    usedLink = 0\n",
    "    for k in 1:N\n",
    "        if s[k] == 1 && c0[k] < bestCost\n",
    "            bestCost = c0[k]\n",
    "            usedLink = k\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[usedLink] = beta\n",
    "    \n",
    "    return flows, bestCost\n",
    "end\n",
    "\n",
    "#function for calculating the flows given a state-action pair\n",
    "function calculateFlows(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    sPrime = s - a\n",
    "    return calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "127d3f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate a VFA at a given state\n",
    "function v(s::Vector{Int64}, params::Vector{Float64}, features::Vector{Function})\n",
    "    numFeatures = length(features)\n",
    "    return params[1] + sum(params[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "960bf4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 2 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version of v that takes flows for the features\n",
    "function v(s::Vector{Int64}, flows::Vector{Float64}, params::Vector{Float64}, features::Vector{Function})\n",
    "    N = length(params)\n",
    "    return params[1] + sum(params[i]*features[i-1](s, flows) for i in 2:N)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05d0fc",
   "metadata": {},
   "source": [
    "# Pre-requisite functions for uniformised AVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c56072f",
   "metadata": {},
   "source": [
    "This section contains functions used within the AVI algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "476755cd",
   "metadata": {},
   "source": [
    "Given a state-action pair, return the next random pre-decision state, the instant cost, and the updated flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d157b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateStateAndFlowsUnif (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateStateAndFlowsUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #observe exogenous information\n",
    "    w = rand(Uniform(0, 1))\n",
    "    \n",
    "    #interpret exog info: is it a demand deg, rare deg, or completed repair \n",
    "    found = false\n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        if runningTotal <= w <= runningTotal + flows[k]*alpha_d[k]*del\n",
    "            found = true\n",
    "            sPrime[k] = 3\n",
    "            #println(\"Demand Deg at \"*string.(k))\n",
    "            break\n",
    "        end\n",
    "        runningTotal = runningTotal + flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    if found == false\n",
    "        for k in 1:N\n",
    "            if runningTotal <= w <= runningTotal + alpha_r[k]*del\n",
    "                found = true\n",
    "                sPrime[k] = 3\n",
    "                #println(\"Rare Deg at \"*string.(k))\n",
    "                break\n",
    "            end\n",
    "            runningTotal = runningTotal + alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if found == false && repair > 0\n",
    "        if runningTotal <= w <= runningTotal + tau(repair)*del\n",
    "            found = true\n",
    "            #find all repairing links\n",
    "            repairing = []\n",
    "            for k in 1:N\n",
    "                if sPrime[k] == 2\n",
    "                    append!(repairing,[k])\n",
    "                end\n",
    "            end\n",
    "            repaired = sample(repairing)\n",
    "            sPrime[repaired] = 1\n",
    "            #println(\"Repair completed at \"*string.(repaired))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if found == false\n",
    "        #println(\"No Event\")\n",
    "    end\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    \n",
    "    return sPrime, (beta*bestCost + sum(r[k]*(sPrime[k]==2) for k in 1:N))*del, newFlows\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "836f070e",
   "metadata": {},
   "source": [
    "Given a state action pair, return the instant cost over the delta timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685eb320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostUnif (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instant cost over the timestep\n",
    "function instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(sPrime[i] == 1 for i in 1:N)\n",
    "    repair = sum(sPrime[i] == 2 for i in 1:N)\n",
    "    damaged = sum(sPrime[i] == 3 for i in 1:N)\n",
    "    \n",
    "    #update flows\n",
    "    flowUpdate = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    newFlows = flowUpdate[1]\n",
    "    bestCost = flowUpdate[2]\n",
    "    \n",
    "    return (beta*bestCost + sum(r[k]*(sPrime[k]==2) for k in 1:N))*del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9603d5cd",
   "metadata": {},
   "source": [
    "Given a state-action pair and a VFA, calculate the expected value of the value function after one timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f0a307f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueUnif (generic function with 2 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h. Also used in Exact PE/PI when using a VFA\n",
    "#One version takes flows as an argument, the other calculates the flows\n",
    "function expectedNextValueUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*v(sPrime, vParams, features)\n",
    "end  \n",
    "\n",
    "function expectedNextValueUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, vParams, features)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    flows = calculateFlows(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*v(sPrime, vParams, features)\n",
    "end  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3920172",
   "metadata": {},
   "source": [
    "# Uniformised AVI functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80cbe1e5",
   "metadata": {},
   "source": [
    "Algorithms that perform RAVI on the uniformised version of the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5310d49c",
   "metadata": {},
   "source": [
    "Given some parallel link problem and VFA architecture, perform RAVI, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f83714d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in uniformised setting, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state\n",
    "function aviApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - g\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + v(sPrime, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "718d21d7",
   "metadata": {},
   "source": [
    "Given some parallel link problem and VFA architecture, perform RAVI, using a full expectation for update targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fb36c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviFull (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function aviFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - g\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d76fa41",
   "metadata": {},
   "source": [
    "Similar to above, but only uses the Binary Action Space (BAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ebdac30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aviUnifBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI with BAS in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function aviUnifBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        vs0 = v(s0, vParams, features)\n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        optV = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - vs0\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        testV = instantCostUnif(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - vs0\n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA = testA\n",
    "            optV = testV\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f587fd5b",
   "metadata": {},
   "source": [
    "# Pre-requisite functions for SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2008a2c4",
   "metadata": {},
   "source": [
    "Helper functions for the SMARVI algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1f36cda",
   "metadata": {},
   "source": [
    "Given a state-action pair and pre-calculated flows, return the expected sojourn time for the state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1506350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sojournTime (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the expected sojourn time of a state-action pair\n",
    "function sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    s = s - a\n",
    "    if s == fill(3,N)\n",
    "        return 1/(beta*sum(alpha_d) + sum(alpha_r) + tau(N))\n",
    "    end\n",
    "    \n",
    "    numRep = sum(i == 2 for i in s)\n",
    "    cumulativeRate = 0.0\n",
    "    for i in 1:N\n",
    "        if s[i] == 1\n",
    "            cumulativeRate += flows[i]*alpha_d[i] + alpha_r[i]\n",
    "        elseif s[i] == 2\n",
    "            cumulativeRate += alpha_r[i] + tau(numRep)/numRep\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return 1/cumulativeRate\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb2c6d2b",
   "metadata": {},
   "source": [
    "Given a state-action pair and flows, calculate the expected cost accumulated until a transition occurs, or calculate the simulated cost accumulated over a simulated time del."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66c5307b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostCont (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the expected cost accumulated until a transition \n",
    "function instantCostCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows; del = 0)\n",
    "    if del == 0\n",
    "        del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    end\n",
    "    \n",
    "    return instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ecc13f2",
   "metadata": {},
   "source": [
    "Given a state-action pair, return the next random pre-decision state, the cost accumulated over the sojourn time, and the updated flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75587424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateStateAndFlowsCont (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given a state-action pair, return the next random pre-decision state, the cost accumulated over the sojourn time, and the updated flows\n",
    "function updateStateAndFlowsCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    actualTime = rand(Exponential(del))\n",
    "    result = updateStateAndFlowsUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "    return result[1], instantCostCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows; del = actualTime), result[3], actualTime\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae4efc43",
   "metadata": {},
   "source": [
    "Given a state-action pair, precalculated flows, and a VFA, return the expected value of the VFA after a transition has occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60cda4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueCont (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h\n",
    "function expectedNextValueCont(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime,vParams,features)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, vParams, features)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, vParams, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, vParams, features)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc5b77a7",
   "metadata": {},
   "source": [
    "Similar to above, but assumes the features of the VFA take precalcuated flows as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "488f16a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContFlows (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and a VFA for h\n",
    "#Assumes the VFA is constructed with features taking arguments (s, flows), so flows are precalculated\n",
    "function expectedNextValueContFlows(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime, flows, vParams,features)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext, flowsNext, vParams, features)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "            runningTotal += alpha_r[k]*del*v(sNext, flowsNext, vParams, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                flowsNext = calculateFlows(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext, flowsNext, vParams, features)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a2e614d",
   "metadata": {},
   "source": [
    "Given a state, flows, and a VFA-g pair, return the optimal action and associated V value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97b01347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "    #formulate optimal action and calculate optV\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "    \n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            testV = v(s-a, vParams, features)\n",
    "            if testV <= optV\n",
    "                optV = testV\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #If wanted, force a repair if optA is passive for [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA = zeros(Int64,N)\n",
    "        optA[1] = 1\n",
    "        optV = v(s-optA, vParams, features)\n",
    "        \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                testV = v(s-a, vParams, features)\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f824e",
   "metadata": {},
   "source": [
    "# SMARVI Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dbccab0",
   "metadata": {},
   "source": [
    "Variety of functions which perform the SMARVI algorithm, with different additional features.\n",
    "For clarity, the \"state-trace\" is a method which collects a sequence of states connected by instantaneous actions together, and ensures they all have the same update target. For example, if in state s we take action a!=0, and in the resulting state s+a we take action 0, both s and s+a will have update target (c + E(V(s')) - gt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39c05112",
   "metadata": {},
   "source": [
    "Given a problem and a VFA architecture, perform SMARVI, with no e-greedy action selection or state trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea289cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs SMARVI\n",
    "function smarvi(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action and calculate optV\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = optV - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5c6e48e",
   "metadata": {},
   "source": [
    "Perform SMARVI with a state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2005234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviST (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in continuous time setting, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "function smarviST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    \n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    actionFlag = false\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update stateTrace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate optimal action and calculate optV\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #if optimal action is passive, update VFA for all states in the stateTrace, and simulate the next state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            \n",
    "            #find simulated next state\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "        \n",
    "            bestV = optV - v(s0, vParams, features)\n",
    "            \n",
    "            #update VFA\n",
    "            traceLength = length(stateTrace)\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            #reset stateTrace\n",
    "            stateTrace = []\n",
    "            \n",
    "            #update flows and average\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #if some action is optimal, simply update the state\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58458323",
   "metadata": {},
   "source": [
    "Performs SMARVI with a given fixed value of g0 for action selection. This prevents bad initial estimates of g from severely impacting the algorithm, but restricts the policy space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1840b7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions, and controlling action selection using some fixed g0\n",
    "function smarvi_g0(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g0)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV + g0*t - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25861988",
   "metadata": {},
   "source": [
    "Similar to regular SMARVI, but only using the BAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "318f0267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI with BAS in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "function smarviBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        tPassive = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tPassive\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        tActive = sojournTime(s, testA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        testV = instantCostCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tActive\n",
    "        \n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        #Ignore passive action for broken network\n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d439d00",
   "metadata": {},
   "source": [
    "Similar to regular SMARVI, but where flows are assumes to be passed to the VFA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0daaef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviFlows (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version of smarvi where flows are passed to features\n",
    "function smarviFlows(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    flows0 = copy(flows)\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContFlows(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        \n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                \n",
    "                if vParams[1] + sum(vParams[i+1]*features[i](s-a, flows) for i in 1:numFeatures) <= optV\n",
    "                    optV = vParams[1] + sum(vParams[i+1]*features[i](s-a, flows) for i in 1:numFeatures)\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #Fix random link if optA is passive for [3,3,...,3]\n",
    "        if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, flows, vParams, features)\n",
    "            \n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    testV = v(s-a, flows, vParams, features)\n",
    "                    if testV <= optV\n",
    "                        optV = testV\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - v(s0, flows0, vParams,features)\n",
    "        else\n",
    "            bestV = optV - v(s0, flows0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s, flows) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s, flows) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8365b78",
   "metadata": {},
   "source": [
    "## e-greedy SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f3cba33",
   "metadata": {},
   "source": [
    "Helper functions and main algorithm for e-greedy SMARVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fd55c88",
   "metadata": {},
   "source": [
    "Samples a random feasible action for a state s of length N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2fd2e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randomActionAllDamaged (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate random action\n",
    "function randomAction(s,N)\n",
    "    #deal with all-damaged case\n",
    "    if s == fill(3,N)\n",
    "        return randomActionAllDamaged(N)\n",
    "    end\n",
    "\n",
    "    damaged = [0]\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            append!(damaged, [i])\n",
    "        end\n",
    "    end\n",
    "            \n",
    "    choice = sample(damaged)\n",
    "    optA = zeros(Int64, N)\n",
    "    if choice == 0\n",
    "        return optA\n",
    "    else\n",
    "        optA[choice] = 1\n",
    "        return optA\n",
    "    end\n",
    "end\n",
    "\n",
    "#calculate random action for [3,3...,3] state\n",
    "function randomActionAllDamaged(N)\n",
    "    choice = sample(1:N)\n",
    "    a = zeros(Int64, N)\n",
    "    a[choice] = 1\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48f72466",
   "metadata": {},
   "source": [
    "Performs an e-greedy version of SMARVI, with e_n = b/b+n for some given b. Allows SMARVI to perform some exploratory actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a38d016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarvi_epsGreedy (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#actions are choosen via e-greedy action selection, where a random action is chosen with probability b/b+n\n",
    "function smarvi_epsGreedy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; b = 1.0, printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate e-greedy action\n",
    "        if rand(Uniform(0,1)) <= b/(b + n) \n",
    "            optA = randomAction(s,N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "            else\n",
    "                optV = v(s-optA, vParams, features)\n",
    "            end\n",
    "        else                \n",
    "            optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, vParams, features, g)\n",
    "        \n",
    "            optA = optAandV[1]\n",
    "            optV = optAandV[2]\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eaa235",
   "metadata": {},
   "source": [
    "# Pre-requisite Functions for Exact DP on Homogeneous Problems "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7b7c104",
   "metadata": {},
   "source": [
    "Helper functions for Homogeneous Exact DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1801225",
   "metadata": {},
   "source": [
    "Calculates instant cost for homogeneous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22678b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instantCostHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instant cost function strictly for homogeneous problem\n",
    "function instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    i1Prime = i1 - a\n",
    "    i2Prime = i2 + a\n",
    "    \n",
    "    #if no links are healthy, return \n",
    "    if N - i1 - i2 == 0\n",
    "        return (beta*c1 + r*i2Prime)*del\n",
    "    end\n",
    "    \n",
    "    \n",
    "    return (beta*c0 + r*i2Prime)*del\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94e504b7",
   "metadata": {},
   "source": [
    "Given state (i_1, i_2) and action a, calculates the expected next value function after one timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a17eacf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates E(h(s')) from s,a strictly for a homogeneous problem\n",
    "function expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    #immediate change\n",
    "    i1Prime = i1 - a\n",
    "    i2Prime = i2 + a\n",
    "    thisH = h[i1+1,i2+1]\n",
    "    \n",
    "    #if all are damaged\n",
    "    if i1Prime == N\n",
    "        return thisH\n",
    "    end\n",
    "    \n",
    "    #if none are healthy\n",
    "    if N - i1 - i2 == 0\n",
    "        return thisH + tau(i2Prime)*del*(h[i1Prime+1,i2Prime-1+1] - thisH) + i2Prime*del*alpha_r*(h[i1Prime+1+1, i2Prime-1+1] - thisH)\n",
    "    end\n",
    "    \n",
    "    #if none are repairing\n",
    "    if i2Prime == 0\n",
    "        return thisH + (beta*alpha_d + (N - i1 - i2)*alpha_r)*del*(h[i1Prime+1+1,i2Prime+1] - thisH)\n",
    "    end\n",
    "    \n",
    "    return thisH + (beta*alpha_d + (N - i1 - i2)*alpha_r)*del*(h[i1Prime+1+1,i2Prime+1] - thisH) + i2Prime*alpha_r*del*(h[i1Prime+1+1,i2Prime-1+1] - thisH) + tau(i2Prime)*del*(h[i1Prime+1,i2Prime-1+1] - thisH)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68c7af12",
   "metadata": {},
   "source": [
    "Given state (i_1,i_2) and value function h, calculates and returns the best action for the state using full expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "beeb2b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the exact PI action strictly for a homogeneous problem\n",
    "function piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "    if i1 == 0\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    if i1 == N && forceActive\n",
    "        optA = 1\n",
    "        optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        for a in 2:i1\n",
    "            testH = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            if testH <= optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "        return optA\n",
    "    end\n",
    "    \n",
    "    optA = 0\n",
    "    optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    for a in 1:i1\n",
    "        testH = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        if testH <= optH\n",
    "            optA = a\n",
    "            optH = testH\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71928099",
   "metadata": {},
   "source": [
    "Similar to above, but uses the approximation Q(s,a) = h(s+a) for a!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b58a1177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the approx PI action based on instantaneous approximation, strictly for a homogeneous problem\n",
    "function piActionHomogApprox(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = false)\n",
    "    #deal with \"nothing damaged\" edge case\n",
    "    if i1 == 0\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    #deal with \"everything damaged\" edge case\n",
    "    if i1 == N && forceActive\n",
    "        optA = 1\n",
    "        optH = h[i1-optA+1,i2+optA+1]\n",
    "        for a in 2:i1\n",
    "            testH = h[i1-a+1,i2+a+1]\n",
    "            if testH <= optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "        return optA\n",
    "    end\n",
    "    \n",
    "    optA = 0\n",
    "    optH = instantCostHomog(i1,i2,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) - g*del\n",
    "    for a in 1:i1\n",
    "        testH = h[i1-a+1,i2+a+1]\n",
    "        if testH <= optH\n",
    "            optA = a\n",
    "            optH = testH\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8ab05cf",
   "metadata": {},
   "source": [
    "Given h, constructs optimal policy using exact PI method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6335a065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the exact PI policy strictly for a homogeneous problem\n",
    "function piPolicyHomog(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = false)\n",
    "    policy = zeros(Int64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            policy[i1+1,i2+1] = piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34525984",
   "metadata": {},
   "source": [
    "Given h, constructs optimal policy using Q(s,a) = h(s,a) for a!=0 approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "404bfec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates the approx PI policy based on instantaneous approximation, strictly for a homogeneous problem\n",
    "function piPolicyHomogApprox(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = false)\n",
    "    policy = zeros(Int64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            policy[i1+1,i2+1] = piActionHomogApprox(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = forceActive)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905b77bf",
   "metadata": {},
   "source": [
    "Constructs a h table from a VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42156f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hFromVFAHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hFromVFAHomog(N, params, features)\n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return hIn\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c11abe",
   "metadata": {},
   "source": [
    "# Exact DP for Homogeneous problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fc32308",
   "metadata": {},
   "source": [
    "Actual DP algorithms "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99f7e6cf",
   "metadata": {},
   "source": [
    "Given a h table, performs PE on PI policy derived from h, and returns g, h, n (number of iterations), and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc148338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given exact h function, strictly for a homogeneous problem \n",
    "function rpiHomog(N::Int64, hIn, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    policy = piPolicyHomog(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = policy[i1+1,i2+1]\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b8046e6",
   "metadata": {},
   "source": [
    "Performs PE on the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e39012f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpeFAHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates the fully active policy, strictly for a homogeneous problem \n",
    "function rpeFAHomog(N::Int64, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = i1\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c051660",
   "metadata": {},
   "source": [
    "Similar to rpiHomog, but uses Q(s,a) = h(s,a) approximation for PI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "964c01ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates an approximate PI policy based on a given exact h function and instananeous actions, strictly for a homogeneous problem \n",
    "function rpiHomogApprox(N::Int64, hIn, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64, g::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    policy = piPolicyHomogApprox(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g; forceActive = forceActive)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = policy[i1+1,i2+1]\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e1c1de",
   "metadata": {},
   "source": [
    "Similar to above, but uses VFA as input h, and uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7146cc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogVFAApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given VFA, using instananeous approximation, strictly for a homogeneous problem \n",
    "function rpiHomogVFAApprox(N::Int64, params, features, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64, g::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    \n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #run standard function\n",
    "    return rpiHomogApprox(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = nMax, delScale = delScale, forceActive = forceActive)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54d455db",
   "metadata": {},
   "source": [
    "Similar to above, WITHOUT Q(s,a) = h(s+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04dc9704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiHomogVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a PI policy based on a given VFA, strictly for a homogeneous problem \n",
    "function rpiHomogVFA(N::Int64, params, features, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    \n",
    "    #construct hIn table\n",
    "    hIn = zeros(Float64, N+1, N+1)\n",
    "    for i1 in 0:N\n",
    "        for i2 in 0:(N - i1)\n",
    "            s = fill(1,N)\n",
    "            if i1 > 0\n",
    "                for i in 1:i1\n",
    "                    s[i] = 3\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if i2 > 0\n",
    "                for i in (i1+1):(i1+i2)\n",
    "                    s[i] = 2\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            hIn[i1+1,i2+1] = v(s, params, features)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #run standard function\n",
    "    return rpiHomog(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, forceActive = forceActive)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "690683ed",
   "metadata": {},
   "source": [
    "Performs RVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "939c3f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rviHomog (generic function with 1 method)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs RVIA \n",
    "function rviHomog(N::Int64, alpha_d::Float64, alpha_r::Float64, beta::Float64, tau, c0::Float64, c1::Float64, r::Float64, epsilon::Float64; nMax = 0, delScale = 1, forceActive = false)\n",
    "    #calculate stepsize and initialise h,w,and policy vectors\n",
    "    del = 1/(delScale*(beta*alpha_d + N*alpha_r + tau(N)))\n",
    "    h = zeros(Float64, N+1, N+1)\n",
    "    w = zeros(Float64, N+1, N+1)\n",
    "    n = 0\n",
    "    #repeat until epsilion-convergence or n = nMax\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #calculate new w values\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                a = piActionHomog(i1, i2, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del; forceActive = forceActive)\n",
    "                w[i1+1,i2+1] = instantCostHomog(i1,i2,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueHomog(i1,i2, h, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #calculate new relative values\n",
    "        hNew = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:(N - i1)\n",
    "                hNew[i1+1,i2+1] = w[i1+1,i2+1] - w[1,1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #check for convergence\n",
    "        deltas = zeros(Float64, N+1, N+1)\n",
    "        for i1 in 0:N\n",
    "            for i2 in 0:N-i1\n",
    "                deltas[i1+1,i2+1] = hNew[i1+1,i2+1] - h[i1+1,i2+1]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        h = hNew\n",
    "        if maximum(deltas) < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    v = beta*c0*del + (beta*alpha_d + (N)*alpha_r)*del*h[1+1,0+1] + (1 - (beta*alpha_d + (N)*alpha_r)*del)*h[0+1,0+1]\n",
    "    \n",
    "    return v/del, h, n\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b3849",
   "metadata": {},
   "source": [
    "# Pre-requisite Functions for Exact DP on Inhomogeneous Problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1e53ece",
   "metadata": {},
   "source": [
    "Helper functions for inhomogeneous exact DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14ea586f",
   "metadata": {},
   "source": [
    "Given a state-action pair and h, calculates the expected next value of the value function after one timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d45adbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueExact (generic function with 1 method)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates E(h(s')) from s,a using exact h table\n",
    "function expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    runningTotal = 0.0\n",
    "    runningTotalProb = 0.0\n",
    "    \n",
    "    flows = zeros(Float64, N)\n",
    "    if healthy > 0\n",
    "        #otherwise, find best route, and return\n",
    "        bestCost = maximum(c0) + 1\n",
    "        usedLink = 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 1 && c0[k] < bestCost\n",
    "                bestCost = c0[k]\n",
    "                usedLink = k\n",
    "            end \n",
    "        end\n",
    "        \n",
    "        flows[usedLink] = beta\n",
    "    end\n",
    "    \n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*h[sNext]\n",
    "        runningTotalProb += flows[k]*alpha_d[k]*del\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*h[sNext]\n",
    "            runningTotalProb += alpha_r[k]*del\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*h[sNext]\n",
    "                runningTotalProb += (tau(repair)/repair)*del\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal + (1 - runningTotalProb)*h[sPrime]\n",
    "end "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f2dad95",
   "metadata": {},
   "source": [
    "Given a state and a h table, calculates the PI action for s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b71c013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExact (generic function with 1 method)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table\n",
    "function piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2cb0d43",
   "metadata": {},
   "source": [
    "Similar to above, but uses the approximation Q(s,a) = h(s+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17b1bbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table, based off instananeous actions\n",
    "function piActionExactInstant(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,h) - g*del\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = h[s-a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7eb2f91a",
   "metadata": {},
   "source": [
    "Similar to above, but uses a VFA instead of a h table, WITHOUT Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bab1aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using a VFA\n",
    "function piActionVFA(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4096d38",
   "metadata": {},
   "source": [
    "Similar to above, WITH Q(s,a) = h(s,a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "604bf237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using a VFA and instananeous actions\n",
    "function piActionVFAInstant(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    optH = instantCostUnif(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueUnif(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del,params,features) - g*del\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = v(s-a,params,features)\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884618d7",
   "metadata": {},
   "source": [
    "Constructs PI policy using h table and no approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1dfe8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExact (generic function with 1 method)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy using exact h table\n",
    "function piPolicyExact(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8842b6de",
   "metadata": {},
   "source": [
    "Constructs PI policy using h table and Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bfb9dcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy using exact h table, interpretting h with instant actions\n",
    "function piPolicyExactInstant(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactInstant(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d2fa28",
   "metadata": {},
   "source": [
    "Constructs PI policy using VFA and no approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72a89a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy from a VFA\n",
    "function piPolicyVFA(params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionVFA(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d72cd38",
   "metadata": {},
   "source": [
    "Constructs PI policy using VFA and Q(s,a) = h(s+a) approximation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14bba9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI policy from a VFA, using instant actions to interpret h\n",
    "function piPolicyVFAInstant(params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionVFAInstant(s, params, features, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f8cc379",
   "metadata": {},
   "source": [
    "Constructs h table from VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28d6c8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hFromVFAInhomog (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hFromVFAInhomog(N, params, features)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = v(s, params. features)\n",
    "    end\n",
    "\n",
    "    return h\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7172ee4",
   "metadata": {},
   "source": [
    "# Exact DP for Inhomogeneous Problem (using exact h or VFA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43e2ee1b",
   "metadata": {},
   "source": [
    "DP algorithms for inhomogeneous problem\n",
    "\n",
    "Note that throughout when we talk of a Q(s,a) = h(s+a) approximation, this only refers to action selection and not update rules, and excludes a=0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2225adc2",
   "metadata": {},
   "source": [
    "Given an explicit policy table, performs PE, returns g, h and n (# of iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b118e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpe (generic function with 1 method)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs PE using exact policy table\n",
    "function rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    h = Dict()\n",
    "    w = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        w[s] = 0.0\n",
    "    end\n",
    "    s0  = fill(1, N)\n",
    "    n = 0\n",
    "    \n",
    "    #do until max iterations met or epsilon convergence\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #find updates for every state\n",
    "        for s in stateSpace\n",
    "            a = policy[s]\n",
    "            w[s] = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "        end\n",
    "        \n",
    "        #calculate relative values and delta\n",
    "        delta = 0\n",
    "        for s in stateSpace\n",
    "            update = w[s] - w[s0]\n",
    "            if delta < update - h[s] || delta == 0\n",
    "                delta = update - h[s]\n",
    "            end\n",
    "            \n",
    "            h[s] = update\n",
    "        end\n",
    "        \n",
    "        #stopping condition\n",
    "        if delta < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if printProgress && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    a = zeros(Int64, N)\n",
    "    g = instantCostUnif(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h) - h[s0]\n",
    "    \n",
    "    return g/del, h, n\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1ad5a2b",
   "metadata": {},
   "source": [
    "Given a h table, constructs PI policy and performs PE, returning g, h, n and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c57636f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiExact (generic function with 1 method)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using exact h table\n",
    "function rpiExact(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    policy = piPolicyExact(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "    output = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "    return output[1], output[2], output[3], policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec78eb5a",
   "metadata": {},
   "source": [
    "Similar to above, but uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "266027ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiExactInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using exact h table, using instant actions to interpet h\n",
    "function rpiExactInstant(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    policy = piPolicyExactInstant(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del, g)\n",
    "    output = rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "    return output[1], output[2], output[3], policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa6660e",
   "metadata": {},
   "source": [
    "Performs PE on the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f773f8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpeFA (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs exact PE on the fully-active policy\n",
    "function rpeFA(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = faAction(s)\n",
    "    end\n",
    "    \n",
    "    return rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c325a927",
   "metadata": {},
   "source": [
    "Performs PE on fully passive policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d304fb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpePassive (generic function with 1 method)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs exact PE on the passive policy\n",
    "function rpePassive(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    return rpe(N, policy, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52c45280",
   "metadata": {},
   "source": [
    "Given a VFA, constructs PI policy and performs PE, returning g, h, n and the PI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ec6cacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using VFA\n",
    "function rpiVFA(N, params, features, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    hIn = hFromVFAInhomog(N, params, features)\n",
    "    return rpiExact(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68209e10",
   "metadata": {},
   "source": [
    "Similar to above, but uses Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a19903b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpiVFAInstant (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs one round of exact PI and PE using VFA and instantaneous actions to interpret h\n",
    "function rpiVFAInstant(N, params, features, g, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    hIn = hFromVFAInhomog(N, params, features)\n",
    "    return rpiExactInstant(N, hIn, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon, g; nMax = nMax, delScale = delScale, printProgress = printProgress, modCounter = modCounter)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b0b6c10",
   "metadata": {},
   "source": [
    "Performs RVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d38fbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rvi (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs RVIA\n",
    "function rvi(N, alpha_d, alpha_r, beta, tau, c0, c1, r, epsilon; nMax = 0, delScale = 1, printProgress = false, modCounter = 100)\n",
    "    #calculate stepsize and initialise h,w,and policy dictionaries\n",
    "    del = 1/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    h = Dict()\n",
    "    w = Dict()\n",
    "    policy = Dict()\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        w[s] = 0.0\n",
    "        policy[s] = zeros(Int64,N)\n",
    "    end\n",
    "    s0  = fill(1, N)\n",
    "    n = 0\n",
    "    \n",
    "    #do until max iterations met or epsilon convergence\n",
    "    while true\n",
    "        n = n + 1\n",
    "        #find updates for every state\n",
    "        for s in stateSpace\n",
    "            a = piActionExact(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "            w[s] = instantCostUnif(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h)\n",
    "        end\n",
    "        \n",
    "        #calculate relative values and delta\n",
    "        delta = 0\n",
    "        for s in stateSpace\n",
    "            update = w[s] - w[s0]\n",
    "            if delta < update - h[s] || delta == 0\n",
    "                delta = update - h[s]\n",
    "            end\n",
    "            \n",
    "            h[s] = update\n",
    "        end\n",
    "        \n",
    "        #stopping condition\n",
    "        if delta < epsilon || n == nMax\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if printProgress && n%modCounter == 0\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    a = zeros(Int64, N)\n",
    "    g = instantCostUnif(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del) + expectedNextValueExact(s0,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, h) - h[s0]\n",
    "    \n",
    "    return g/del, h, n, policy\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb424a1e",
   "metadata": {},
   "source": [
    "# Evaluation via simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bcbbb37",
   "metadata": {},
   "source": [
    "Various evaluation functions for approximating g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ef631d9",
   "metadata": {},
   "source": [
    "Takes a trained VFA and learns g, using g also for control, starting from state s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8261907a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation (generic function with 1 method)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA via PI using simulation\n",
    "function gEvaluation(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, printState = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if printState\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "320c63e1",
   "metadata": {},
   "source": [
    "Similar to above, but starting from a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9fd18438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFromS (generic function with 1 method)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluationFromS(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    flowResult = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    flows = flowResult[1]\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end \n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ceb6a98",
   "metadata": {},
   "source": [
    "Similar to gEvaluation, but uses a fixed g0 for control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba5ce5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluation_g0(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3cd3eae",
   "metadata": {},
   "source": [
    "Similar to above, but starts from a given state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b85a37ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFromS_g0 (generic function with 1 method)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluationFromS_g0(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    flowResult = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    flows = flowResult[1]\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end \n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = v(s-optA, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6636431",
   "metadata": {},
   "source": [
    "Finds the g of the fully active policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33be91d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationFA (generic function with 1 method)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates the FA policy\n",
    "function gEvaluationFA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate FA action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d651dbd0",
   "metadata": {},
   "source": [
    "Similar to gEvaluation, but only uses the BAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8ba1023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationBAS (generic function with 1 method)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA via PI using simulation\n",
    "function gEvaluationBAS(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        tPassive = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tPassive\n",
    "        \n",
    "        testA = faAction(s)\n",
    "        tActive = sojournTime(s, testA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        testV = instantCostCont(s,testA, N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,testA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*tActive\n",
    "        if testV <= optV\n",
    "            optV = testV\n",
    "            optA = testA\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA = testA\n",
    "            optV = testV\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            append!(runningTotals, [runningTotal])\n",
    "            timePassed += time\n",
    "            append!(times,[timePassed])\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d81acf",
   "metadata": {},
   "source": [
    "Similar to gEvaluation_g0, but assumes that flows are passed to the VFA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "862863bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluation_g0_flows (generic function with 1 method)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluates a VFA using simulation\n",
    "function gEvaluation_g0_flows(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, vParams, features, g0; printProgress = false, modCounter = 100000, forceActive = false, stateTrace = false)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    runningTotals = [0.0]\n",
    "    times = [0.0]\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        if stateTrace\n",
    "            println(s)\n",
    "        end\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optA = zeros(Int64,N)\n",
    "        t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "        optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContFlows(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g0*t\n",
    "        for i in 1:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                vTest = v(s-a, flows, vParams, features)\n",
    "                if vTest <= optV\n",
    "                    optV = vTest\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if forceActive && s == fill(3,N) && optA == zeros(Int64,N)\n",
    "            optA[1] = 1\n",
    "            optV = v(s-optA, flows, vParams, features)\n",
    "            for i in 2:N\n",
    "                if s[i] == 3\n",
    "                    a = zeros(Int64, N)\n",
    "                    a[i] = 1\n",
    "                    vTest = v(s-a, flows, vParams, features)\n",
    "                    if vTest <= optV\n",
    "                        optV = vTest\n",
    "                        optA = a\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        #update state and flows\n",
    "        bestA = optA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            s = result[1]\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs,[g])\n",
    "        append!(runningTotals, [runningTotal])\n",
    "        append!(times,[timePassed])\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gs, runningTotals, times\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88307ffe",
   "metadata": {},
   "source": [
    "# APE on Fully Active Policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7749e9f",
   "metadata": {},
   "source": [
    "Performs APE on the Fully Active Policy using each of the four approaches to estimating a VFA (mixes of uniform/smar and simulated-next-state/expectation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "666815fa",
   "metadata": {},
   "source": [
    "Returns the FA action for a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1319dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "faAction (generic function with 1 method)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computes the Fully Active action for a given state s\n",
    "function faAction(s)\n",
    "    N = length(s)\n",
    "    a = zeros(Int64,N)\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a[i] = 1\n",
    "        end\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9168d0b",
   "metadata": {},
   "source": [
    "Evaluates the FA policy using a VFA and uniformisation, and update targets c + V(s') - gt, where s' is simulated next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e894ba39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAUnifApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE of FA policy in uniformised setting, approximating E(h(s')) for update targets using just h(s'), where s' is the next simulated state\n",
    "function apeFAUnifApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + v(sPrime, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e96581e",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation for updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ccde3d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apeFAUnifFull (generic function with 1 method)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE of FA policy in uniformised setting, approximating E(h(s')) using all possible transitions\n",
    "function apeFAUnifFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; delScale = 1.0, printProgress = false, modCounter = 100000, forceActive = false)\n",
    "    #initialise\n",
    "    del = 1.0/(delScale*(beta*sum(alpha_d) + sum(alpha_r) + tau(N)))\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        c = instantCostUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, del)\n",
    "        bestV = c + expectedNextValueUnif(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, del, vParams, features) - v(s0, vParams,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, vParams, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        c = result[2]\n",
    "        s = sPrime\n",
    "        flows = result[3]\n",
    "        g += (1/n)*(c - g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d7bdaa9",
   "metadata": {},
   "source": [
    "Performs SMARPE on FA policy, with update target c + V(s') - gt where s' is the next simulated state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd7df03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAApprox (generic function with 1 method)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs SMARPE on FA policy, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "function smarpeFAApprox(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + v(sPrime, vParams, features) - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af073bb3",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation in update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8865800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAFull (generic function with 1 method)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarpeFAFull(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + expectedNextValueCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t - v(s0, vParams,features)\n",
    "        else\n",
    "            bestV = v(s - bestA, vParams, features) - v(s0, vParams,features)\n",
    "        end\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = vParams[1] + sum(vParams[i+1]*features[i](s) for i in 1:numFeatures)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[vParams])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ceece53",
   "metadata": {},
   "source": [
    "Similar to smarpeFAApprox, but incorporates state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b68eb6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAApproxST (generic function with 1 method)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE on FA policy in continuous time setting, approximating E(h(s')) as h(s') where s' is the next simulated state\n",
    "#Also incorporates the state trace when actions are taken\n",
    "function smarpeFAApproxST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update state trace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #for passive action, do proper update\n",
    "        if bestA == zeros(Int64,N)\n",
    "            #find value of v^n\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + v(sPrime, vParams, features) - g*t - v(s0, vParams,features)\n",
    "\n",
    "            #update VFA\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            stateTrace = []\n",
    "            \n",
    "            #update g, state, and flows\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #for other action, simply update state and move on\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c19d280d",
   "metadata": {},
   "source": [
    "Similar to above, but uses full expectation for update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47d8caaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeFAFullST (generic function with 1 method)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE on FA policy in continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Also incorporates the state trace when actions are taken\n",
    "function smarpeFAFullST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, vParams, features; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    stateTrace = []\n",
    "    flows = zeros(N)\n",
    "    paramHist = [vParams]\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        #update state trace\n",
    "        append!(stateTrace, [s])\n",
    "        \n",
    "        #formulate action\n",
    "        bestA = faAction(s)\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #for passive action, do proper update\n",
    "        if bestA == zeros(Int64,N)\n",
    "            #find value of v^n\n",
    "            c = instantCostCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            bestV = c + expectedNextValueCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, vParams, features) - g*t - v(s0, vParams,features)\n",
    "\n",
    "            #update VFA\n",
    "            for sTrace in stateTrace\n",
    "                currentEst = v(sTrace, vParams, features)\n",
    "                grad = append!([1.0],[features[i](sTrace) for i in 1:numFeatures])\n",
    "                vParams = vParams + (stepsize)*(bestV - currentEst)*grad\n",
    "                append!(paramHist,[vParams])\n",
    "            end\n",
    "            \n",
    "            stateTrace = []\n",
    "            \n",
    "            #update g, state, and flows\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "            \n",
    "        #for other action, simply update state and move on\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return vParams, paramHist, g\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f5b89",
   "metadata": {},
   "source": [
    "# SMARPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54992054",
   "metadata": {},
   "source": [
    "## Semi-Markov Approximate Relative Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be11b69",
   "metadata": {},
   "source": [
    "- SMARPE takes some trained VFA as input, and seeks to learn the associated long run cost g and the value function of the policy derived from the given VFA\n",
    "\n",
    "- Standard SMARPE uses the online training value of g for action selection, allowing the policy to vary throughout training\n",
    "\n",
    "- SMARPE_g0 takes a pre-learned value of g0 to be used for action selection, keeping the policy constant throughout. This value of g0 might be taken directly from SMARVI or from some gEval function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffc082",
   "metadata": {},
   "source": [
    "Worth also discussing is the exact behaviour of the gEval functions.\n",
    "\n",
    "- Standard gEval simply evaluates a VFA, and learns g throughout. In turn, this value of g is used for action selection, so the policy may vary throughout evaluation.\n",
    "\n",
    "- gEval_g0 evaluates a VFA-g0 pair, keeping the policy constant throughout. It may be good practice to always follow standard gEval with gEval_g0, due to the lack of policy variability.\n",
    "\n",
    "gEval functions are the part that actually calculate the PI actions based on the VFAs derived from SMARPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a0e5de3",
   "metadata": {},
   "source": [
    "Given a VFA-g pair, evaluates the PI policy derived from the pair via a new VFA with the same architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6082e341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpe (generic function with 1 method)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs APE in the continuous time setting, approximating E(h(s')) using all possible transitions, and with a fixed g0 for action selection\n",
    "function smarpe(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, stepsize, paramsIn, paramsOut, features, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    numFeatures = length(features)\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    paramHist = [paramsOut]\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, paramsIn, features, g0)\n",
    "        \n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #recalculate optA in terms of new VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, paramsOut, features) - g*t\n",
    "        else\n",
    "            optV = v(s - bestA, paramsOut, features)\n",
    "        end \n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - v(s0, paramsOut ,features)\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = v(s, paramsOut, features)\n",
    "        grad = append!([1.0],[features[i](s) for i in 1:numFeatures])\n",
    "        paramsOut = paramsOut + (stepsize)*(bestV - currentEst)*grad\n",
    "        append!(paramHist,[paramsOut])\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        append!(gs, [g])\n",
    "        \n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return paramsOut, paramHist, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d24567",
   "metadata": {},
   "source": [
    "# Tabular SMARVI and gEval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b101ab9",
   "metadata": {},
   "source": [
    "Tabular SMARVI algorithms (non e-greedy, e-greedy, and e-greedt with state trace), associated gEval function, and helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a829f15",
   "metadata": {},
   "source": [
    "Given a state s, its flows, and a h-g pair, return the optimal action and V value for s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d500f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromTable (generic function with 1 method)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "    #find optimal action\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        \n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "               \n",
    "            if h[s-a] <= optV\n",
    "                optV = h[s-a]\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    #Fix choose optimal non-passive action if state is [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optV = h[s-optA]\n",
    "            \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "                testV = h[s-a]\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94fd262e",
   "metadata": {},
   "source": [
    "Given a state-action pair and a h table, compute the next expected h value given that a transition has occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bd0e5b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContTab (generic function with 1 method)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and tabular h\n",
    "function expectedNextValueContTab(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return h[sPrime]\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*h[sNext]\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*h[sNext]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*h[sNext]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e27d5cd6",
   "metadata": {},
   "source": [
    "Tabular version of SMARVI, with no state trace or e-greedy actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6447836b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab (generic function with 1 method)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Uses tabular representation instead of VFA\n",
    "function smarviTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        bestA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        if bestA == zeros(Int64,N)\n",
    "            bestV = optV - h[s0]\n",
    "        else\n",
    "            bestV = optV - h[s0]\n",
    "        end \n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = h[s]\n",
    "        h[s] += (b/(b + numVisits[s]))*(bestV - currentEst)\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2833561",
   "metadata": {},
   "source": [
    "e-greedy version of the above. e can be chosen to depend on the state or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf860122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab_epsGreedy (generic function with 1 method)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs AVI in the continuous time setting, approximating E(h(s')) using all possible transitions\n",
    "#Uses tabular representation instead of VFA and e-greedy action selection\n",
    "function smarviTab_epsGreedy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        currentEst = h[s]\n",
    "        h[s] += (b/(b + numVisits[s]))*(bestV - currentEst)\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f7777e",
   "metadata": {},
   "source": [
    "Similar to above, but incorporates the state trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "de3e948e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTab_epsGreedyST (generic function with 1 method)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses tabular representation instead of VFA, e-greedy action selection, and state trace \n",
    "function smarviTab_epsGreedyST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        #choose actual action\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5619d4e1",
   "metadata": {},
   "source": [
    "Similar to above (so e-greedy and state trace), but uses a moving average window to approximate g, allowing old estimates to be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5e0f86bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviTabMA (generic function with 1 method)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tabular SMARVI with moving average online approximation for g, e-greedy action selection, and state trace\n",
    "function smarviTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; window = 2500000, printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    totalCosts = [0.0]\n",
    "    timePassed = 0.0\n",
    "    totalTimes = [0.0]\n",
    "    lenTotals = 1\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    h = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "    end\n",
    "    \n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optAandV = smarActionAndVFromTable(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, h, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        \n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        #choose actual action\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N)\n",
    "                t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            else\n",
    "                optV = h[s - optA]\n",
    "            end    \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV - h[s0]\n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            push!(totalCosts, runningTotal)\n",
    "            push!(totalTimes, timePassed)\n",
    "            lenTotals += 1\n",
    "            if lenTotals <= window\n",
    "                g = runningTotal/timePassed\n",
    "            else\n",
    "                g = (runningTotal - totalCosts[lenTotals - window])/(timePassed - totalTimes[lenTotals - window])\n",
    "            end\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0ca583e",
   "metadata": {},
   "source": [
    "Given a state, a h table and g, return the PI action for s. Uses the Q(s,a) = h(s+a) approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a89e1d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piActionExactCont (generic function with 1 method)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates PI action using exact h table, based off continuous model\n",
    "function piActionExactCont(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    if s == fill(1, N)\n",
    "        return zeros(Int64, N)\n",
    "    end\n",
    "    \n",
    "    flows = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)[1]\n",
    "    \n",
    "    optA = zeros(Int64, N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optH = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s, optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows,h) - g*t\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64,N)\n",
    "            a[i] = 1\n",
    "            testH = h[s-a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optH = h[s - optA]\n",
    "\n",
    "        for i in 2:N\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            testH = h[s - a]\n",
    "            if testH < optH\n",
    "                optA = a\n",
    "                optH = testH\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5696a2b2",
   "metadata": {},
   "source": [
    "Constructs a PI policy using the above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "47ec161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactCont (generic function with 1 method)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piPolicyExactCont(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = piActionExactCont(s, h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g)\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c7cc371",
   "metadata": {},
   "source": [
    "Given a h table and fixed g0, approximates the g of the PI policy derived using the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "161485ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationTab (generic function with 1 method)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, h, g0; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    println(\"State Space Completed\")\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    policy = piPolicyExactCont(h, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Completed\")\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = policy[s]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6efb11fd",
   "metadata": {},
   "source": [
    "Returns an array of feasible actions for N-dim state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c00dff36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enumerateFeasibleActions (generic function with 1 method)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function enumerateFeasibleActions(s,N)\n",
    "    actionSpace = []\n",
    "    if s == fill(3, N)\n",
    "        for i in 1:N\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            push!(actionSpace, a)\n",
    "        end\n",
    "        return actionSpace\n",
    "    end\n",
    "\n",
    "    push!(actionSpace, zeros(Int64,N))\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            push!(actionSpace, a)\n",
    "        end\n",
    "    end\n",
    "    return actionSpace\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884ee22f",
   "metadata": {},
   "source": [
    "# SMART Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc90c99f",
   "metadata": {},
   "source": [
    "Tabular SMART algorithm, associated gEvaluation function, and helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6bef550",
   "metadata": {},
   "source": [
    "Given a state and a q-table, return optimal action and associated Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9b86ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actionFromQTab (generic function with 1 method)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function actionFromQTab(s, N, q)\n",
    "    feasibleActions = enumerateFeasibleActions(s,N)\n",
    "\n",
    "    #formulate action\n",
    "    optA = zeros(Int64, N)\n",
    "    if s == fill(3,N)\n",
    "        optA[1] = 1\n",
    "    end\n",
    "    optQ = q[s,optA]\n",
    "    for a in feasibleActions\n",
    "        testQ = q[s,a]\n",
    "        if testQ < optQ\n",
    "            optQ = testQ\n",
    "            optA = a\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optQ\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93d0c99a",
   "metadata": {},
   "source": [
    "Construct policy using above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "233657b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piPolicyExactContQ (generic function with 1 method)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function piPolicyExactContQ(q, N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    policy = Dict()\n",
    "    for s in stateSpace\n",
    "        policy[s] = actionFromQTab(s, N, q)[1]\n",
    "    end\n",
    "\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bcb0dda",
   "metadata": {},
   "source": [
    "Performs SMART, using a state-action trace and e-greedy action selection, where e can be chosen to depend on the state or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1f7db3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartTab (generic function with 1 method)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smartTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    optAandQ = actionFromQTab(s, N, q)\n",
    "    optA = optAandQ[1]\n",
    "    optQ = optAandQ[2]\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        optFlag = true\n",
    "        \n",
    "        #choose e-greedy action\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            optQ = q[s,optA]\n",
    "            optFlag = false\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g if optimal action taken\n",
    "            if optFlag\n",
    "                runningTotal += c\n",
    "                timePassed += time\n",
    "                g = runningTotal/timePassed\n",
    "            end\n",
    "\n",
    "            #find next optimal action and q value\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb19c603",
   "metadata": {},
   "source": [
    "Taking a Q table as input, formulates the associated policy and simulates it to approximate g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0943848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gEvaluationTabQ (generic function with 1 method)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gEvaluationTabQ(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, q; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    println(\"State Space Completed\")\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    \n",
    "    policy = piPolicyExactContQ(q, N)\n",
    "    println(\"Policy Completed\")\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #formulate optimal action\n",
    "        bestA = policy[s]\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #update flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return g, gs, policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15b7f9ed",
   "metadata": {},
   "source": [
    "On-Policy equivalent of SMART, using next chosen action instead of next optimal action for the update target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "433d3d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartOnPolicyTab (generic function with 1 method)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smartOnPolicyTab(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #choose only action for s = s0\n",
    "    optA = zeros(Int64, N)\n",
    "    optQ = q[s,optA]\n",
    "    optFlag = true\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g\n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "\n",
    "            #find next e-greedy action and q value\n",
    "            #find optimal action and q-value\n",
    "            optFlag = true\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            #choose epsilon\n",
    "            epsilon = c/(c + n)\n",
    "            \n",
    "            if stateDepEpsilon\n",
    "                epsilon = c/(c + numVisits[sPrime])\n",
    "            end\n",
    "\n",
    "            #select random action with probability epsilon\n",
    "            if rand(Uniform(0,1)) < epsilon\n",
    "                nextOptA = randomAction(sPrime, N)\n",
    "                nextOptQ = q[sPrime,nextOptA]\n",
    "                optFlag = false\n",
    "            end\n",
    "            \n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            optFlag = true\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            #choose epsilon\n",
    "            epsilon = c/(c + n)\n",
    "            \n",
    "            if stateDepEpsilon\n",
    "                epsilon = c/(c + numVisits[sPrime])\n",
    "            end\n",
    "\n",
    "            #select random action with probability epsilon\n",
    "            if rand(Uniform(0,1)) < epsilon\n",
    "                nextOptA = randomAction(sPrime, N)\n",
    "                nextOptQ = q[sPrime,nextOptA]\n",
    "                optFlag = false\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70994d31",
   "metadata": {},
   "source": [
    "Version of SMART using moving average window to approximate g, discarding older data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d8fa137b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartTabMA (generic function with 1 method)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SMART with an MA approximation for g\n",
    "function smartTabMA(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; window = 1000000, printProgress = false, modCounter = 100000, stateDepEpsilon = false)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    actionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    totalCosts = [0.0]\n",
    "    lenTotalCosts = 1\n",
    "    totalTimes = [0.0]\n",
    "    lenTotal = 1\n",
    "    stateActionTrace = []\n",
    "    q = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        for a in enumerateFeasibleActions(s,N)\n",
    "            q[s,a] = 0.0\n",
    "        end\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    optAandQ = actionFromQTab(s, N, q)\n",
    "    optA = optAandQ[1]\n",
    "    optQ = optAandQ[2]\n",
    "\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        numVisits[s] += 1\n",
    "        \n",
    "        optFlag = true\n",
    "        \n",
    "        #choose e-greedy action\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            optQ = q[s,optA]\n",
    "            optFlag = false\n",
    "        end\n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        push!(stateActionTrace, (s,bestA))\n",
    "\n",
    "        nextOptA = zeros(Int64, N)\n",
    "        nextOptQ = 0.0\n",
    "        #update q, flows and average\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #simulate transition\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "\n",
    "            c = result[2]\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "  \n",
    "            #update g if optimal action taken\n",
    "            if optFlag\n",
    "                runningTotal += c\n",
    "                timePassed += time\n",
    "                push!(totalCosts, runningTotal)               \n",
    "                push!(totalTimes, timePassed)\n",
    "                lenTotal += 1\n",
    "                if lenTotal <= window\n",
    "                    g = runningTotal/timePassed\n",
    "                else\n",
    "                    g = (runningTotal - totalCosts[lenTotal - window])/(timePassed - totalTimes[lenTotal - window])\n",
    "                end\n",
    "            end\n",
    "\n",
    "            #find next optimal action and q value\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "\n",
    "            for saPair in stateActionTrace\n",
    "                st = saPair[1]\n",
    "                q[saPair] += (b/(b + numVisits[st]))*(c + nextOptQ - g*time - q[saPair])\n",
    "            end\n",
    "\n",
    "            stateActionTrace = []\n",
    "        else\n",
    "            sPrime = s - bestA\n",
    "\n",
    "            nextOptAandQ = actionFromQTab(sPrime, N, q)\n",
    "            nextOptA = nextOptAandQ[1]\n",
    "            nextOptQ = nextOptAandQ[2]\n",
    "        end\n",
    "        \n",
    "        s = sPrime\n",
    "        optA = nextOptA\n",
    "        optQ = nextOptQ\n",
    "\n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return q, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "199fc408",
   "metadata": {},
   "source": [
    "# Tabular SMARPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "29ea47f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabST (generic function with 1 method)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SMARPE with tabular representation instead of VFA, and state trace \n",
    "function smarpeTabST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, hIn, g0, nMax, b; copyH = false, printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    policy = piPolicyExactCont(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Constructed\")\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        if copyH\n",
    "            h[s] = hIn[s]\n",
    "        else\n",
    "            h[s] = 0.0\n",
    "        end\n",
    "        \n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action\n",
    "        optA = policy[s]\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9483ef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabStochST (generic function with 1 method)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses tabular representation instead of VFA, e-greedy action selection, and state trace \n",
    "function smarpeTabStochST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b; printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        h[s] = 0.0\n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #choose random action\n",
    "        optA = randomAction(s,N)\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        bestA = optA\n",
    "        \n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2da93d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarpeTabST_epsSoft_onPolicy (generic function with 1 method)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tabular SMARPE with state trace using e-soft policy\n",
    "function smarpeTabST_epsSoft_onPolicy(N,alpha_d, alpha_r, beta, tau, c0, c1, r, hIn, g0, nMax, b, c; copyH = false, printProgress = false, modCounter = 100000, stateDepEpsilon = true)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    stateSpace = enumerateStates(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    policy = piPolicyExactCont(hIn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, g0)\n",
    "    println(\"Policy Constructed\")\n",
    "    stateTrace = []\n",
    "    \n",
    "    h = Dict()\n",
    "    numVisits = Dict()\n",
    "    for s in stateSpace\n",
    "        if copyH\n",
    "            h[s] = hIn[s]\n",
    "        else\n",
    "            h[s] = 0.0\n",
    "        end\n",
    "        \n",
    "        numVisits[s] = 0\n",
    "    end\n",
    "    \n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    #do nMax iterations of SMARPE\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        numVisits[s] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate action\n",
    "        optA = policy[s]\n",
    "        optV = 0.0\n",
    "        if optA == zeros(Int64, N)\n",
    "            t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "            optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "        end\n",
    "\n",
    "        #e-greedy action\n",
    "        bestA = optA\n",
    "        epsilon = c/(c + n)\n",
    "        if stateDepEpsilon\n",
    "            epsilon = c/(c + numVisits[s])\n",
    "        end\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            bestA = randomAction(s,N)\n",
    "            if bestA == zeros(Int64, N)\n",
    "                t = sojournTime(s, bestA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "                optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContTab(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, h) - g*t\n",
    "            end\n",
    "        end\n",
    "\n",
    "        #find simulated next state\n",
    "        result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "        sPrime = result[1]\n",
    "        \n",
    "        #if action is passive, update VFA across state trace and simulate the next state, and update g. otherwise, simply update current state\n",
    "        if bestA == zeros(Int64, N)\n",
    "            bestV = optV - h[s0]\n",
    "            for st in stateTrace\n",
    "                currentEst = h[st]\n",
    "                h[st] += (b/(b + numVisits[st]))*(bestV - currentEst)\n",
    "            end\n",
    "            stateTrace = []\n",
    "\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return h, g, gs\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe61cfb6",
   "metadata": {},
   "source": [
    "# New Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2411428a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subStates (generic function with 1 method)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns substates of edges in an array, and state of destination node\n",
    "function subStates(s, N, flows)\n",
    "    sis = []\n",
    "    sn = 0\n",
    "    for i in 1:N\n",
    "        if s[i] == 2 || s[i] == 3\n",
    "            push!(sis, s[i])\n",
    "        elseif flows[i] == 0\n",
    "            push!(sis, 0)\n",
    "        else\n",
    "            push!(sis, 1)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if flows == fill(0.0, N)\n",
    "        sn = 1\n",
    "    end\n",
    "    \n",
    "    return sis, sn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b40410e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v (generic function with 5 methods)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculates v from seperate ve and vn tables\n",
    "function v(s::Vector{Int64}, N::Int64, flows::Vector{Float64}, ve::Dict, vn::Dict)\n",
    "    substates = subStates(s, N, flows)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    v = 0.0\n",
    "    for i in 1:N\n",
    "        si = sis[i]\n",
    "        v += ve[i,si]\n",
    "    end\n",
    "\n",
    "    v += vn[sn]\n",
    "\n",
    "    return v\n",
    "end\n",
    "\n",
    "function v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    flowsAndCost = calculateFlows(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r)\n",
    "    return v(s, N, flowsAndCost[1], ve, vn)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bdcbb3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expectedNextValueContNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates E(h(s')) given a state-action pair, and VFA from ve and vn tables\n",
    "function expectedNextValueContNewVFA(s,a,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn)\n",
    "    del = sojournTime(s, a, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    #immediate change\n",
    "    sPrime = s - a\n",
    "    healthy = sum(i == 1 for i in sPrime)\n",
    "    repair = sum(i == 2 for i in sPrime)\n",
    "    damaged = sum(i == 3 for i in sPrime)\n",
    "    \n",
    "    #different treatment for all-damaged state\n",
    "    if sPrime == fill(3,N)\n",
    "        return v(sPrime,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    end\n",
    "    \n",
    "    runningTotal = 0\n",
    "    \n",
    "    #demand degs\n",
    "    for k in 1:N\n",
    "        sNext = copy(sPrime)\n",
    "        sNext[k] = 3\n",
    "        runningTotal += flows[k]*alpha_d[k]*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    end\n",
    "    \n",
    "    #rare degs\n",
    "    for k in 1:N\n",
    "        if sPrime[k] != 3\n",
    "            sNext = copy(sPrime)\n",
    "            sNext[k] = 3\n",
    "            runningTotal += alpha_r[k]*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #repairs\n",
    "    if repair > 0\n",
    "        for k in 1:N\n",
    "            if sPrime[k] == 2\n",
    "                sNext = copy(sPrime)\n",
    "                sNext[k] = 1\n",
    "                runningTotal += (tau(repair)/repair)*del*v(sNext,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return runningTotal\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e63f3219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarActionAndVFromNewVFA (generic function with 1 method)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "    #find optimal action\n",
    "    optA = zeros(Int64,N)\n",
    "    t = sojournTime(s, optA, flows, N, alpha_d, alpha_r, beta, tau)\n",
    "    optV = instantCostCont(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows) + expectedNextValueContNewVFA(s,optA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows, ve, vn) - g*t\n",
    "    zeroV = optV\n",
    "\n",
    "    for i in 1:N\n",
    "        if s[i] == 3\n",
    "            a = zeros(Int64, N)\n",
    "            a[i] = 1\n",
    "            \n",
    "            testV = v(s-a, N, flows, ve, vn)\n",
    "            if testV <= optV\n",
    "                optV = testV\n",
    "                optA = a\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    #Fix choose optimal non-passive action if state is [3,3,...,3]\n",
    "    if s == fill(3,N) && optA == zeros(Int64, N)\n",
    "        optA[1] = 1\n",
    "        optV = v(s-optA, N, flows, ve, vn)\n",
    "            \n",
    "        for i in 2:N\n",
    "            if s[i] == 3\n",
    "                a = zeros(Int64, N)\n",
    "                a[i] = 1\n",
    "\n",
    "                testV = v(s-a, N, flows, ve, vn)\n",
    "                if testV <= optV\n",
    "                    optV = testV\n",
    "                    optA = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return optA, optV, zeroV\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "66a7c3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateVFA (generic function with 4 methods)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateVFA(s, substates, target, ve, vn, numVisitsE, numVisitsN, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += (b/(b + numVisitsE[i, si]))*(target - currentEst)\n",
    "    end\n",
    "\n",
    "    vn[sn] += (b/(b + numVisitsN[sn]))*(target - currentEst)\n",
    "\n",
    "    return ve, vn\n",
    "end\n",
    "\n",
    "function updateVFA(s, substates, target, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, stepsize)\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += stepsize*(target - currentEst)\n",
    "    end\n",
    "\n",
    "    vn[sn] += stepsize*(target - currentEst)\n",
    "\n",
    "    return ve, vn\n",
    "end\n",
    "\n",
    "function updateVFA(s, substates, target, ve, vn, n, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "    currentEst = v(s,N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)\n",
    "    sis = substates[1]\n",
    "    sn = substates[2]\n",
    "\n",
    "    for i in 1:N \n",
    "        si = sis[i]\n",
    "        ve[i, si] += (b/(b + n))*(target - currentEst)\n",
    "    end\n",
    "\n",
    "    vn[sn] += (b/(b + n))*(target - currentEst)\n",
    "\n",
    "    return ve, vn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "379e3f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smarviNewVFA_ST (generic function with 1 method)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses new VFA architecture, e-greedy action selection, and state trace \n",
    "#stepsizeType options: \n",
    "# - varyByNumVisits: uses stepsize b/(b + numVisits)\n",
    "# - varyByIteration: uses stepsize b/(b + n) where n is the iteration modCounter\n",
    "# - constant: uses stepsize b\n",
    "function smarviNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, b, c; stepsizeType = \"varyByNumVisits\", printProgress = false, modCounter = 100000)\n",
    "    #initialise\n",
    "    s = [1 for i in 1:N]\n",
    "    s0 = [1 for i in 1:N]\n",
    "    flows = zeros(N)\n",
    "    reducedActionSpace = enumerateRestrictedActions(N)\n",
    "    runningTotal = 0.0\n",
    "    timePassed = 0.0\n",
    "    g = 0.0\n",
    "    gs = [g]\n",
    "    stateTrace = []\n",
    "    \n",
    "    #initialise ve and vn tables\n",
    "    ve = Dict()\n",
    "    numVisitsE = Dict()\n",
    "    for i in 1:N\n",
    "        for si in 0:3\n",
    "            ve[i,si] = 0.0\n",
    "            numVisitsE[i,si] = 0\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    vn = Dict()\n",
    "    numVisitsN = Dict()\n",
    "    for i in 0:1\n",
    "        vn[i] = 0.0\n",
    "        numVisitsN[i] = 0\n",
    "    end\n",
    "\n",
    "    veHist = [ve]\n",
    "    vnHist = [vn]\n",
    "    #initialise flows\n",
    "    bestCost = maximum(c0) + 1\n",
    "    bestLink = 0\n",
    "    for i in 1:N\n",
    "        if c0[i] < bestCost\n",
    "            bestCost = c0[i]\n",
    "            bestLink = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    flows[bestLink] = beta\n",
    "    \n",
    "    flows0 = copy(flows)\n",
    "    #do nMax iterations of AVI\n",
    "    for n in 1:nMax\n",
    "        \n",
    "        #update numVisits\n",
    "        substates = subStates(s, N, flows)\n",
    "        sis = substates[1]\n",
    "        sn = substates[2]\n",
    "        for i in 1:N\n",
    "            numVisitsE[i,sis[i]] += 1\n",
    "        end\n",
    "\n",
    "        numVisitsN[sn] += 1\n",
    "\n",
    "        #update trace\n",
    "        push!(stateTrace, s)\n",
    "\n",
    "        #formulate optimal action and v value\n",
    "        optAandV = smarActionAndVFromNewVFA(s, flows, N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn, g)\n",
    "        optA = optAandV[1]\n",
    "        optV = optAandV[2]\n",
    "        zeroV = optAandV[3]\n",
    "\n",
    "        #choose epsilon\n",
    "        epsilon = c/(c + n)\n",
    "\n",
    "        #if random action chosen, choose action action and v value\n",
    "        if rand(Uniform(0,1)) < epsilon\n",
    "            optA = randomAction(s, N)\n",
    "            if optA == zeros(Int64, N) \n",
    "                optV = zeroV\n",
    "            else \n",
    "                optV = v(s - optA, N, flows, ve, vn)\n",
    "            end \n",
    "        end \n",
    "        \n",
    "        bestA = optA\n",
    "        \n",
    "        #find value of v^n:\n",
    "        bestV = optV \n",
    "        \n",
    "        #update VFA\n",
    "        if bestA == zeros(Int64, N)\n",
    "            for st in stateTrace\n",
    "                if stepsizeType == \"varyByNumVisits\"\n",
    "                    ve,vn = updateVFA(st, substates, bestV, ve, vn, numVisitsE, numVisitsN, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "                elseif stepsizeType == \"constant\"\n",
    "                    ve,vn = updateVFA(st, substates, bestV, ve, vn, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "                elseif stepsizeType == \"varyByIteration\"\n",
    "                    ve,vn = updateVFA(st, substates, bestV, ve, vn, n, N, alpha_d, alpha_r, beta, tau, c0, c1, r, b)\n",
    "                else\n",
    "                    println(\"Invalid stepsize rule\")\n",
    "                    return 0\n",
    "                end\n",
    "\n",
    "                push!(veHist, copy(ve))\n",
    "                push!(vnHist, copy(vn))\n",
    "            end\n",
    "            stateTrace = []\n",
    "        end\n",
    "\n",
    "        #update state, flows and g\n",
    "        if bestA == zeros(Int64, N)\n",
    "            #find simulated next state, cost, flows and sampled sojourn time\n",
    "            result = updateStateAndFlowsCont(s,bestA,N,alpha_d, alpha_r, beta, tau, c0, c1, r, flows)\n",
    "            sPrime = result[1]\n",
    "            c = result[2]\n",
    "            s = sPrime\n",
    "            flows = result[3]\n",
    "            time = result[4]\n",
    "            \n",
    "            runningTotal += c\n",
    "            timePassed += time\n",
    "            g = runningTotal/timePassed\n",
    "        else\n",
    "            s = s - bestA\n",
    "        end\n",
    "        \n",
    "        push!(gs, g)\n",
    "        if printProgress == true && n%modCounter == 0\n",
    "            sleep(0.001)\n",
    "            println(n)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return ve, vn, g, gs, veHist, vnHist\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b30394b",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9bce9bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 100.0\n",
       " 200.0\n",
       " 300.0\n",
       " 400.0"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 4\n",
    "function tau(x)\n",
    "    return x\n",
    "end\n",
    "\n",
    "alpha_d = [0.01*i for i in 1:N]\n",
    "alpha_r = [0.001*i for i in 1:N] \n",
    "beta=10.0\n",
    "c0=[1.0*i for i in 1:N] \n",
    "c1=100.0\n",
    "r=[100.0*i for i in 1:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f00d0796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dict{Any, Any}((1, 2) => 43.73300028085405, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.24257279609415, (2, 0) => 3.4055736746519187, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.13759834566615, (4, 2) => 211.71632323546314…), Dict{Any, Any}(0 => 11.433765470621731, 1 => 407.6730128734733), 28.453389551425108, [0.0, 10.0, 18.740510320254998, 21.451289006722703, 21.910068916929863, 21.910068916929863, 21.910068916929863, 21.910068916929863, 21.910068916929863, 144.8579960373283  …  28.453417634767785, 28.453417634767785, 28.45342386085225, 28.453421449923503, 28.453421449923503, 28.45342312508546, 28.453420630017355, 28.453420630017355, 28.453425087316205, 28.453389551425108], Dict{Any, Any}[Dict((1, 2) => 43.73300028085405, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.24257279609415, (2, 0) => 3.4055736746519187, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.13759834566615, (4, 2) => 211.71632323546314…), Dict((1, 2) => 0.0, (3, 1) => 0.0, (1, 3) => 0.0, (3, 2) => 0.0, (4, 0) => 45.45454545454545, (2, 0) => 45.45454545454545, (3, 3) => 0.0, (4, 1) => 0.0, (2, 1) => 0.0, (4, 2) => 0.0…), Dict((1, 2) => 0.0, (3, 1) => 0.0, (1, 3) => 1.196172248803805, (3, 2) => 0.0, (4, 0) => 46.25199362041466, (2, 0) => 45.45454545454545, (3, 3) => 0.0, (4, 1) => 0.0, (2, 1) => 1.196172248803805, (4, 2) => 0.0…), Dict((1, 2) => 0.0, (3, 1) => -4.788065735704073, (1, 3) => -1.9958715749989104, (3, 2) => 0.0, (4, 0) => 43.85796075256262, (2, 0) => 45.45454545454545, (3, 3) => 0.0, (4, 1) => 0.0, (2, 1) => 1.196172248803805, (4, 2) => 0.0…), Dict((1, 2) => 0.0, (3, 1) => -4.788065735704073, (1, 3) => -1.4821990197749, (3, 2) => 0.0, (4, 0) => 43.85796075256262, (2, 0) => 45.45454545454545, (3, 3) => 1.027345110448021, (4, 1) => 1.027345110448021, (2, 1) => 1.196172248803805, (4, 2) => 0.0…), Dict((1, 2) => 276.34994871501596, (3, 1) => -4.788065735704073, (1, 3) => -1.4821990197749, (3, 2) => 138.17497435750798, (4, 0) => 43.85796075256262, (2, 0) => 45.45454545454545, (3, 3) => 1.027345110448021, (4, 1) => 1.027345110448021, (2, 1) => 1.196172248803805, (4, 2) => 184.23329914334397…), Dict((1, 2) => 449.31999840349, (3, 1) => -4.788065735704073, (1, 3) => -1.4821990197749, (3, 2) => 224.659999201745, (4, 0) => 43.85796075256262, (2, 0) => 45.45454545454545, (3, 3) => 1.027345110448021, (4, 1) => 1.027345110448021, (2, 1) => 1.196172248803805, (4, 2) => 299.5466656023267…), Dict((1, 2) => 447.05136949387503, (3, 1) => -4.788065735704073, (1, 3) => -1.4821990197749, (3, 2) => 223.52568474693751, (4, 0) => 43.85796075256262, (2, 0) => 45.45454545454545, (3, 3) => 1.027345110448021, (4, 1) => 1.027345110448021, (2, 1) => 1.196172248803805, (4, 2) => 298.03424632925004…), Dict((1, 2) => 297.16460524723095, (3, 1) => -4.788065735704073, (1, 3) => -1.4821990197749, (3, 2) => 148.58230262361548, (4, 0) => 43.85796075256262, (2, 0) => 45.45454545454545, (3, 3) => 1.027345110448021, (4, 1) => 1.027345110448021, (2, 1) => 1.196172248803805, (4, 2) => 198.10973683148734…), Dict((1, 2) => 140.3468649013958, (3, 1) => -4.788065735704073, (1, 3) => -1.4821990197749, (3, 2) => 70.1734324506979, (4, 0) => 43.85796075256262, (2, 0) => 45.45454545454545, (3, 3) => 1.027345110448021, (4, 1) => 1.027345110448021, (2, 1) => 1.196172248803805, (4, 2) => 93.56457660093058…)  …  Dict((1, 2) => 43.732907761130605, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.24262501552354, (2, 0) => 3.4055290241085574, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.13766116142529, (4, 2) => 211.71632323546314…), Dict((1, 2) => 43.732870039366375, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.2426400024265, (2, 0) => 3.4055290241085574, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.13768677242373, (4, 2) => 211.71632323546314…), Dict((1, 2) => 43.7329386011256, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.24261276275593, (2, 0) => 3.4055290241085574, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.137640222768795, (4, 2) => 211.71632323546314…), Dict((1, 2) => 43.7329386011256, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.24260760899574, (2, 0) => 3.405543907660446, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.137640222768795, (4, 2) => 211.71632323546314…), Dict((1, 2) => 43.732900879415865, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.24262259587283, (2, 0) => 3.405543907660446, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.13766583371687, (4, 2) => 211.71632323546314…), Dict((1, 2) => 43.732969441029695, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.24259535626763, (2, 0) => 3.405543907660446, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.13761928418494, (4, 2) => 211.71632323546314…), Dict((1, 2) => 43.732969441029695, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.242590202516, (2, 0) => 3.4055587911885743, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.13761928418494, (4, 2) => 211.71632323546314…), Dict((1, 2) => 43.73293171938003, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.24260518936504, (2, 0) => 3.4055587911885743, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.137644895078864, (4, 2) => 211.71632323546314…), Dict((1, 2) => 43.73300028085405, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.24257794982302, (2, 0) => 3.4055587911885743, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.13759834566615, (4, 2) => 211.71632323546314…), Dict((1, 2) => 43.73300028085405, (3, 1) => -93.39531496318428, (1, 3) => 116.64255365483555, (3, 2) => 134.47051619417797, (4, 0) => -141.24257279609415, (2, 0) => 3.4055736746519187, (3, 3) => 195.67851845160243, (4, 1) => -22.843992598280813, (2, 1) => -56.13759834566615, (4, 2) => 211.71632323546314…)], Dict{Any, Any}[Dict(0 => 11.433765470621731, 1 => 407.6730128734733), Dict(0 => 45.45454545454545, 1 => 0.0), Dict(0 => 46.25199362041466, 1 => 0.0), Dict(0 => 43.85796075256262, 1 => 0.0), Dict(0 => 44.26889879674183, 1 => 0.0), Dict(0 => 44.26889879674183, 1 => 92.11664957167199), Dict(0 => 44.26889879674183, 1 => 149.77333280116335), Dict(0 => 44.26889879674183, 1 => 149.01712316462502), Dict(0 => 44.26889879674183, 1 => 99.05486841574367), Dict(0 => 44.26889879674183, 1 => 46.78228830046529)  …  Dict(0 => 11.433719814964533, 1 => 407.6730128734733), Dict(0 => 11.433706711858138, 1 => 407.6730128734733), Dict(0 => 11.433730527605997, 1 => 407.6730128734733), Dict(0 => 11.433735033558513, 1 => 407.6730128734733), Dict(0 => 11.433721930471851, 1 => 407.6730128734733), Dict(0 => 11.433745746167745, 1 => 407.6730128734733), Dict(0 => 11.433750252113759, 1 => 407.6730128734733), Dict(0 => 11.43373714904877, 1 => 407.6730128734733), Dict(0 => 11.433760964694638, 1 => 407.6730128734733), Dict(0 => 11.433765470621731, 1 => 407.6730128734733)])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nMax = 2000000\n",
    "resultNewVFA = smarviNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, 1.0, 1.0; stepsizeType = \"varyByNumVisits\", printProgress = true, modCounter = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "443cd336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x00000000CED73EB0>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPlot.plot(resultNewVFA[4][1:nMax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e237426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-374.8578257723984"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ve = resultNewVFA[1]\n",
    "vn = resultNewVFA[2]\n",
    "v([1,1,1,1],N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3a80d675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 2 entries:\n",
       "  0 => 11.4338\n",
       "  1 => 407.673"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "69284c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vnHist = resultNewVFA[6]\n",
    "vn0 = []\n",
    "vn1 = []\n",
    "for vn in vnHist\n",
    "    push!(vn0, vn[0])\n",
    "    push!(vn1, vn[1])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a1f19e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x00000000D88CC2B0>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPlot.plot(vn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6c88a971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{PyCall.PyObject}:\n",
       " PyObject <matplotlib.lines.Line2D object at 0x00000000C5B92FD0>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPlot.plot(vn1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ef55784",
   "metadata": {},
   "source": [
    "Clearly not converged and learning too slowly after 250000 iterations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a3e1fa1",
   "metadata": {},
   "source": [
    "## Constant Stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52017ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nMax = 2000000\n",
    "resultNewVFA = smarviNewVFA_ST(N,alpha_d, alpha_r, beta, tau, c0, c1, r, nMax, 0.01, 1.0; stepsizeType = \"constant\", printProgress = true, modCounter = 100000)\n",
    "println(\"Complete\")\n",
    "println(resultNewVFA[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(resultNewVFA[4][200000:nMax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "712a3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vnHist = resultNewVFA[6]\n",
    "vn0 = []\n",
    "vn1 = []\n",
    "for vn in vnHist\n",
    "    push!(vn0, vn[0])\n",
    "    push!(vn1, vn[1])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(vn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPlot.plot(vn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fc6c2339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523.0231240970654"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ve = resultNewVFA[1]\n",
    "vn = resultNewVFA[2]\n",
    "v([1,1,1,1],N,alpha_d, alpha_r, beta, tau, c0, c1, r, ve, vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d8219338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 2 entries:\n",
       "  0 => 55.437\n",
       "  1 => 281.849"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
